{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a3abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, max, min, count, broadcast, desc, asc, when, lit,row_number, rank, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2650fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cfebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:08.628 [Thread-3] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ebb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b3eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:08.753 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "13:47:08.763 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/illidan/proyecto_desde0/ETL/spark-warehouse'.\n",
      "13:47:08.778 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4a338390{/SQL,null,AVAILABLE,@Spark}\n",
      "13:47:08.780 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b04bc19{/SQL/json,null,AVAILABLE,@Spark}\n",
      "13:47:08.781 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6507af6f{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "13:47:08.782 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43fd4f69{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "13:47:08.794 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e474c9a{/static/sql,null,AVAILABLE,@Spark}\n",
      "13:47:09.855 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 81 ms to list leaf files for 1 paths.\n",
      "13:47:10.412 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:10.436 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:10.438 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:10.439 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:10.440 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:10.446 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:10.537 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "13:47:10.582 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "13:47:10.587 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.23.57.81:32937 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "13:47:10.591 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:10.615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:10.617 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "13:47:10.694 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "13:47:10.716 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:11.148 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3053 bytes result sent to driver\n",
      "13:47:11.160 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 491 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:11.162 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "13:47:11.167 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.704 s\n",
      "13:47:11.171 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:11.171 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "13:47:11.173 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.759995 s\n",
      "13:47:11.570 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.23.57.81:32937 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "13:47:12.046 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "13:47:12.099 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:12.101 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:12.101 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:12.102 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:12.102 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:12.104 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:12.116 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "13:47:12.123 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "13:47:12.124 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.23.57.81:32937 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "13:47:12.126 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:12.127 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:12.127 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "13:47:12.130 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "13:47:12.131 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n",
      "13:47:12.161 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1662 bytes result sent to driver\n",
      "13:47:12.164 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 35 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:12.165 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "13:47:12.166 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.060 s\n",
      "13:47:12.166 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:12.167 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "13:47:12.167 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.067779 s\n",
      "13:47:12.193 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 1 paths.\n",
      "13:47:12.215 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:32937 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "13:47:12.257 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:12.259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:12.259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:12.259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:12.260 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:12.261 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:12.273 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "13:47:12.277 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "13:47:12.280 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.23.57.81:32937 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "13:47:12.281 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:12.283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:12.283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "13:47:12.286 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9225 bytes) \n",
      "13:47:12.288 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "13:47:12.308 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1945 bytes result sent to driver\n",
      "13:47:12.311 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 25 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:12.311 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "13:47:12.312 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.050 s\n",
      "13:47:12.313 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:12.313 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "13:47:12.314 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.056340 s\n",
      "13:47:12.827 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:32937 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "13:47:12.857 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "13:47:12.858 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "13:47:13.413 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 279.923572 ms\n",
      "13:47:13.444 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)\n",
      "13:47:13.452 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "13:47:13.453 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.23.57.81:32937 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "13:47:13.455 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from count at NativeMethodAccessorImpl.java:0\n",
      "13:47:13.469 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:13.533 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "13:47:13.538 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "13:47:13.539 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "13:47:13.539 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:13.543 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:13.545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:13.603 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)\n",
      "13:47:13.610 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "13:47:13.611 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.23.57.81:32937 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "13:47:13.612 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:13.614 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "13:47:13.615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 8 tasks resource profile 0\n",
      "13:47:13.621 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.622 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 4) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.625 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 5) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.626 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 6) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.627 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 3.0 (TID 7) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.628 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 3.0 (TID 8) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.628 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 3.0 (TID 9) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.629 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 3.0 (TID 10) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:13.630 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)\n",
      "13:47:13.632 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 5)\n",
      "13:47:13.632 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 4)\n",
      "13:47:13.638 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 3.0 (TID 7)\n",
      "13:47:13.638 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 3.0 (TID 8)\n",
      "13:47:13.642 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 6)\n",
      "13:47:13.643 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 3.0 (TID 9)\n",
      "13:47:13.644 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 3.0 (TID 10)\n",
      "13:47:13.799 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.205484 ms\n",
      "13:47:13.818 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "13:47:13.818 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "13:47:13.820 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "13:47:13.821 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "13:47:13.826 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "13:47:13.826 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "13:47:13.828 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "13:47:13.830 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "13:47:14.084 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 3.0 (TID 10). 2222 bytes result sent to driver\n",
      "13:47:14.087 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 5). 2222 bytes result sent to driver\n",
      "13:47:14.089 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2222 bytes result sent to driver\n",
      "13:47:14.085 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 3.0 (TID 9). 2222 bytes result sent to driver\n",
      "13:47:14.090 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 6). 2222 bytes result sent to driver\n",
      "13:47:14.090 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 3.0 (TID 8). 2222 bytes result sent to driver\n",
      "13:47:14.089 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 3.0 (TID 7). 2222 bytes result sent to driver\n",
      "13:47:14.090 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 4). 2222 bytes result sent to driver\n",
      "13:47:14.098 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 482 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "13:47:14.099 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 3.0 (TID 9) in 471 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "13:47:14.099 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 5) in 475 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "13:47:14.102 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 3.0 (TID 7) in 475 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "13:47:14.103 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 3.0 (TID 8) in 476 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "13:47:14.104 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 6) in 478 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "13:47:14.104 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 3.0 (TID 10) in 475 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "13:47:14.105 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 4) in 482 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "13:47:14.105 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "13:47:14.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.563 s\n",
      "13:47:14.115 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:14.116 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:14.116 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:14.117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:14.187 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.171588 ms\n",
      "13:47:14.218 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "13:47:14.220 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:14.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "13:47:14.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)\n",
      "13:47:14.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:14.222 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:14.231 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
      "13:47:14.243 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "13:47:14.245 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.23.57.81:32937 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "13:47:14.246 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.23.57.81:32937 in memory (size: 7.8 KiB, free: 434.4 MiB)\n",
      "13:47:14.247 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:14.249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:14.249 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "13:47:14.259 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 11) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:14.261 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============>                                            (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:14.331 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:14.334 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms\n",
      "13:47:14.355 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.526386 ms\n",
      "13:47:14.386 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 11). 4081 bytes result sent to driver\n",
      "13:47:14.389 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 11) in 132 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:14.391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.162 s\n",
      "13:47:14.391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:14.392 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "13:47:14.392 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished\n",
      "13:47:14.394 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.175326 s\n",
      "13:47:14.443 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "13:47:14.443 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "13:47:14.471 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)\n",
      "13:47:14.481 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "13:47:14.483 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.23.57.81:32937 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "13:47:14.484 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 6 from count at NativeMethodAccessorImpl.java:0\n",
      "13:47:14.486 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:14.493 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "13:47:14.493 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:14.493 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "13:47:14.493 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:14.495 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:14.497 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:14.502 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "13:47:14.512 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "13:47:14.513 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.23.57.81:32937 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "13:47:14.515 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.23.57.81:32937 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "13:47:14.515 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:14.517 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:14.517 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0\n",
      "13:47:14.519 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 12) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:14.520 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 12)\n",
      "13:47:14.526 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:14.540 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 12). 2136 bytes result sent to driver\n",
      "13:47:14.544 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 12) in 26 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:14.545 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "13:47:14.546 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.048 s\n",
      "13:47:14.560 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:14.560 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:14.560 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:14.560 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:14.616 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "13:47:14.619 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:14.619 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\n",
      "13:47:14.619 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)\n",
      "13:47:14.620 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:14.621 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:14.625 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)\n",
      "13:47:14.638 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)\n",
      "13:47:14.639 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.23.57.81:32937 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "13:47:14.641 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.23.57.81:32937 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "13:47:14.641 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:14.642 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:14.642 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks resource profile 0\n",
      "13:47:14.645 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 13) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:14.648 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 13)\n",
      "13:47:14.653 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:14.653 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:14.660 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 13). 3995 bytes result sent to driver\n",
      "13:47:14.662 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 13) in 17 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:14.663 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "13:47:14.664 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.041 s\n",
      "13:47:14.665 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:14.665 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished\n",
      "13:47:14.666 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.049115 s\n",
      "13:47:14.712 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "13:47:14.712 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "13:47:14.735 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)\n",
      "13:47:14.756 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 172.23.57.81:32937 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "13:47:14.760 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)\n",
      "13:47:14.761 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.23.57.81:32937 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "13:47:14.763 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 9 from count at NativeMethodAccessorImpl.java:0\n",
      "13:47:14.765 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:14.774 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 23 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "13:47:14.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:14.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "13:47:14.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:14.779 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.23.57.81:32937 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "13:47:14.780 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:14.783 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:14.789 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "13:47:14.791 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "13:47:14.793 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.23.57.81:32937 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "13:47:14.794 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:14.795 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:14.795 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0\n",
      "13:47:14.816 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 14) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9684 bytes) \n",
      "13:47:14.818 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 14)\n",
      "13:47:14.826 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "13:47:14.853 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 14). 2222 bytes result sent to driver\n",
      "13:47:14.855 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 14) in 58 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:14.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.072 s\n",
      "13:47:14.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:14.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:14.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:14.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:14.858 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "13:47:14.924 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "13:47:14.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:14.927 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)\n",
      "13:47:14.927 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)\n",
      "13:47:14.927 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:14.929 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:14.933 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)\n",
      "13:47:14.935 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)\n",
      "13:47:14.937 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.23.57.81:32937 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "13:47:14.938 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:14.939 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:14.939 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0\n",
      "13:47:14.941 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 15) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:14.943 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 15)\n",
      "13:47:14.953 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:14.953 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:14.958 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 15). 3995 bytes result sent to driver\n",
      "13:47:14.960 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 15) in 19 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:14.961 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "13:47:14.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.031 s\n",
      "13:47:14.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:14.964 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished\n",
      "13:47:14.965 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.040409 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flight_row_count = df_flights.count()\n",
    "\n",
    "airline_row_count = df_airline.count()\n",
    "\n",
    "airports_row_count = df_airports.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1952a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #avg per fly\n",
    "    avg_flight = df_flights.join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                        .groupBy(df_flights.AIRLINE, df_airline.AIRLINE) \\\n",
    "                        .agg(round(avg(df_flights.DISTANCE), 2).alias(\"avg_DISTANCE\")) \\\n",
    "                        .orderBy(desc(\"avg_DISTANCE\")) \\\n",
    "                        .select(df_airline.AIRLINE.alias(\"AIRLINE_Name\"), \"avg_DISTANCE\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable avg_flight:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "237699b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #how many times a flight visit an airport\n",
    "    airline_in_airport = df_flights.join(broadcast(df_airports), df_flights.ORIGIN_AIRPORT == df_airports.IATA_CODE) \\\n",
    "                                .join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                                .repartition(df_flights.DESTINATION_AIRPORT) \\\n",
    "                                .groupBy(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE) \\\n",
    "                                    .agg(count(df_flights.AIRLINE).alias(\"Count_visit_per_airline\")) \\\n",
    "                                .orderBy(asc(df_flights.DESTINATION_AIRPORT)) \\\n",
    "                                .select(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE,\"Count_visit_per_airline\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airline_in_airport:\" + str(error))\n",
    "\n",
    "#a Rank of wich airline is the most visited in each airport\n",
    "try:\n",
    "    window_spec = Window.partitionBy(\"DESTINATION_AIRPORT\").orderBy(desc(\"Count_visit_per_airline\"))\n",
    "    \n",
    "    Rank_airline_in_airport = airline_in_airport.withColumn(\"Ranking\", rank().over(window_spec))\n",
    "\n",
    "\n",
    "except Exception as error:\n",
    "    logger.error(\"Error Rank_airline_in_airport:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad20a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #mention how many times an airline visit a destination and canceled this arrival\n",
    "    flights_per_cancell = df_flights.select(col(\"AIRLINE\"), col(\"CANCELLED\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .filter(col(\"CANCELLED\") == \"1\") \\\n",
    "                                .repartition(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .groupBy(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                    .agg(count(col(\"AIRLINE\")).alias(\"count_airlines_cancel\")) \\\n",
    "                                .orderBy(desc(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .limit(100)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable flights_per_cancell:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7929cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #i create this variable because i need to know how many airports have flights but are not in the list of airports\n",
    "    airport_notin_thelist = df_flights.join(broadcast(df_airports), df_flights.DESTINATION_AIRPORT == df_airports.IATA_CODE, \"left_anti\") \\\n",
    "                                        .select(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .repartition(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .groupBy(\"DESTINATION_AIRPORT\").agg(count(\"*\").alias(\"Count_airports\"))\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airport_notin_thelist:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7983760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:15.897 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "13:47:15.897 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "13:47:15.902 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "13:47:15.902 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "13:47:15.954 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 172.23.57.81:32937 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "13:47:15.959 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 172.23.57.81:32937 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "13:47:16.026 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.997595 ms\n",
      "13:47:16.036 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 200.3 KiB, free 433.7 MiB)\n",
      "13:47:16.047 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.7 MiB)\n",
      "13:47:16.049 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.23.57.81:32937 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "13:47:16.051 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:16.055 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:16.089 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:16.090 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "13:47:16.090 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "13:47:16.090 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:16.091 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:16.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "13:47:16.094 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 15.1 KiB, free 433.7 MiB)\n",
      "13:47:16.096 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.7 MiB)\n",
      "13:47:16.099 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.23.57.81:32937 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "13:47:16.100 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:16.100 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:16.100 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks resource profile 0\n",
      "13:47:16.102 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "13:47:16.104 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)\n",
      "13:47:16.130 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.55455 ms\n",
      "13:47:16.132 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "13:47:16.159 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "13:47:16.248 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "13:47:16.387 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.23.57.81:32937 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "13:47:16.428 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 2241 bytes result sent to driver\n",
      "13:47:16.429 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 327 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:16.429 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "13:47:16.430 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.337 s\n",
      "13:47:16.431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:16.431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished\n",
      "13:47:16.431 [broadcast-exchange-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.341721 s\n",
      "13:47:16.453 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.36649 ms\n",
      "13:47:16.459 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 2.0 MiB, free 431.9 MiB)\n",
      "13:47:16.464 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 686.0 B, free 431.9 MiB)\n",
      "13:47:16.465 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.23.57.81:32937 (size: 686.0 B, free: 434.3 MiB)\n",
      "13:47:16.465 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:16.479 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "13:47:16.479 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "13:47:16.654 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 83.35629 ms\n",
      "13:47:16.659 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 200.3 KiB, free 431.7 MiB)\n",
      "13:47:16.667 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.7 MiB)\n",
      "13:47:16.668 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 172.23.57.81:32937 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "13:47:16.670 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 15 from parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:16.671 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:16.740 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 34 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "13:47:16.741 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 10 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "13:47:16.741 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:16.741 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:16.743 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:16.745 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:16.752 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 53.5 KiB, free 431.6 MiB)\n",
      "13:47:16.763 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 431.6 MiB)\n",
      "13:47:16.766 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 172.23.57.81:32937 (size: 23.2 KiB, free: 434.3 MiB)\n",
      "13:47:16.766 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:16.767 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "13:47:16.767 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 8 tasks resource profile 0\n",
      "13:47:16.769 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 17) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.770 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 172.23.57.81:32937 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "13:47:16.770 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 18) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.770 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 13.0 (TID 19) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.771 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 13.0 (TID 20) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.771 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 13.0 (TID 21) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.771 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 13.0 (TID 22) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.772 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 13.0 (TID 23) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.772 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 13.0 (TID 24) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:16.773 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 13.0 (TID 22)\n",
      "13:47:16.774 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 13.0 (TID 23)\n",
      "13:47:16.775 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 13.0 (TID 19)\n",
      "13:47:16.775 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 13.0 (TID 24)\n",
      "13:47:16.776 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 17)\n",
      "13:47:16.779 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 13.0 (TID 21)\n",
      "13:47:16.779 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 18)\n",
      "13:47:16.780 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 13.0 (TID 20)\n",
      "13:47:16.836 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.23.57.81:32937 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "13:47:16.903 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 98.24157 ms\n",
      "13:47:16.929 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.71447 ms\n",
      "13:47:16.945 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.18693 ms\n",
      "13:47:16.995 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.52932 ms\n",
      "13:47:17.030 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.21297 ms\n",
      "13:47:17.041 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "13:47:17.044 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "13:47:17.045 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "13:47:17.045 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "13:47:17.046 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "13:47:17.049 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "13:47:17.052 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "13:47:17.053 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "13:47:17.054 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.056 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.061 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.062 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.063 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.070 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.092 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.100 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "13:47:17.102 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "13:47:17.104 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "13:47:17.105 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "13:47:17.107 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "13:47:17.108 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "13:47:17.109 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "13:47:17.129 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:18.577 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 13.0 (TID 23). 3926 bytes result sent to driver\n",
      "13:47:18.582 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 13.0 (TID 23) in 1810 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "13:47:18.583 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 17). 3926 bytes result sent to driver\n",
      "13:47:18.584 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 13.0 (TID 24). 3926 bytes result sent to driver\n",
      "13:47:18.587 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 17) in 1818 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "13:47:18.589 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 13.0 (TID 24) in 1817 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "13:47:18.596 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 13.0 (TID 20). 3926 bytes result sent to driver\n",
      "13:47:18.597 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 13.0 (TID 20) in 1826 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "13:47:18.612 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 13.0 (TID 22). 3926 bytes result sent to driver\n",
      "13:47:18.613 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 13.0 (TID 22) in 1842 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "13:47:18.617 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 13.0 (TID 19). 3926 bytes result sent to driver\n",
      "13:47:18.620 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 13.0 (TID 19) in 1850 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "13:47:18.621 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 13.0 (TID 21). 3926 bytes result sent to driver\n",
      "13:47:18.622 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 13.0 (TID 21) in 1851 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "13:47:18.627 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 13.0 (TID 18). 3926 bytes result sent to driver\n",
      "13:47:18.628 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 13.0 (TID 18) in 1858 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "13:47:18.628 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "13:47:18.629 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.882 s\n",
      "13:47:18.629 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:18.629 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:18.630 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:18.630 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:18.649 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "13:47:18.675 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "13:47:18.718 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.04118 ms\n",
      "13:47:18.760 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.47262 ms\n",
      "13:47:18.804 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:18.807 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:18.807 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:18.807 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)\n",
      "13:47:18.807 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:18.809 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:18.818 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 54.3 KiB, free 431.8 MiB)\n",
      "13:47:18.820 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 431.8 MiB)\n",
      "13:47:18.821 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 172.23.57.81:32937 (size: 24.1 KiB, free: 434.3 MiB)\n",
      "13:47:18.822 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:18.822 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:18.822 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0\n",
      "13:47:18.824 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 25) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:18.825 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 25)\n",
      "13:47:18.842 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:18.842 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "13:47:18.867 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.88646 ms\n",
      "13:47:18.878 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.64775 ms\n",
      "13:47:18.894 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 25). 6848 bytes result sent to driver\n",
      "13:47:18.895 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 25) in 71 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:18.895 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "13:47:18.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.081 s\n",
      "13:47:18.897 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:18.897 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished\n",
      "13:47:18.897 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.092511 s\n",
      "13:47:18.907 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 40 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "13:47:18.907 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:18.907 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:18.907 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)\n",
      "13:47:18.908 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:18.909 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:18.923 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 54.7 KiB, free 431.7 MiB)\n",
      "13:47:18.925 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 431.7 MiB)\n",
      "13:47:18.925 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 172.23.57.81:32937 (size: 24.3 KiB, free: 434.3 MiB)\n",
      "13:47:18.926 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:18.927 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:18.927 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks resource profile 0\n",
      "13:47:18.928 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 26) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "13:47:18.929 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 26)\n",
      "13:47:18.949 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.3833 ms\n",
      "13:47:18.955 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:18.955 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:18.974 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 26). 6433 bytes result sent to driver\n",
      "13:47:18.975 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 26) in 47 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:18.975 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "13:47:18.976 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.061 s\n",
      "13:47:18.977 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:18.977 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:18.977 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:18.977 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:18.983 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:19.030 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:19.043 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:19.044 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:19.044 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:19.045 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:19.045 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:19.045 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:19.096 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.76077 ms\n",
      "13:47:19.112 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:19.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:19.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:19.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)\n",
      "13:47:19.116 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:19.117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:19.145 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 250.3 KiB, free 431.5 MiB)\n",
      "13:47:19.155 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 431.4 MiB)\n",
      "13:47:19.156 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 172.23.57.81:32937 (size: 93.3 KiB, free: 434.2 MiB)\n",
      "13:47:19.157 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:19.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:19.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks resource profile 0\n",
      "13:47:19.160 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 172.23.57.81:32937 in memory (size: 24.3 KiB, free: 434.2 MiB)\n",
      "13:47:19.160 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 27) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:19.161 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 27)\n",
      "13:47:19.181 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 172.23.57.81:32937 in memory (size: 24.1 KiB, free: 434.2 MiB)\n",
      "13:47:19.215 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 172.23.57.81:32937 in memory (size: 23.2 KiB, free: 434.2 MiB)\n",
      "13:47:19.227 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1323.0 B) non-empty blocks including 1 (1323.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:19.227 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:19.248 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.0484 ms\n",
      "13:47:19.265 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.92453 ms\n",
      "13:47:19.294 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.86402 ms\n",
      "13:47:19.304 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:19.305 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:19.305 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:19.306 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:19.306 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:19.306 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:19.311 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:19.315 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:19.343 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "13:47:19.354 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE_Name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_DISTANCE\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE_Name (STRING);\n",
      "  optional double avg_DISTANCE;\n",
      "}\n",
      "\n",
      "       \n",
      "13:47:19.395 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]\n",
      "13:47:19.491 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131347196151372508443783029_0020_m_000000_27' to file:/home/illidan/proyecto_desde0/archivos_parquet/avg_flight/_temporary/0/task_202506131347196151372508443783029_0020_m_000000\n",
      "13:47:19.492 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131347196151372508443783029_0020_m_000000_27: Committed. Elapsed time: 1 ms.\n",
      "13:47:19.500 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 27). 8767 bytes result sent to driver\n",
      "13:47:19.502 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 27) in 342 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:19.503 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "13:47:19.504 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.384 s\n",
      "13:47:19.505 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:19.505 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished\n",
      "13:47:19.506 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.394415 s\n",
      "13:47:19.508 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job b58aef8a-4485-4f50-b35d-d65090a79bb3.\n",
      "13:47:19.536 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job b58aef8a-4485-4f50-b35d-d65090a79bb3 committed. Elapsed time: 27 ms.\n",
      "13:47:19.540 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job b58aef8a-4485-4f50-b35d-d65090a79bb3.\n",
      "13:47:19.629 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "13:47:19.630 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "13:47:19.632 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "13:47:19.632 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "13:47:19.633 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "13:47:19.634 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "13:47:19.695 [broadcast-exchange-1] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.8323 ms\n",
      "13:47:19.695 [broadcast-exchange-2] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.6587 ms\n",
      "13:47:19.702 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 200.2 KiB, free 431.4 MiB)\n",
      "13:47:19.704 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 200.3 KiB, free 431.2 MiB)\n",
      "13:47:19.716 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.2 MiB)\n",
      "13:47:19.716 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 172.23.57.81:32937 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "13:47:19.717 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.1 MiB)\n",
      "13:47:19.717 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:19.718 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 172.23.57.81:32937 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "13:47:19.718 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:19.719 [broadcast-exchange-1] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:19.719 [broadcast-exchange-2] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:19.740 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:19.740 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:19.741 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "13:47:19.742 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "13:47:19.742 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:19.742 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:19.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "13:47:19.746 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 15.1 KiB, free 431.1 MiB)\n",
      "13:47:19.748 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 431.1 MiB)\n",
      "13:47:19.748 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 172.23.57.81:32937 (size: 6.5 KiB, free: 434.2 MiB)\n",
      "13:47:19.749 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:19.749 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:19.749 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks resource profile 0\n",
      "13:47:19.750 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "13:47:19.751 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "13:47:19.751 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 28) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "13:47:19.751 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:19.751 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:19.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "13:47:19.752 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 28)\n",
      "13:47:19.754 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 14.5 KiB, free 431.1 MiB)\n",
      "13:47:19.755 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 431.1 MiB)\n",
      "13:47:19.756 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 172.23.57.81:32937 (size: 6.3 KiB, free: 434.2 MiB)\n",
      "13:47:19.757 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:19.759 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:19.759 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 1 tasks resource profile 0\n",
      "13:47:19.761 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 29) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "13:47:19.762 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 29)\n",
      "13:47:19.775 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.63606 ms\n",
      "13:47:19.776 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "13:47:19.777 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.08269 ms\n",
      "13:47:19.779 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "13:47:19.781 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "13:47:19.783 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "13:47:19.787 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 28). 2198 bytes result sent to driver\n",
      "13:47:19.789 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 28) in 39 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:19.789 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "13:47:19.790 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.045 s\n",
      "13:47:19.790 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 29). 3613 bytes result sent to driver\n",
      "13:47:19.790 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:19.791 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 21: Stage finished\n",
      "13:47:19.791 [broadcast-exchange-2] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.050567 s\n",
      "13:47:19.792 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 29) in 31 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:19.792 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "13:47:19.793 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.040 s\n",
      "13:47:19.793 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:19.793 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished\n",
      "13:47:19.795 [broadcast-exchange-1] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.054301 s\n",
      "13:47:19.796 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 2.0 MiB, free 429.1 MiB)\n",
      "13:47:19.798 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 686.0 B, free 429.1 MiB)\n",
      "13:47:19.798 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 172.23.57.81:32937 (size: 686.0 B, free: 434.2 MiB)\n",
      "13:47:19.799 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 24 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:19.803 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 2.0 MiB, free 427.1 MiB)\n",
      "13:47:19.806 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 427.1 MiB)\n",
      "13:47:19.806 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 172.23.57.81:32937 (size: 4.9 KiB, free: 434.2 MiB)\n",
      "13:47:19.807 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:19.807 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "13:47:19.808 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "13:47:19.820 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "13:47:19.820 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "13:47:19.871 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.46388 ms\n",
      "13:47:19.876 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 200.5 KiB, free 426.9 MiB)\n",
      "13:47:19.885 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 426.9 MiB)\n",
      "13:47:19.886 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 172.23.57.81:32937 (size: 34.7 KiB, free: 434.1 MiB)\n",
      "13:47:19.887 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 26 from parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:19.888 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:19.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 55 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "13:47:19.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 16 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "13:47:19.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:19.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:19.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:19.898 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:19.904 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 22.1 KiB, free 426.8 MiB)\n",
      "13:47:19.906 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 426.8 MiB)\n",
      "13:47:19.907 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 172.23.57.81:32937 (size: 9.0 KiB, free: 434.1 MiB)\n",
      "13:47:19.908 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:19.909 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "13:47:19.909 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 8 tasks resource profile 0\n",
      "13:47:19.912 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 30) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.912 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 31) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.913 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 32) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.913 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 33) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.914 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 23.0 (TID 34) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.915 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 23.0 (TID 35) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.915 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 23.0 (TID 36) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.916 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 23.0 (TID 37) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:19.916 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 30)\n",
      "13:47:19.916 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 31)\n",
      "13:47:19.916 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 23.0 (TID 35)\n",
      "13:47:19.917 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 33)\n",
      "13:47:19.917 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 23.0 (TID 36)\n",
      "13:47:19.917 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 23.0 (TID 34)\n",
      "13:47:19.919 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 23.0 (TID 37)\n",
      "13:47:19.916 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 32)\n",
      "13:47:19.945 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.90123 ms\n",
      "13:47:19.961 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.46321 ms\n",
      "13:47:19.963 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "13:47:19.963 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "13:47:19.963 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "13:47:19.963 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "13:47:19.963 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "13:47:19.963 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "13:47:19.963 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "13:47:19.963 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "13:47:19.974 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:19.975 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:19.976 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:19.977 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:19.979 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:19.987 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:19.989 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:19.992 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "13:47:20.180 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 172.23.57.81:32937 in memory (size: 6.5 KiB, free: 434.1 MiB)\n",
      "13:47:20.192 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 172.23.57.81:32937 in memory (size: 6.3 KiB, free: 434.1 MiB)\n",
      "13:47:20.216 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 172.23.57.81:32937 in memory (size: 93.3 KiB, free: 434.2 MiB)\n",
      "13:47:20.570 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 172.23.57.81:32937 in memory (size: 34.6 KiB, free: 434.2 MiB)\n",
      "13:47:20.587 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 172.23.57.81:32937 in memory (size: 686.0 B, free: 434.3 MiB)\n",
      "13:47:20.613 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 172.23.57.81:32937 in memory (size: 34.6 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:20.967 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 23.0 (TID 36). 2494 bytes result sent to driver\n",
      "13:47:20.980 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 23.0 (TID 36) in 1065 ms on 172.23.57.81 (executor driver) (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:21.579 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 23.0 (TID 37). 2494 bytes result sent to driver\n",
      "13:47:21.590 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 23.0 (TID 37) in 1674 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "13:47:21.633 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 23.0 (TID 35). 2451 bytes result sent to driver\n",
      "13:47:21.635 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 23.0 (TID 35) in 1720 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "13:47:21.717 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 31). 2494 bytes result sent to driver\n",
      "13:47:21.718 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 31) in 1806 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "13:47:21.727 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 33). 2451 bytes result sent to driver\n",
      "13:47:21.728 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 33) in 1815 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "13:47:21.758 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 32). 2451 bytes result sent to driver\n",
      "13:47:21.759 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 32) in 1846 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "13:47:21.765 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 23.0 (TID 34). 2451 bytes result sent to driver\n",
      "13:47:21.766 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 23.0 (TID 34) in 1851 ms on 172.23.57.81 (executor driver) (7/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:21.814 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 30). 2451 bytes result sent to driver\n",
      "13:47:21.816 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 30) in 1905 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "13:47:21.816 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "13:47:21.816 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.917 s\n",
      "13:47:21.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:21.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:21.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:21.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:21.824 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(5), advisory target size: 67108864, actual target size 3493000, minimum partition size: 1048576\n",
      "13:47:21.837 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:21.839 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:21.839 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:21.839 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:21.839 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:21.839 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:21.839 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:21.870 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.07338 ms\n",
      "13:47:21.871 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "13:47:21.914 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.26245 ms\n",
      "13:47:21.939 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:21.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 59 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "13:47:21.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 17 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:21.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 26 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:21.942 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 25)\n",
      "13:47:21.942 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 25)\n",
      "13:47:21.943 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 25 (MapPartitionsRDD[59] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:21.959 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 63.5 KiB, free 429.6 MiB)\n",
      "13:47:21.960 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 429.6 MiB)\n",
      "13:47:21.961 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 172.23.57.81:32937 (size: 26.3 KiB, free: 434.3 MiB)\n",
      "13:47:21.961 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:21.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[59] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "13:47:21.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 10 tasks resource profile 0\n",
      "13:47:21.965 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 38) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.966 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 25.0 (TID 39) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.966 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 25.0 (TID 40) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.967 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 25.0 (TID 41) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.967 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 25.0 (TID 42) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.967 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 25.0 (TID 43) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.968 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 25.0 (TID 44) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.968 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 25.0 (TID 45) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8988 bytes) \n",
      "13:47:21.969 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 38)\n",
      "13:47:21.969 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 25.0 (TID 41)\n",
      "13:47:21.969 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 25.0 (TID 42)\n",
      "13:47:21.969 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 25.0 (TID 45)\n",
      "13:47:21.969 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 25.0 (TID 43)\n",
      "13:47:21.969 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 25.0 (TID 40)\n",
      "13:47:21.969 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 25.0 (TID 39)\n",
      "13:47:21.970 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 25.0 (TID 44)\n",
      "13:47:21.986 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.83397 ms\n",
      "13:47:22.002 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.002 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:22.002 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.002 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "13:47:22.002 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.002 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:22.003 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.003 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:22.005 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.005 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:22.007 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.008 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:22.008 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.008 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "13:47:22.008 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.008 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "13:47:22.049 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 45.35829 ms\n",
      "13:47:22.068 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.22718 ms\n",
      "13:47:22.080 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.35876 ms\n",
      "13:47:22.471 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 172.23.57.81:32937 in memory (size: 9.0 KiB, free: 434.3 MiB)\n",
      "13:47:22.502 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 25.0 (TID 45). 8199 bytes result sent to driver\n",
      "13:47:22.508 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 25.0 (TID 46) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8988 bytes) \n",
      "13:47:22.509 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 25.0 (TID 45) in 541 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "13:47:22.511 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 25.0 (TID 46)\n",
      "13:47:22.525 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.531 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms\n",
      "13:47:22.635 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 25.0 (TID 43). 8199 bytes result sent to driver\n",
      "13:47:22.639 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 25.0 (TID 47) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8988 bytes) \n",
      "13:47:22.641 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 25.0 (TID 43) in 674 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "13:47:22.643 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 25.0 (TID 47)\n",
      "13:47:22.660 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.660 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:22.677 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 25.0 (TID 44). 8199 bytes result sent to driver\n",
      "13:47:22.679 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 25.0 (TID 44) in 712 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "13:47:22.699 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 25.0 (TID 39). 8199 bytes result sent to driver\n",
      "13:47:22.703 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 25.0 (TID 39) in 737 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "13:47:22.716 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 25.0 (TID 40). 8199 bytes result sent to driver\n",
      "13:47:22.718 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 25.0 (TID 40) in 752 ms on 172.23.57.81 (executor driver) (5/10)\n",
      "13:47:22.722 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 38). 8199 bytes result sent to driver\n",
      "13:47:22.723 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 38) in 758 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "13:47:22.745 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 25.0 (TID 41). 8199 bytes result sent to driver\n",
      "13:47:22.746 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 25.0 (TID 41) in 780 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "13:47:22.761 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 25.0 (TID 42). 8199 bytes result sent to driver\n",
      "13:47:22.762 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 25.0 (TID 42) in 795 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "13:47:22.776 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 25.0 (TID 46). 8199 bytes result sent to driver\n",
      "13:47:22.777 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 25.0 (TID 46) in 269 ms on 172.23.57.81 (executor driver) (9/10)\n",
      "13:47:22.820 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 25.0 (TID 47). 8199 bytes result sent to driver\n",
      "13:47:22.821 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 25.0 (TID 47) in 183 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "13:47:22.822 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "13:47:22.822 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 25 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.868 s\n",
      "13:47:22.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:22.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:22.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 26)\n",
      "13:47:22.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:22.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 26 (MapPartitionsRDD[62] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:22.873 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 244.8 KiB, free 429.4 MiB)\n",
      "13:47:22.876 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 91.3 KiB, free 429.3 MiB)\n",
      "13:47:22.876 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 172.23.57.81:32937 (size: 91.3 KiB, free: 434.2 MiB)\n",
      "13:47:22.877 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:22.878 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[62] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:22.878 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 26.0 with 1 tasks resource profile 0\n",
      "13:47:22.880 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 26.0 (TID 48) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:22.881 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 26.0 (TID 48)\n",
      "13:47:22.916 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 10 (16.9 KiB) non-empty blocks including 10 (16.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:22.916 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "13:47:22.921 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:22.930 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:22.931 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:22.931 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:22.931 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:22.931 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:22.931 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:22.932 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:22.934 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "13:47:22.936 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_visit_per_airline\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  optional binary AIRLINE (STRING);\n",
      "  required int64 Count_visit_per_airline;\n",
      "}\n",
      "\n",
      "       \n",
      "13:47:22.996 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131347217384814616476563601_0026_m_000000_48' to file:/home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/_temporary/0/task_202506131347217384814616476563601_0026_m_000000\n",
      "13:47:22.996 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131347217384814616476563601_0026_m_000000_48: Committed. Elapsed time: 0 ms.\n",
      "13:47:22.998 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 26.0 (TID 48). 9172 bytes result sent to driver\n",
      "13:47:22.999 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 26.0 (TID 48) in 119 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:22.999 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "13:47:23.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 26 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.175 s\n",
      "13:47:23.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:23.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 26: Stage finished\n",
      "13:47:23.001 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.060922 s\n",
      "13:47:23.002 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 47feb1ed-c522-4c81-b145-a1bd42d37e36.\n",
      "13:47:23.022 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 47feb1ed-c522-4c81-b145-a1bd42d37e36 committed. Elapsed time: 19 ms.\n",
      "13:47:23.022 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 47feb1ed-c522-4c81-b145-a1bd42d37e36.\n",
      "13:47:23.066 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(CANCELLED),EqualTo(CANCELLED,1)\n",
      "13:47:23.066 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(CANCELLED#24),(CANCELLED#24 = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:23.109 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.62907 ms\n",
      "13:47:23.114 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 200.5 KiB, free 429.1 MiB)\n",
      "13:47:23.124 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 429.0 MiB)\n",
      "13:47:23.125 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 172.23.57.81:32937 (size: 34.8 KiB, free: 434.1 MiB)\n",
      "13:47:23.126 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 30 from parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:23.127 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:23.131 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 66 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "13:47:23.131 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 18 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "13:47:23.131 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:23.131 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:23.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:23.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 27 (MapPartitionsRDD[66] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:23.136 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 18.8 KiB, free 429.0 MiB)\n",
      "13:47:23.137 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 429.0 MiB)\n",
      "13:47:23.138 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 172.23.57.81:32937 (size: 8.2 KiB, free: 434.1 MiB)\n",
      "13:47:23.138 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:23.143 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[66] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "13:47:23.143 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 8 tasks resource profile 0\n",
      "13:47:23.145 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 49) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.145 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 50) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.145 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 51) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.146 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 27.0 (TID 52) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.146 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 27.0 (TID 53) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.146 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 27.0 (TID 54) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.147 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 27.0 (TID 55) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.147 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 27.0 (TID 56) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:23.148 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 49)\n",
      "13:47:23.148 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 50)\n",
      "13:47:23.148 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 27.0 (TID 52)\n",
      "13:47:23.149 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 27.0 (TID 55)\n",
      "13:47:23.148 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 27.0 (TID 54)\n",
      "13:47:23.150 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 27.0 (TID 56)\n",
      "13:47:23.150 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 27.0 (TID 53)\n",
      "13:47:23.151 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 51)\n",
      "13:47:23.165 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.4767 ms\n",
      "13:47:23.169 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "13:47:23.169 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "13:47:23.169 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "13:47:23.169 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "13:47:23.170 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "13:47:23.170 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "13:47:23.170 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "13:47:23.178 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "13:47:23.184 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.184 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.185 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.187 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.189 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.192 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.193 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.198 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "13:47:23.512 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 27.0 (TID 53). 2296 bytes result sent to driver\n",
      "13:47:23.515 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 27.0 (TID 53) in 369 ms on 172.23.57.81 (executor driver) (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:23.834 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 27.0 (TID 55). 2296 bytes result sent to driver\n",
      "13:47:23.839 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 27.0 (TID 55) in 693 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "13:47:24.009 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 50). 2296 bytes result sent to driver\n",
      "13:47:24.010 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 50) in 865 ms on 172.23.57.81 (executor driver) (3/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:24.108 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 51). 2296 bytes result sent to driver\n",
      "13:47:24.110 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 51) in 965 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "13:47:24.153 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 27.0 (TID 52). 2296 bytes result sent to driver\n",
      "13:47:24.154 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 27.0 (TID 52) in 1008 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "13:47:24.155 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 49). 2296 bytes result sent to driver\n",
      "13:47:24.155 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 27.0 (TID 54). 2296 bytes result sent to driver\n",
      "13:47:24.159 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 27.0 (TID 54) in 1013 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "13:47:24.159 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 49) in 1015 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "13:47:24.159 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 27.0 (TID 56). 2296 bytes result sent to driver\n",
      "13:47:24.160 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 27.0 (TID 56) in 1013 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "13:47:24.160 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "13:47:24.161 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.028 s\n",
      "13:47:24.161 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:24.161 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:24.161 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:24.161 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:24.168 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "13:47:24.178 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:24.180 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:24.180 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:24.180 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:24.180 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:24.180 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:24.181 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:24.215 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.83893 ms\n",
      "13:47:24.217 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "13:47:24.260 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 36.29628 ms\n",
      "13:47:24.277 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:24.279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 19 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:24.279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:24.279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 28)\n",
      "13:47:24.279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:24.280 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[70] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:24.303 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 254.2 KiB, free 428.8 MiB)\n",
      "13:47:24.305 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 93.4 KiB, free 428.7 MiB)\n",
      "13:47:24.306 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 172.23.57.81:32937 (size: 93.4 KiB, free: 434.0 MiB)\n",
      "13:47:24.307 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:24.308 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[70] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:24.308 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 1 tasks resource profile 0\n",
      "13:47:24.310 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 57) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:24.311 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 57)\n",
      "13:47:24.325 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.36938 ms\n",
      "13:47:24.348 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (350.4 KiB) non-empty blocks including 8 (350.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:24.349 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms\n",
      "13:47:24.388 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 36.85977 ms\n",
      "13:47:24.476 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:24.476 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:24.477 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:24.477 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:24.477 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:24.477 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:24.477 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:24.478 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:24.480 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "13:47:24.480 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"count_airlines_cancel\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE (STRING);\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 count_airlines_cancel;\n",
      "}\n",
      "\n",
      "       \n",
      "13:47:24.532 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131347241623060087806177686_0029_m_000000_57' to file:/home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/_temporary/0/task_202506131347241623060087806177686_0029_m_000000\n",
      "13:47:24.532 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131347241623060087806177686_0029_m_000000_57: Committed. Elapsed time: 1 ms.\n",
      "13:47:24.535 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 57). 7478 bytes result sent to driver\n",
      "13:47:24.536 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 57) in 227 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:24.536 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "13:47:24.537 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.251 s\n",
      "13:47:24.537 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:24.537 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 29: Stage finished\n",
      "13:47:24.538 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.260227 s\n",
      "13:47:24.539 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 7ed4f283-318b-47d3-849f-ab766739fc10.\n",
      "13:47:24.557 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 7ed4f283-318b-47d3-849f-ab766739fc10 committed. Elapsed time: 17 ms.\n",
      "13:47:24.557 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 7ed4f283-318b-47d3-849f-ab766739fc10.\n",
      "13:47:24.595 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "13:47:24.595 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "13:47:24.596 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "13:47:24.596 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "13:47:24.629 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 200.2 KiB, free 428.5 MiB)\n",
      "13:47:24.636 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 428.4 MiB)\n",
      "13:47:24.637 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on 172.23.57.81:32937 (size: 34.6 KiB, free: 434.0 MiB)\n",
      "13:47:24.638 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:24.639 [broadcast-exchange-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "13:47:24.650 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:24.651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "13:47:24.651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "13:47:24.651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:24.651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:24.651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 30 (MapPartitionsRDD[74] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "13:47:24.654 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34 stored as values in memory (estimated size 14.5 KiB, free 428.4 MiB)\n",
      "13:47:24.655 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 428.4 MiB)\n",
      "13:47:24.655 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_34_piece0 in memory on 172.23.57.81:32937 (size: 6.3 KiB, free: 434.0 MiB)\n",
      "13:47:24.656 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:24.657 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[74] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:24.657 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 30.0 with 1 tasks resource profile 0\n",
      "13:47:24.658 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 30.0 (TID 58) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "13:47:24.659 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 30.0 (TID 58)\n",
      "13:47:24.663 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "13:47:24.668 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "13:47:24.672 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 30.0 (TID 58). 3613 bytes result sent to driver\n",
      "13:47:24.673 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 30.0 (TID 58) in 15 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:24.673 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "13:47:24.674 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.022 s\n",
      "13:47:24.674 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:24.674 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 30: Stage finished\n",
      "13:47:24.674 [broadcast-exchange-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.024425 s\n",
      "13:47:24.680 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35 stored as values in memory (estimated size 2.0 MiB, free 426.4 MiB)\n",
      "13:47:24.682 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 426.4 MiB)\n",
      "13:47:24.682 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_35_piece0 in memory on 172.23.57.81:32937 (size: 4.9 KiB, free: 434.0 MiB)\n",
      "13:47:24.683 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "13:47:24.688 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "13:47:24.689 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "13:47:24.715 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.05319 ms\n",
      "13:47:24.717 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36 stored as values in memory (estimated size 200.2 KiB, free 426.2 MiB)\n",
      "13:47:24.726 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 426.2 MiB)\n",
      "13:47:24.727 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_36_piece0 in memory on 172.23.57.81:32937 (size: 34.5 KiB, free: 434.0 MiB)\n",
      "13:47:24.728 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 36 from parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:24.729 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:24.735 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 78 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "13:47:24.735 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 21 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "13:47:24.735 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:24.735 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "13:47:24.735 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:24.736 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 31 (MapPartitionsRDD[78] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:24.740 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37 stored as values in memory (estimated size 18.9 KiB, free 426.2 MiB)\n",
      "13:47:24.742 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 426.2 MiB)\n",
      "13:47:24.743 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_37_piece0 in memory on 172.23.57.81:32937 (size: 8.4 KiB, free: 434.0 MiB)\n",
      "13:47:24.744 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 37 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:24.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[78] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "13:47:24.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 31.0 with 8 tasks resource profile 0\n",
      "13:47:24.746 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 31.0 (TID 59) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.746 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 31.0 (TID 60) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.746 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 31.0 (TID 61) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.747 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 31.0 (TID 62) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.747 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 31.0 (TID 63) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.747 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 31.0 (TID 64) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.749 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 31.0 (TID 65) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.749 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 31.0 (TID 66) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "13:47:24.749 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 31.0 (TID 59)\n",
      "13:47:24.750 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 31.0 (TID 60)\n",
      "13:47:24.750 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 31.0 (TID 62)\n",
      "13:47:24.750 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 31.0 (TID 61)\n",
      "13:47:24.751 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 31.0 (TID 63)\n",
      "13:47:24.751 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 31.0 (TID 64)\n",
      "13:47:24.752 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 31.0 (TID 65)\n",
      "13:47:24.751 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 31.0 (TID 66)\n",
      "13:47:24.773 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.73281 ms\n",
      "13:47:24.785 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.8578 ms\n",
      "13:47:24.787 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "13:47:24.787 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "13:47:24.787 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "13:47:24.787 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "13:47:24.787 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "13:47:24.787 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "13:47:24.787 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "13:47:24.787 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "13:47:24.885 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_32_piece0 on 172.23.57.81:32937 in memory (size: 93.4 KiB, free: 434.0 MiB)\n",
      "13:47:24.932 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_34_piece0 on 172.23.57.81:32937 in memory (size: 6.3 KiB, free: 434.1 MiB)\n",
      "13:47:25.160 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 31.0 (TID 61). 2210 bytes result sent to driver\n",
      "13:47:25.165 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 31.0 (TID 61) in 419 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "13:47:25.267 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 31.0 (TID 66). 2210 bytes result sent to driver\n",
      "13:47:25.271 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 31.0 (TID 66) in 522 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "13:47:25.311 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 31.0 (TID 62). 2210 bytes result sent to driver\n",
      "13:47:25.312 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 31.0 (TID 62) in 565 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "13:47:25.324 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 31.0 (TID 60). 2210 bytes result sent to driver\n",
      "13:47:25.325 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 31.0 (TID 63). 2210 bytes result sent to driver\n",
      "13:47:25.326 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 31.0 (TID 60) in 580 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "13:47:25.328 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 31.0 (TID 63) in 581 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "13:47:25.343 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 31.0 (TID 59). 2210 bytes result sent to driver\n",
      "13:47:25.344 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 31.0 (TID 59) in 599 ms on 172.23.57.81 (executor driver) (6/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:47:25.422 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 31.0 (TID 64). 2339 bytes result sent to driver\n",
      "13:47:25.423 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 31.0 (TID 64) in 676 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "13:47:25.484 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 31.0 (TID 65). 2339 bytes result sent to driver\n",
      "13:47:25.485 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 31.0 (TID 65) in 737 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "13:47:25.485 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "13:47:25.485 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.748 s\n",
      "13:47:25.486 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "13:47:25.486 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "13:47:25.486 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "13:47:25.486 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "13:47:25.492 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "13:47:25.505 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:25.506 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:25.506 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:25.507 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:25.507 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:25.507 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:25.507 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:25.539 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "13:47:25.578 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.44679 ms\n",
      "13:47:25.590 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "13:47:25.592 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 22 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "13:47:25.592 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "13:47:25.592 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 32)\n",
      "13:47:25.593 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "13:47:25.593 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 33 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "13:47:25.613 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38 stored as values in memory (estimated size 251.0 KiB, free 426.3 MiB)\n",
      "13:47:25.615 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38_piece0 stored as bytes in memory (estimated size 92.4 KiB, free 426.2 MiB)\n",
      "13:47:25.616 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_38_piece0 in memory on 172.23.57.81:32937 (size: 92.4 KiB, free: 434.0 MiB)\n",
      "13:47:25.617 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 38 from broadcast at DAGScheduler.scala:1585\n",
      "13:47:25.619 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "13:47:25.619 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 33.0 with 1 tasks resource profile 0\n",
      "13:47:25.621 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 33.0 (TID 67) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "13:47:25.621 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 33.0 (TID 67)\n",
      "13:47:25.638 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (457.0 KiB) non-empty blocks including 2 (457.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "13:47:25.639 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "13:47:25.675 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.89461 ms\n",
      "13:47:25.677 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:25.677 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:25.678 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:25.678 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "13:47:25.678 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "13:47:25.678 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "13:47:25.678 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:25.679 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "13:47:25.681 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "13:47:25.682 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_airports\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 Count_airports;\n",
      "}\n",
      "\n",
      "       \n",
      "13:47:25.754 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.21343 ms\n",
      "13:47:25.911 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131347253384930417681109208_0033_m_000000_67' to file:/home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/_temporary/0/task_202506131347253384930417681109208_0033_m_000000\n",
      "13:47:25.911 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131347253384930417681109208_0033_m_000000_67: Committed. Elapsed time: 0 ms.\n",
      "13:47:25.912 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 33.0 (TID 67). 7199 bytes result sent to driver\n",
      "13:47:25.913 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 33.0 (TID 67) in 293 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "13:47:25.913 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "13:47:25.914 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.317 s\n",
      "13:47:25.914 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "13:47:25.915 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 33: Stage finished\n",
      "13:47:25.915 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.324219 s\n",
      "13:47:25.916 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job d05d256c-ccc0-4da3-ab33-ea0851b37cef.\n",
      "13:47:25.934 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job d05d256c-ccc0-4da3-ab33-ea0851b37cef committed. Elapsed time: 17 ms.\n",
      "13:47:25.934 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job d05d256c-ccc0-4da3-ab33-ea0851b37cef.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create parquet files\n",
    "\n",
    "try:\n",
    "    avg_flight.write.parquet(config[\"Parquet_file\"][\"avg_flight\"], mode=\"overwrite\")\n",
    "\n",
    "    airline_in_airport.write.parquet(config[\"Parquet_file\"][\"airline_in_airport\"], mode=\"overwrite\")\n",
    "\n",
    "    flights_per_cancell.write.parquet(config[\"Parquet_file\"][\"flights_per_cancell\"], mode=\"overwrite\")\n",
    "\n",
    "    airport_notin_thelist.write.parquet(config[\"Parquet_file\"][\"airport_notin_thelist\"], mode=\"overwrite\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error write parquet files:\" + str(error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
