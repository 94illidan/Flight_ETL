{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a3abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, max, min, count, broadcast, desc, asc, when, lit, round\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2650fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cfebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:27:59.884 [Thread-3] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ebb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b3eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:00.097 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "14:28:00.108 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/illidan/proyecto_desde0/ETL/spark-warehouse'.\n",
      "14:28:00.124 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67eb9272{/SQL,null,AVAILABLE,@Spark}\n",
      "14:28:00.125 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d7eadb7{/SQL/json,null,AVAILABLE,@Spark}\n",
      "14:28:00.126 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2eef4296{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "14:28:00.127 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12422f79{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "14:28:00.141 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21162118{/static/sql,null,AVAILABLE,@Spark}\n",
      "14:28:01.268 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 81 ms to list leaf files for 1 paths.\n",
      "14:28:01.909 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:01.933 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:01.934 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:01.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:01.937 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:01.943 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:02.040 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "14:28:02.081 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "14:28:02.085 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.23.57.81:42041 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:28:02.091 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:02.115 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:02.116 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "14:28:02.198 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "14:28:02.221 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:02.709 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3010 bytes result sent to driver\n",
      "14:28:02.723 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 558 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:02.726 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "14:28:02.732 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.772 s\n",
      "14:28:02.735 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:02.736 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "14:28:02.738 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.828063 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:02.923 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.23.57.81:42041 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:28:03.627 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "14:28:03.691 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:03.693 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:03.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:03.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:03.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:03.696 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:03.714 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "14:28:03.719 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "14:28:03.720 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.23.57.81:42041 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:28:03.721 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:03.724 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:03.724 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "14:28:03.727 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "14:28:03.729 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n",
      "14:28:03.756 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1662 bytes result sent to driver\n",
      "14:28:03.760 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 34 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:03.760 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "14:28:03.762 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.063 s\n",
      "14:28:03.762 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:03.762 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "14:28:03.763 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.071146 s\n",
      "14:28:03.795 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 1 paths.\n",
      "14:28:03.833 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:42041 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:28:03.863 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:03.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:03.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:03.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:03.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:03.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:03.877 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "14:28:03.881 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "14:28:03.883 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.23.57.81:42041 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:28:03.884 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:03.886 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:03.886 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "14:28:03.888 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9225 bytes) \n",
      "14:28:03.890 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "14:28:03.911 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1945 bytes result sent to driver\n",
      "14:28:03.915 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 26 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:03.915 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "14:28:03.916 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.047 s\n",
      "14:28:03.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:03.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "14:28:03.918 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.054446 s\n",
      "14:28:04.563 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:28:04.564 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:28:04.636 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:42041 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:28:05.124 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 280.918068 ms\n",
      "14:28:05.157 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)\n",
      "14:28:05.168 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "14:28:05.169 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.23.57.81:42041 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:28:05.170 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from count at NativeMethodAccessorImpl.java:0\n",
      "14:28:05.189 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:05.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "14:28:05.268 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:28:05.269 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:28:05.270 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:05.274 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:05.277 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:05.358 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)\n",
      "14:28:05.370 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "14:28:05.372 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.23.57.81:42041 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:28:05.373 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:05.375 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:28:05.375 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 8 tasks resource profile 0\n",
      "14:28:05.382 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.383 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 4) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.384 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 5) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.384 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 6) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.385 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 3.0 (TID 7) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.386 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 3.0 (TID 8) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.387 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 3.0 (TID 9) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.387 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 3.0 (TID 10) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:05.388 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)\n",
      "14:28:05.390 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 4)\n",
      "14:28:05.392 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 5)\n",
      "14:28:05.393 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 6)\n",
      "14:28:05.398 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 3.0 (TID 8)\n",
      "14:28:05.398 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 3.0 (TID 9)\n",
      "14:28:05.399 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 3.0 (TID 7)\n",
      "14:28:05.407 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 3.0 (TID 10)\n",
      "14:28:05.547 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.832938 ms\n",
      "14:28:05.587 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:28:05.587 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:28:05.589 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:28:05.591 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:28:05.592 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:28:05.595 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:28:05.597 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:28:05.598 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:05.882 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 3.0 (TID 7). 2222 bytes result sent to driver\n",
      "14:28:05.884 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2222 bytes result sent to driver\n",
      "14:28:05.884 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 6). 2222 bytes result sent to driver\n",
      "14:28:05.885 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 3.0 (TID 9). 2222 bytes result sent to driver\n",
      "14:28:05.887 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 4). 2179 bytes result sent to driver\n",
      "14:28:05.891 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 3.0 (TID 8). 2222 bytes result sent to driver\n",
      "14:28:05.891 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 514 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:28:05.892 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 3.0 (TID 10). 2222 bytes result sent to driver\n",
      "14:28:05.892 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 5). 2222 bytes result sent to driver\n",
      "14:28:05.892 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 3.0 (TID 7) in 507 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:28:05.893 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 6) in 509 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:28:05.893 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 3.0 (TID 9) in 507 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:28:05.894 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 4) in 510 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:28:05.895 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 3.0 (TID 10) in 507 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:28:05.895 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 5) in 511 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:28:05.896 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 3.0 (TID 8) in 511 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:28:05.896 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "14:28:05.905 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.618 s\n",
      "14:28:05.906 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:05.906 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:05.908 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:05.908 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:05.975 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.823587 ms\n",
      "14:28:06.022 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:28:06.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:06.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:28:06.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)\n",
      "14:28:06.026 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:06.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:06.042 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
      "14:28:06.046 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "14:28:06.048 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.23.57.81:42041 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:28:06.050 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:06.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:06.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "14:28:06.058 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 11) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:06.063 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 11)\n",
      "14:28:06.128 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:06.131 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms\n",
      "14:28:06.152 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.06011 ms\n",
      "14:28:06.173 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 11). 4038 bytes result sent to driver\n",
      "14:28:06.175 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 11) in 118 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:06.175 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "14:28:06.176 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.138 s\n",
      "14:28:06.176 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:06.177 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished\n",
      "14:28:06.177 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.154407 s\n",
      "14:28:06.219 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:28:06.220 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:28:06.255 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 200.1 KiB, free 433.9 MiB)\n",
      "14:28:06.290 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "14:28:06.293 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.23.57.81:42041 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:28:06.294 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 6 from count at NativeMethodAccessorImpl.java:0\n",
      "14:28:06.299 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:06.300 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.23.57.81:42041 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:28:06.310 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "14:28:06.311 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:06.312 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:28:06.312 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:06.314 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.23.57.81:42041 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:28:06.316 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:06.318 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:06.335 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 17.0 KiB, free 434.1 MiB)\n",
      "14:28:06.338 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.23.57.81:42041 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:28:06.338 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "14:28:06.339 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.23.57.81:42041 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:28:06.342 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:06.343 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:06.344 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0\n",
      "14:28:06.347 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 12) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:06.351 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 12)\n",
      "14:28:06.361 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-ef405ddd-6793-4fa9-8dc9-28ccb32b79bf-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:28:06.379 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 12). 2179 bytes result sent to driver\n",
      "14:28:06.383 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 12) in 37 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:06.384 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "14:28:06.385 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.056 s\n",
      "14:28:06.386 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:06.386 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:06.386 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:06.386 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:06.438 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:28:06.441 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:06.441 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:28:06.441 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)\n",
      "14:28:06.442 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:06.443 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:06.448 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
      "14:28:06.451 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "14:28:06.452 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.23.57.81:42041 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:28:06.453 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:06.455 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:06.456 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks resource profile 0\n",
      "14:28:06.460 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 13) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:06.461 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 13)\n",
      "14:28:06.467 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:06.467 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:06.474 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 13). 3995 bytes result sent to driver\n",
      "14:28:06.477 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 13) in 19 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:06.478 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "14:28:06.479 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.033 s\n",
      "14:28:06.479 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:06.479 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished\n",
      "14:28:06.480 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.041379 s\n",
      "14:28:06.523 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:28:06.523 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:28:06.563 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 200.1 KiB, free 433.9 MiB)\n",
      "14:28:06.592 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.23.57.81:42041 in memory (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:28:06.599 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "14:28:06.600 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.23.57.81:42041 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:28:06.603 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 9 from count at NativeMethodAccessorImpl.java:0\n",
      "14:28:06.608 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:06.627 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.23.57.81:42041 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:28:06.653 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 23 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "14:28:06.662 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:06.662 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:28:06.662 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:06.668 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:06.670 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:06.689 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 172.23.57.81:42041 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:28:06.690 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)\n",
      "14:28:06.692 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "14:28:06.694 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.23.57.81:42041 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:28:06.698 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:06.699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:06.699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0\n",
      "14:28:06.701 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 14) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9684 bytes) \n",
      "14:28:06.702 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 14)\n",
      "14:28:06.712 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-1cf42ad0-ccd6-4da8-94c2-7aa2f67986e2-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:28:06.727 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 14). 2136 bytes result sent to driver\n",
      "14:28:06.729 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 14) in 28 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:06.729 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "14:28:06.731 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.059 s\n",
      "14:28:06.731 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:06.731 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:06.731 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:06.731 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:06.775 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:28:06.777 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:06.777 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:28:06.777 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)\n",
      "14:28:06.778 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:06.779 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:06.783 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
      "14:28:06.785 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "14:28:06.788 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.23.57.81:42041 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:28:06.789 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:06.790 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:06.790 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0\n",
      "14:28:06.793 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 15) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:06.794 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 15)\n",
      "14:28:06.805 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:06.805 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:06.809 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 15). 3995 bytes result sent to driver\n",
      "14:28:06.811 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 15) in 19 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:06.811 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "14:28:06.813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.031 s\n",
      "14:28:06.813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:06.813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished\n",
      "14:28:06.814 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.038201 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flight_row_count = df_flights.count()\n",
    "\n",
    "airline_row_count = df_airline.count()\n",
    "\n",
    "airports_row_count = df_airports.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1952a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #avg per fly\n",
    "    avg_flight = df_flights.join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                        .groupBy(df_flights.AIRLINE, df_airline.AIRLINE) \\\n",
    "                        .agg(round(avg(df_flights.DISTANCE), 2).alias(\"avg_DISTANCE\")) \\\n",
    "                        .orderBy(desc(\"avg_DISTANCE\")) \\\n",
    "                        .select(df_airline.AIRLINE.alias(\"AIRLINE_Name\"), \"avg_DISTANCE\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable avg_flight:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237699b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #how many times a flight visit an airport\n",
    "    airline_in_airport = df_flights.join(broadcast(df_airports), df_flights.ORIGIN_AIRPORT == df_airports.IATA_CODE) \\\n",
    "                                .join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                                .repartition(df_flights.DESTINATION_AIRPORT) \\\n",
    "                                .groupBy(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE) \\\n",
    "                                    .agg(count(df_flights.AIRLINE).alias(\"Count_visit_per_airline\")) \\\n",
    "                                .orderBy(asc(df_flights.DESTINATION_AIRPORT)) \\\n",
    "                                .select(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE,\"Count_visit_per_airline\") \\\n",
    "                                .limit(100)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airline_in_airport:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad20a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #mention how many times an airline visit a destination and canceled this arrival\n",
    "    flights_per_cancell = df_flights.select(col(\"AIRLINE\"), col(\"CANCELLED\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .filter(col(\"CANCELLED\") == \"1\") \\\n",
    "                                .repartition(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .groupBy(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                    .agg(count(col(\"AIRLINE\")).alias(\"count_airlines_cancel\")) \\\n",
    "                                .orderBy(desc(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .limit(100)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable flights_per_cancell:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7929cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #i create this variable because i need to know how many airports have flights but are not in the list of airports\n",
    "    airport_notin_thelist = df_flights.join(broadcast(df_airports), df_flights.DESTINATION_AIRPORT == df_airports.IATA_CODE, \"left_anti\") \\\n",
    "                                        .select(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .repartition(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .groupBy(\"DESTINATION_AIRPORT\").agg(count(\"*\").alias(\"Count_airports\"))\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airport_notin_thelist:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7983760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:07.744 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "14:28:07.745 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "14:28:07.751 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:28:07.752 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "14:28:07.816 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 172.23.57.81:42041 in memory (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:28:07.822 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 172.23.57.81:42041 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:28:07.903 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.034763 ms\n",
      "14:28:07.912 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 200.3 KiB, free 434.0 MiB)\n",
      "14:28:07.921 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.9 MiB)\n",
      "14:28:07.923 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.23.57.81:42041 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "14:28:07.925 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:07.929 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:07.951 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:07.953 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:28:07.953 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:28:07.953 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:07.953 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:07.955 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:28:07.959 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 15.1 KiB, free 433.9 MiB)\n",
      "14:28:07.962 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "14:28:07.964 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.23.57.81:42041 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "14:28:07.965 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:07.968 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:07.969 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks resource profile 0\n",
      "14:28:07.971 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "14:28:07.973 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)\n",
      "14:28:08.007 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.306604 ms\n",
      "14:28:08.008 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-ef405ddd-6793-4fa9-8dc9-28ccb32b79bf-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:28:08.031 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:28:08.122 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:28:08.293 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.23.57.81:42041 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:28:08.315 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 2241 bytes result sent to driver\n",
      "14:28:08.316 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 345 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:08.316 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "14:28:08.317 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.361 s\n",
      "14:28:08.318 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:08.318 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished\n",
      "14:28:08.318 [broadcast-exchange-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.367209 s\n",
      "14:28:08.347 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.11812 ms\n",
      "14:28:08.354 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 2.0 MiB, free 432.1 MiB)\n",
      "14:28:08.359 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 686.0 B, free 432.1 MiB)\n",
      "14:28:08.359 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.23.57.81:42041 (size: 686.0 B, free: 434.4 MiB)\n",
      "14:28:08.360 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:08.372 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "14:28:08.373 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "14:28:08.546 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 87.237144 ms\n",
      "14:28:08.551 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 200.3 KiB, free 432.0 MiB)\n",
      "14:28:08.564 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 431.9 MiB)\n",
      "14:28:08.565 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 172.23.57.81:42041 (size: 34.7 KiB, free: 434.3 MiB)\n",
      "14:28:08.566 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 15 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:08.568 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:08.644 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 34 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "14:28:08.645 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 10 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:28:08.645 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:08.645 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:08.647 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:08.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:08.655 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 53.5 KiB, free 431.9 MiB)\n",
      "14:28:08.676 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 431.8 MiB)\n",
      "14:28:08.678 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 172.23.57.81:42041 (size: 23.2 KiB, free: 434.3 MiB)\n",
      "14:28:08.682 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:08.683 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:28:08.683 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 8 tasks resource profile 0\n",
      "14:28:08.685 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 17) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.686 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 18) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.687 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 13.0 (TID 19) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.688 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 13.0 (TID 20) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.688 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 172.23.57.81:42041 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "14:28:08.689 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 13.0 (TID 21) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.689 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 13.0 (TID 22) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.689 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 13.0 (TID 23) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.690 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 13.0 (TID 24) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:08.690 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 17)\n",
      "14:28:08.691 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 13.0 (TID 21)\n",
      "14:28:08.691 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 13.0 (TID 22)\n",
      "14:28:08.691 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 18)\n",
      "14:28:08.691 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 13.0 (TID 19)\n",
      "14:28:08.695 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 13.0 (TID 20)\n",
      "14:28:08.706 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 13.0 (TID 23)\n",
      "14:28:08.707 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 13.0 (TID 24)\n",
      "14:28:08.827 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 102.337235 ms\n",
      "14:28:08.857 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.681401 ms\n",
      "14:28:08.878 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.219294 ms\n",
      "14:28:08.925 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.040668 ms\n",
      "14:28:08.966 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.394873 ms\n",
      "14:28:08.987 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:28:08.987 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:28:08.988 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:28:08.990 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:28:08.990 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:28:08.997 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:08.998 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:28:08.998 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:28:09.025 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:28:09.032 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:09.039 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:09.040 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:09.054 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:28:09.055 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:09.060 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:09.061 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:28:09.060 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:09.067 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:28:09.068 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:28:09.072 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:28:09.083 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:28:09.092 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:28:09.098 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:10.138 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 13.0 (TID 24). 3926 bytes result sent to driver\n",
      "14:28:10.140 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 13.0 (TID 24) in 1451 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:28:10.157 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 17). 3926 bytes result sent to driver\n",
      "14:28:10.158 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 13.0 (TID 20). 3926 bytes result sent to driver\n",
      "14:28:10.160 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 13.0 (TID 23). 3926 bytes result sent to driver\n",
      "14:28:10.162 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 17) in 1478 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:28:10.165 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 13.0 (TID 18). 3926 bytes result sent to driver\n",
      "14:28:10.165 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 13.0 (TID 20) in 1477 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:28:10.167 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 13.0 (TID 23) in 1478 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:28:10.172 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 13.0 (TID 18) in 1486 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:28:10.173 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 13.0 (TID 21). 3926 bytes result sent to driver\n",
      "14:28:10.175 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 13.0 (TID 21) in 1486 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:28:10.177 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 13.0 (TID 22). 3926 bytes result sent to driver\n",
      "14:28:10.179 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 13.0 (TID 19). 3926 bytes result sent to driver\n",
      "14:28:10.179 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 13.0 (TID 22) in 1489 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:28:10.181 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 13.0 (TID 19) in 1494 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:28:10.181 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "14:28:10.182 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.532 s\n",
      "14:28:10.182 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:10.182 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:10.182 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:10.183 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:10.203 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:28:10.236 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:28:10.309 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 57.505308 ms\n",
      "14:28:10.381 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.951412 ms\n",
      "14:28:10.443 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:10.445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:10.445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:10.445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)\n",
      "14:28:10.445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:10.447 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:10.455 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 54.3 KiB, free 431.8 MiB)\n",
      "14:28:10.458 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 431.8 MiB)\n",
      "14:28:10.459 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 172.23.57.81:42041 (size: 24.1 KiB, free: 434.3 MiB)\n",
      "14:28:10.460 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:10.461 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:10.461 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0\n",
      "14:28:10.463 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 25) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:10.465 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 25)\n",
      "14:28:10.483 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:10.483 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:10.511 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.331827 ms\n",
      "14:28:10.524 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.618841 ms\n",
      "14:28:10.542 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 25). 6848 bytes result sent to driver\n",
      "14:28:10.543 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 25) in 80 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:10.544 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "14:28:10.545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.092 s\n",
      "14:28:10.545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:10.545 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished\n",
      "14:28:10.546 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.102108 s\n",
      "14:28:10.555 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 40 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "14:28:10.555 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:10.556 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:10.556 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)\n",
      "14:28:10.557 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:10.558 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:10.571 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 54.7 KiB, free 431.7 MiB)\n",
      "14:28:10.573 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 431.7 MiB)\n",
      "14:28:10.573 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 172.23.57.81:42041 (size: 24.3 KiB, free: 434.3 MiB)\n",
      "14:28:10.574 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:10.575 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:10.575 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks resource profile 0\n",
      "14:28:10.576 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 26) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "14:28:10.577 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:10.602 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.43469 ms\n",
      "14:28:10.617 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:10.617 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:10.639 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 26). 6433 bytes result sent to driver\n",
      "14:28:10.641 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 26) in 64 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:10.641 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "14:28:10.642 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.079 s\n",
      "14:28:10.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:10.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:10.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:10.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:10.650 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:28:10.708 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:10.725 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:10.725 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:10.727 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:10.727 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:10.727 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:10.728 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:10.753 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 172.23.57.81:42041 in memory (size: 24.3 KiB, free: 434.3 MiB)\n",
      "14:28:10.760 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 172.23.57.81:42041 in memory (size: 24.1 KiB, free: 434.3 MiB)\n",
      "14:28:10.836 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.141768 ms\n",
      "14:28:10.852 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:10.854 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:10.854 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:10.854 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)\n",
      "14:28:10.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:10.859 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:10.890 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 250.3 KiB, free 431.6 MiB)\n",
      "14:28:10.892 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 431.5 MiB)\n",
      "14:28:10.893 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 172.23.57.81:42041 (size: 93.3 KiB, free: 434.2 MiB)\n",
      "14:28:10.894 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:10.895 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:10.895 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks resource profile 0\n",
      "14:28:10.897 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 27) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:10.898 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 27)\n",
      "14:28:10.933 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1323.0 B) non-empty blocks including 1 (1323.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:10.933 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:10.949 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.238146 ms\n",
      "14:28:10.962 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.502858 ms\n",
      "14:28:10.981 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.163236 ms\n",
      "14:28:10.992 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:10.992 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:10.993 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:10.993 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:10.993 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:10.993 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:11.000 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:11.003 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:11.032 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:28:11.047 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE_Name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_DISTANCE\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE_Name (STRING);\n",
      "  optional double avg_DISTANCE;\n",
      "}\n",
      "\n",
      "       \n",
      "14:28:11.149 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]\n",
      "14:28:11.221 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506041428109049518429095068669_0020_m_000000_27' to file:/home/illidan/proyecto_desde0/archivos_parquet/avg_flight/_temporary/0/task_202506041428109049518429095068669_0020_m_000000\n",
      "14:28:11.222 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506041428109049518429095068669_0020_m_000000_27: Committed. Elapsed time: 0 ms.\n",
      "14:28:11.230 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 27). 8767 bytes result sent to driver\n",
      "14:28:11.232 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 27) in 335 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:11.233 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "14:28:11.234 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.373 s\n",
      "14:28:11.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:11.235 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished\n",
      "14:28:11.236 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.383517 s\n",
      "14:28:11.238 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job f2729b2f-b046-4d74-a9b0-1e6bc6641bab.\n",
      "14:28:11.304 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job f2729b2f-b046-4d74-a9b0-1e6bc6641bab committed. Elapsed time: 64 ms.\n",
      "14:28:11.320 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job f2729b2f-b046-4d74-a9b0-1e6bc6641bab.\n",
      "14:28:11.525 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:28:11.526 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:28:11.528 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:28:11.528 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "14:28:11.530 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:28:11.531 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "14:28:11.601 [broadcast-exchange-2] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.882711 ms\n",
      "14:28:11.602 [broadcast-exchange-1] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.933765 ms\n",
      "14:28:11.608 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 200.3 KiB, free 431.1 MiB)\n",
      "14:28:11.608 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 200.2 KiB, free 431.1 MiB)\n",
      "14:28:11.620 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.1 MiB)\n",
      "14:28:11.621 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 172.23.57.81:42041 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:28:11.621 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:11.622 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.1 MiB)\n",
      "14:28:11.622 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 172.23.57.81:42041 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:28:11.622 [broadcast-exchange-1] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:11.623 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:11.624 [broadcast-exchange-2] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:11.644 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:11.645 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:11.646 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:28:11.646 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:28:11.646 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:11.646 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:11.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:28:11.650 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 15.1 KiB, free 431.1 MiB)\n",
      "14:28:11.652 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 431.0 MiB)\n",
      "14:28:11.652 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 172.23.57.81:42041 (size: 6.5 KiB, free: 434.1 MiB)\n",
      "14:28:11.653 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:11.654 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:11.654 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks resource profile 0\n",
      "14:28:11.656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:28:11.656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:28:11.656 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 28) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "14:28:11.656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:11.656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:11.657 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 28)\n",
      "14:28:11.658 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:28:11.660 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 14.5 KiB, free 431.0 MiB)\n",
      "14:28:11.663 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 431.0 MiB)\n",
      "14:28:11.665 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 172.23.57.81:42041 (size: 6.3 KiB, free: 434.1 MiB)\n",
      "14:28:11.666 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:11.667 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:11.667 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 1 tasks resource profile 0\n",
      "14:28:11.668 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 29) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "14:28:11.669 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 29)\n",
      "14:28:11.677 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.918967 ms\n",
      "14:28:11.679 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-ef405ddd-6793-4fa9-8dc9-28ccb32b79bf-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:28:11.683 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.321669 ms\n",
      "14:28:11.684 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:28:11.685 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-1cf42ad0-ccd6-4da8-94c2-7aa2f67986e2-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:28:11.692 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 28). 2198 bytes result sent to driver\n",
      "14:28:11.693 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:28:11.693 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 28) in 37 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:11.694 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "14:28:11.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.046 s\n",
      "14:28:11.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:11.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 21: Stage finished\n",
      "14:28:11.695 [broadcast-exchange-2] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.050954 s\n",
      "14:28:11.699 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 2.0 MiB, free 429.0 MiB)\n",
      "14:28:11.716 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 686.0 B, free 429.0 MiB)\n",
      "14:28:11.716 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 172.23.57.81:42041 (size: 686.0 B, free: 434.1 MiB)\n",
      "14:28:11.717 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 24 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:11.720 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 29). 3699 bytes result sent to driver\n",
      "14:28:11.724 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 29) in 56 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:11.724 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "14:28:11.726 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.066 s\n",
      "14:28:11.726 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:11.726 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished\n",
      "14:28:11.733 [broadcast-exchange-1] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.086951 s\n",
      "14:28:11.740 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:28:11.740 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 172.23.57.81:42041 in memory (size: 6.5 KiB, free: 434.1 MiB)\n",
      "14:28:11.740 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:28:11.769 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 2.0 MiB, free 427.0 MiB)\n",
      "14:28:11.782 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 427.0 MiB)\n",
      "14:28:11.795 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 172.23.57.81:42041 (size: 4.9 KiB, free: 434.1 MiB)\n",
      "14:28:11.795 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 172.23.57.81:42041 in memory (size: 93.3 KiB, free: 434.2 MiB)\n",
      "14:28:11.796 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:11.807 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:28:11.807 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:28:11.862 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 172.23.57.81:42041 in memory (size: 23.2 KiB, free: 434.3 MiB)\n",
      "14:28:11.905 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.846636 ms\n",
      "14:28:11.912 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 200.5 KiB, free 427.2 MiB)\n",
      "14:28:11.924 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 427.2 MiB)\n",
      "14:28:11.925 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 172.23.57.81:42041 (size: 34.8 KiB, free: 434.2 MiB)\n",
      "14:28:11.925 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 26 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:11.927 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:11.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 55 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "14:28:11.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 16 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:28:11.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:11.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:11.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:11.937 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:11.943 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 22.1 KiB, free 427.2 MiB)\n",
      "14:28:11.947 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 427.2 MiB)\n",
      "14:28:11.947 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 172.23.57.81:42041 (size: 9.0 KiB, free: 434.2 MiB)\n",
      "14:28:11.949 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:11.949 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:28:11.950 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 8 tasks resource profile 0\n",
      "14:28:11.951 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 30) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.952 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 31) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.952 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 32) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.952 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 33) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.953 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 23.0 (TID 34) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.953 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 23.0 (TID 35) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.954 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 23.0 (TID 36) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.954 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 23.0 (TID 37) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:11.955 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 30)\n",
      "14:28:11.956 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 31)\n",
      "14:28:11.956 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 33)\n",
      "14:28:11.957 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 23.0 (TID 37)\n",
      "14:28:11.958 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 32)\n",
      "14:28:11.960 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 23.0 (TID 34)\n",
      "14:28:11.964 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 23.0 (TID 35)\n",
      "14:28:11.966 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 23.0 (TID 36)\n",
      "14:28:11.987 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.081087 ms\n",
      "14:28:12.006 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.251836 ms\n",
      "14:28:12.007 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:28:12.007 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:28:12.007 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:28:12.007 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:28:12.008 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:28:12.008 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:28:12.007 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:28:12.007 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:28:12.024 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:28:12.025 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:28:12.025 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:28:12.026 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:28:12.026 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:28:12.030 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:28:12.040 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:28:12.048 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:12.698 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 172.23.57.81:42041 in memory (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:28:12.741 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 172.23.57.81:42041 in memory (size: 6.3 KiB, free: 434.2 MiB)\n",
      "14:28:12.762 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 172.23.57.81:42041 in memory (size: 34.7 KiB, free: 434.3 MiB)\n",
      "14:28:12.836 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 172.23.57.81:42041 in memory (size: 686.0 B, free: 434.3 MiB)\n",
      "14:28:13.580 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 23.0 (TID 36). 2451 bytes result sent to driver\n",
      "14:28:13.582 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 23.0 (TID 36) in 1629 ms on 172.23.57.81 (executor driver) (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:14.332 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 23.0 (TID 37). 2494 bytes result sent to driver\n",
      "14:28:14.333 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 23.0 (TID 37) in 2379 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:28:14.344 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 23.0 (TID 35). 2451 bytes result sent to driver\n",
      "14:28:14.345 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 32). 2451 bytes result sent to driver\n",
      "14:28:14.345 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 23.0 (TID 35) in 2392 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:28:14.347 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 33). 2451 bytes result sent to driver\n",
      "14:28:14.354 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 33) in 2402 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:28:14.355 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 32) in 2403 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:28:14.363 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 31). 2451 bytes result sent to driver\n",
      "14:28:14.364 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 31) in 2413 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:28:14.368 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 23.0 (TID 34). 2451 bytes result sent to driver\n",
      "14:28:14.369 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 23.0 (TID 34) in 2416 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:28:14.394 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 30). 2451 bytes result sent to driver\n",
      "14:28:14.395 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 30) in 2443 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:28:14.395 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "14:28:14.395 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.457 s\n",
      "14:28:14.395 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:14.396 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:14.396 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:14.396 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:14.402 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(5), advisory target size: 67108864, actual target size 3493000, minimum partition size: 1048576\n",
      "14:28:14.419 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:14.421 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:14.421 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:14.421 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:14.421 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:14.421 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:14.422 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:14.592 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.547532 ms\n",
      "14:28:14.595 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:28:14.661 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 55.411987 ms\n",
      "14:28:14.697 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:14.698 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 59 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "14:28:14.699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 17 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:14.699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 26 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:14.699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 25)\n",
      "14:28:14.699 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 25)\n",
      "14:28:14.703 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 25 (MapPartitionsRDD[59] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:14.728 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 63.5 KiB, free 429.6 MiB)\n",
      "14:28:14.730 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.3 KiB, free 429.6 MiB)\n",
      "14:28:14.731 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 172.23.57.81:42041 (size: 26.3 KiB, free: 434.3 MiB)\n",
      "14:28:14.732 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:14.733 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[59] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "14:28:14.733 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 10 tasks resource profile 0\n",
      "14:28:14.735 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 38) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.736 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 25.0 (TID 39) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.737 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 25.0 (TID 40) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.738 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 25.0 (TID 41) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.739 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 25.0 (TID 42) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.739 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 25.0 (TID 43) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.740 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 25.0 (TID 44) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.741 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 25.0 (TID 45) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8988 bytes) \n",
      "14:28:14.742 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 38)\n",
      "14:28:14.742 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 25.0 (TID 43)\n",
      "14:28:14.742 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 25.0 (TID 41)\n",
      "14:28:14.742 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 25.0 (TID 40)\n",
      "14:28:14.742 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 25.0 (TID 39)\n",
      "14:28:14.742 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 25.0 (TID 42)\n",
      "14:28:14.742 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 25.0 (TID 45)\n",
      "14:28:14.742 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 25.0 (TID 44)\n",
      "14:28:14.766 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.379751 ms\n",
      "14:28:14.783 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.783 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:14.783 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.783 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:28:14.783 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.784 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:28:14.784 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.784 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:14.788 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.788 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:14.791 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.791 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.792 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:28:14.792 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms\n",
      "14:28:14.793 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:14.794 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms\n",
      "14:28:14.836 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 51.235891 ms\n",
      "14:28:14.862 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.543179 ms\n",
      "14:28:14.874 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.191631 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:15.408 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 25.0 (TID 45). 8199 bytes result sent to driver\n",
      "14:28:15.414 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 25.0 (TID 46) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8988 bytes) \n",
      "14:28:15.416 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 25.0 (TID 45) in 676 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "14:28:15.417 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 25.0 (TID 46)\n",
      "14:28:15.463 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:15.463 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:28:15.526 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 25.0 (TID 43). 8199 bytes result sent to driver\n",
      "14:28:15.541 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 25.0 (TID 47) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8988 bytes) \n",
      "14:28:15.542 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 25.0 (TID 43) in 803 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "14:28:15.544 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 25.0 (TID 47)\n",
      "14:28:15.566 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:15.566 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:15.604 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 25.0 (TID 40). 8199 bytes result sent to driver\n",
      "14:28:15.612 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 25.0 (TID 40) in 875 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "14:28:15.652 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 25.0 (TID 39). 8199 bytes result sent to driver\n",
      "14:28:15.663 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 25.0 (TID 39) in 927 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "14:28:15.714 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 25.0 (TID 44). 8199 bytes result sent to driver\n",
      "14:28:15.715 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 25.0 (TID 44) in 975 ms on 172.23.57.81 (executor driver) (5/10)\n",
      "14:28:15.721 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 25.0 (TID 41). 8199 bytes result sent to driver\n",
      "14:28:15.728 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 25.0 (TID 41) in 990 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "14:28:15.756 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 25.0 (TID 42). 8199 bytes result sent to driver\n",
      "14:28:15.758 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 25.0 (TID 42) in 1019 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "14:28:15.782 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 38). 8199 bytes result sent to driver\n",
      "14:28:15.783 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 38) in 1048 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "14:28:15.834 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 25.0 (TID 46). 8199 bytes result sent to driver\n",
      "14:28:15.835 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 25.0 (TID 46) in 421 ms on 172.23.57.81 (executor driver) (9/10)\n",
      "14:28:15.863 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 25.0 (TID 47). 8199 bytes result sent to driver\n",
      "14:28:15.864 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 25.0 (TID 47) in 330 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "14:28:15.864 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:==================================>                      (6 + 4) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:15.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 25 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.145 s\n",
      "14:28:15.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:15.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:15.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 26)\n",
      "14:28:15.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:15.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 26 (MapPartitionsRDD[62] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:15.892 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 244.8 KiB, free 429.3 MiB)\n",
      "14:28:15.894 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 91.3 KiB, free 429.2 MiB)\n",
      "14:28:15.895 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 172.23.57.81:42041 (size: 91.3 KiB, free: 434.2 MiB)\n",
      "14:28:15.896 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:15.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[62] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:15.896 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 26.0 with 1 tasks resource profile 0\n",
      "14:28:15.898 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 26.0 (TID 48) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:15.899 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 26.0 (TID 48)\n",
      "14:28:15.919 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 10 (16.9 KiB) non-empty blocks including 10 (16.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:15.920 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:28:15.924 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:15.924 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:15.924 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:15.924 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:15.924 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:15.925 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:15.925 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:15.925 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:15.927 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:28:15.931 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_visit_per_airline\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  optional binary AIRLINE (STRING);\n",
      "  required int64 Count_visit_per_airline;\n",
      "}\n",
      "\n",
      "       \n",
      "14:28:16.110 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506041428146776194414985360937_0026_m_000000_48' to file:/home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/_temporary/0/task_202506041428146776194414985360937_0026_m_000000\n",
      "14:28:16.110 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506041428146776194414985360937_0026_m_000000_48: Committed. Elapsed time: 0 ms.\n",
      "14:28:16.112 [Executor task launch worker for task 0.0 in stage 26.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 26.0 (TID 48). 9172 bytes result sent to driver\n",
      "14:28:16.115 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 26.0 (TID 48) in 218 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:16.115 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "14:28:16.116 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 26 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.249 s\n",
      "14:28:16.118 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:16.119 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 26: Stage finished\n",
      "14:28:16.120 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.423058 s\n",
      "14:28:16.122 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job fc4548fb-873a-4936-ac09-b1add8ca999a.\n",
      "14:28:16.202 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job fc4548fb-873a-4936-ac09-b1add8ca999a committed. Elapsed time: 79 ms.\n",
      "14:28:16.202 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job fc4548fb-873a-4936-ac09-b1add8ca999a.\n",
      "14:28:16.250 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(CANCELLED),EqualTo(CANCELLED,1)\n",
      "14:28:16.251 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(CANCELLED#24),(CANCELLED#24 = 1)\n",
      "14:28:16.307 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.312321 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:16.318 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 200.5 KiB, free 429.0 MiB)\n",
      "14:28:16.336 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 429.0 MiB)\n",
      "14:28:16.337 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 172.23.57.81:42041 (size: 34.8 KiB, free: 434.1 MiB)\n",
      "14:28:16.338 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 30 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:16.339 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:16.356 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 66 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "14:28:16.356 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 18 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:28:16.356 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:16.356 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:16.357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:16.357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 27 (MapPartitionsRDD[66] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:16.364 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 18.8 KiB, free 429.0 MiB)\n",
      "14:28:16.366 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 429.0 MiB)\n",
      "14:28:16.367 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 172.23.57.81:42041 (size: 8.2 KiB, free: 434.1 MiB)\n",
      "14:28:16.367 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:16.368 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[66] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:28:16.368 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 8 tasks resource profile 0\n",
      "14:28:16.371 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 49) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.373 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 50) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.374 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 51) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.374 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 27.0 (TID 52) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.374 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 27.0 (TID 53) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.374 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 27.0 (TID 54) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.375 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 27.0 (TID 55) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.378 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 27.0 (TID 56) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:16.379 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 27.0 (TID 55)\n",
      "14:28:16.379 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 27.0 (TID 54)\n",
      "14:28:16.379 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 50)\n",
      "14:28:16.381 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 49)\n",
      "14:28:16.382 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 27.0 (TID 53)\n",
      "14:28:16.383 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 27.0 (TID 56)\n",
      "14:28:16.387 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 51)\n",
      "14:28:16.394 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 27.0 (TID 52)\n",
      "14:28:16.404 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.231114 ms\n",
      "14:28:16.408 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:28:16.410 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:28:16.413 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:28:16.415 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:28:16.423 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.424 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:28:16.413 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:28:16.427 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:28:16.416 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:28:16.428 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.432 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.438 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.441 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.442 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.463 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.441 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:28:16.485 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_29_piece0 on 172.23.57.81:42041 in memory (size: 91.3 KiB, free: 434.2 MiB)\n",
      "14:28:16.919 [Executor task launch worker for task 4.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 27.0 (TID 53). 2339 bytes result sent to driver\n",
      "14:28:16.920 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 27.0 (TID 53) in 546 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:28:17.002 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 172.23.57.81:42041 in memory (size: 686.0 B, free: 434.2 MiB)\n",
      "14:28:17.026 [Executor task launch worker for task 0.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 49). 2296 bytes result sent to driver\n",
      "14:28:17.028 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 49) in 658 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:28:17.035 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 172.23.57.81:42041 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
      "14:28:17.050 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 172.23.57.81:42041 in memory (size: 9.0 KiB, free: 434.3 MiB)\n",
      "14:28:17.058 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 172.23.57.81:42041 in memory (size: 34.6 KiB, free: 434.3 MiB)\n",
      "14:28:17.070 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_28_piece0 on 172.23.57.81:42041 in memory (size: 26.3 KiB, free: 434.3 MiB)\n",
      "14:28:17.082 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 172.23.57.81:42041 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "14:28:17.091 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on 172.23.57.81:42041 in memory (size: 4.9 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:17.155 [Executor task launch worker for task 3.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 27.0 (TID 52). 2296 bytes result sent to driver\n",
      "14:28:17.157 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 27.0 (TID 52) in 783 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:28:17.160 [Executor task launch worker for task 2.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 51). 2296 bytes result sent to driver\n",
      "14:28:17.161 [Executor task launch worker for task 5.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 27.0 (TID 54). 2296 bytes result sent to driver\n",
      "14:28:17.162 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 51) in 789 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:28:17.162 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 27.0 (TID 54) in 788 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:28:17.163 [Executor task launch worker for task 6.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 27.0 (TID 55). 2296 bytes result sent to driver\n",
      "14:28:17.164 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 27.0 (TID 55) in 790 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:28:17.167 [Executor task launch worker for task 7.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 27.0 (TID 56). 2296 bytes result sent to driver\n",
      "14:28:17.168 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 27.0 (TID 56) in 793 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:28:17.172 [Executor task launch worker for task 1.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 50). 2296 bytes result sent to driver\n",
      "14:28:17.173 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 50) in 801 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:28:17.174 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "14:28:17.174 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.815 s\n",
      "14:28:17.174 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:17.174 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:17.174 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:17.174 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:17.183 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:28:17.197 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:17.199 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:17.199 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:17.199 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:17.199 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:17.200 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:17.200 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:17.293 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.239728 ms\n",
      "14:28:17.294 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:28:17.342 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.653864 ms\n",
      "14:28:17.363 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:17.366 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 19 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:17.366 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:17.366 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 28)\n",
      "14:28:17.366 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:17.367 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[70] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:17.401 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 254.2 KiB, free 433.9 MiB)\n",
      "14:28:17.402 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 93.4 KiB, free 433.8 MiB)\n",
      "14:28:17.403 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 172.23.57.81:42041 (size: 93.4 KiB, free: 434.3 MiB)\n",
      "14:28:17.404 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:17.404 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[70] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:17.404 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 1 tasks resource profile 0\n",
      "14:28:17.406 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 57) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:17.407 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 57)\n",
      "14:28:17.419 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.1046 ms\n",
      "14:28:17.445 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (350.4 KiB) non-empty blocks including 8 (350.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:17.445 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:28:17.477 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.641572 ms\n",
      "14:28:17.590 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:17.590 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:17.591 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:17.591 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:17.591 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:17.591 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:17.591 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:17.592 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:17.595 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:28:17.597 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"count_airlines_cancel\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE (STRING);\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 count_airlines_cancel;\n",
      "}\n",
      "\n",
      "       \n",
      "14:28:17.707 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506041428178717659544928347739_0029_m_000000_57' to file:/home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/_temporary/0/task_202506041428178717659544928347739_0029_m_000000\n",
      "14:28:17.707 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506041428178717659544928347739_0029_m_000000_57: Committed. Elapsed time: 0 ms.\n",
      "14:28:17.708 [Executor task launch worker for task 0.0 in stage 29.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 57). 7521 bytes result sent to driver\n",
      "14:28:17.710 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 57) in 304 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:17.710 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "14:28:17.711 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.330 s\n",
      "14:28:17.711 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:17.711 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 29: Stage finished\n",
      "14:28:17.711 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.347997 s\n",
      "14:28:17.712 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 7bf799cf-19db-4410-8e72-385abca53345.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:17.888 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 7bf799cf-19db-4410-8e72-385abca53345 committed. Elapsed time: 175 ms.\n",
      "14:28:17.888 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 7bf799cf-19db-4410-8e72-385abca53345.\n",
      "14:28:17.934 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:28:17.934 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:28:17.936 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:28:17.936 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "14:28:17.971 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 200.2 KiB, free 433.6 MiB)\n",
      "14:28:17.979 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.6 MiB)\n",
      "14:28:17.980 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on 172.23.57.81:42041 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:28:17.981 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 33 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:17.982 [broadcast-exchange-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:17.992 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:17.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:28:17.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:28:17.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:17.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:17.994 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 30 (MapPartitionsRDD[74] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:28:17.995 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34 stored as values in memory (estimated size 14.5 KiB, free 433.6 MiB)\n",
      "14:28:17.996 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 433.6 MiB)\n",
      "14:28:17.997 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_34_piece0 in memory on 172.23.57.81:42041 (size: 6.3 KiB, free: 434.2 MiB)\n",
      "14:28:17.998 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:17.998 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[74] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:17.999 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 30.0 with 1 tasks resource profile 0\n",
      "14:28:17.999 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 30.0 (TID 58) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "14:28:18.000 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 30.0 (TID 58)\n",
      "14:28:18.003 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-1cf42ad0-ccd6-4da8-94c2-7aa2f67986e2-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:28:18.009 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:28:18.015 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 30.0 (TID 58). 3613 bytes result sent to driver\n",
      "14:28:18.015 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 30.0 (TID 58) in 16 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:18.016 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "14:28:18.016 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.021 s\n",
      "14:28:18.016 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:18.016 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 30: Stage finished\n",
      "14:28:18.017 [broadcast-exchange-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.024456 s\n",
      "14:28:18.022 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35 stored as values in memory (estimated size 2.0 MiB, free 431.5 MiB)\n",
      "14:28:18.024 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 431.5 MiB)\n",
      "14:28:18.024 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_35_piece0 in memory on 172.23.57.81:42041 (size: 4.9 KiB, free: 434.2 MiB)\n",
      "14:28:18.025 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 35 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:28:18.030 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:28:18.030 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:28:18.057 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.272975 ms\n",
      "14:28:18.061 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36 stored as values in memory (estimated size 200.2 KiB, free 431.3 MiB)\n",
      "14:28:18.072 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 431.3 MiB)\n",
      "14:28:18.072 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_36_piece0 in memory on 172.23.57.81:42041 (size: 34.5 KiB, free: 434.2 MiB)\n",
      "14:28:18.073 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 36 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:18.074 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:28:18.080 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 78 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "14:28:18.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 21 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:28:18.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:18.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:28:18.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:18.081 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 31 (MapPartitionsRDD[78] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:18.085 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37 stored as values in memory (estimated size 18.9 KiB, free 431.3 MiB)\n",
      "14:28:18.086 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 431.3 MiB)\n",
      "14:28:18.087 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_37_piece0 in memory on 172.23.57.81:42041 (size: 8.4 KiB, free: 434.2 MiB)\n",
      "14:28:18.087 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 37 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:18.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[78] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:28:18.088 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 31.0 with 8 tasks resource profile 0\n",
      "14:28:18.089 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 31.0 (TID 59) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.090 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 31.0 (TID 60) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.090 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 31.0 (TID 61) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.090 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 31.0 (TID 62) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.090 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 31.0 (TID 63) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.091 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 31.0 (TID 64) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.091 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 31.0 (TID 65) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.091 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 31.0 (TID 66) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:28:18.092 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 31.0 (TID 60)\n",
      "14:28:18.092 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 31.0 (TID 59)\n",
      "14:28:18.092 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 31.0 (TID 63)\n",
      "14:28:18.092 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 31.0 (TID 62)\n",
      "14:28:18.097 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 31.0 (TID 61)\n",
      "14:28:18.097 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 31.0 (TID 66)\n",
      "14:28:18.097 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 31.0 (TID 64)\n",
      "14:28:18.099 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 31.0 (TID 65)\n",
      "14:28:18.110 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.656895 ms\n",
      "14:28:18.129 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.400083 ms\n",
      "14:28:18.131 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:28:18.131 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:28:18.131 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:28:18.131 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:28:18.131 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:28:18.131 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:28:18.133 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:28:18.133 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-5ffaede9-285a-402a-80ce-e9c6b527d892-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:28:18.418 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_31_piece0 on 172.23.57.81:42041 in memory (size: 8.2 KiB, free: 434.2 MiB)\n",
      "14:28:18.455 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_32_piece0 on 172.23.57.81:42041 in memory (size: 93.4 KiB, free: 434.3 MiB)\n",
      "14:28:18.492 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_34_piece0 on 172.23.57.81:42041 in memory (size: 6.3 KiB, free: 434.3 MiB)\n",
      "14:28:18.510 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_30_piece0 on 172.23.57.81:42041 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
      "14:28:18.731 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 31.0 (TID 59). 2210 bytes result sent to driver\n",
      "14:28:18.734 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 31.0 (TID 59) in 644 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:28:18.736 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 31.0 (TID 61). 2210 bytes result sent to driver\n",
      "14:28:18.738 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 31.0 (TID 61) in 647 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:28:18.764 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 31.0 (TID 66). 2210 bytes result sent to driver\n",
      "14:28:18.770 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 31.0 (TID 66) in 679 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:28:18.782 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 31.0 (TID 62). 2210 bytes result sent to driver\n",
      "14:28:18.782 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 31.0 (TID 60). 2210 bytes result sent to driver\n",
      "14:28:18.783 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 31.0 (TID 62) in 693 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:28:18.786 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 31.0 (TID 60) in 697 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:28:18.812 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 31.0 (TID 63). 2210 bytes result sent to driver\n",
      "14:28:18.819 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 31.0 (TID 63) in 729 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:28:18.895 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 31.0 (TID 64). 2339 bytes result sent to driver\n",
      "14:28:18.896 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 31.0 (TID 64) in 806 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:28:18.917 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 31.0 (TID 65). 2339 bytes result sent to driver\n",
      "14:28:18.918 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 31.0 (TID 65) in 827 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:28:18.918 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "14:28:18.919 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.837 s\n",
      "14:28:18.919 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:28:18.919 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:28:18.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:28:18.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:28:18.928 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:28:18.943 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:18.944 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:18.945 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:18.945 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:18.945 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:18.945 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:18.945 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:19.063 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:28:19.095 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.907557 ms\n",
      "14:28:19.106 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:28:19.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 22 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:28:19.107 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:28:19.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 32)\n",
      "14:28:19.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:28:19.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 33 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:28:19.127 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38 stored as values in memory (estimated size 251.0 KiB, free 431.6 MiB)\n",
      "14:28:19.130 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38_piece0 stored as bytes in memory (estimated size 92.4 KiB, free 431.6 MiB)\n",
      "14:28:19.130 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_38_piece0 in memory on 172.23.57.81:42041 (size: 92.4 KiB, free: 434.2 MiB)\n",
      "14:28:19.131 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 38 from broadcast at DAGScheduler.scala:1585\n",
      "14:28:19.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:28:19.132 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 33.0 with 1 tasks resource profile 0\n",
      "14:28:19.133 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 33.0 (TID 67) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:28:19.134 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 33.0 (TID 67)\n",
      "14:28:19.145 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (457.0 KiB) non-empty blocks including 2 (457.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:28:19.146 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:28:19.182 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.711958 ms\n",
      "14:28:19.185 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:19.185 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:19.186 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:19.186 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:28:19.186 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:28:19.186 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:28:19.187 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:19.188 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:28:19.190 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:28:19.191 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_airports\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 Count_airports;\n",
      "}\n",
      "\n",
      "       \n",
      "14:28:19.293 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 4.812929 ms\n",
      "14:28:19.477 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506041428194203113355287045251_0033_m_000000_67' to file:/home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/_temporary/0/task_202506041428194203113355287045251_0033_m_000000\n",
      "14:28:19.477 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506041428194203113355287045251_0033_m_000000_67: Committed. Elapsed time: 0 ms.\n",
      "14:28:19.479 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 33.0 (TID 67). 7199 bytes result sent to driver\n",
      "14:28:19.481 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 33.0 (TID 67) in 348 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:28:19.481 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "14:28:19.482 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.369 s\n",
      "14:28:19.482 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:28:19.482 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 33: Stage finished\n",
      "14:28:19.483 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.376089 s\n",
      "14:28:19.483 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 5b9a0fe4-fe04-4f06-b74f-7019b4d65730.\n",
      "14:28:19.502 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 5b9a0fe4-fe04-4f06-b74f-7019b4d65730 committed. Elapsed time: 18 ms.\n",
      "14:28:19.503 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 5b9a0fe4-fe04-4f06-b74f-7019b4d65730.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create parquet files\n",
    "\n",
    "try:\n",
    "    avg_flight.write.parquet(config[\"Parquet_file\"][\"avg_flight\"], mode=\"overwrite\")\n",
    "\n",
    "    airline_in_airport.write.parquet(config[\"Parquet_file\"][\"airline_in_airport\"], mode=\"overwrite\")\n",
    "\n",
    "    flights_per_cancell.write.parquet(config[\"Parquet_file\"][\"flights_per_cancell\"], mode=\"overwrite\")\n",
    "\n",
    "    airport_notin_thelist.write.parquet(config[\"Parquet_file\"][\"airport_notin_thelist\"], mode=\"overwrite\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error write parquet files:\" + str(error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
