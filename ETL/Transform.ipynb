{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, max, min, count, broadcast, desc, asc, when, rank, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2650fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cfebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:21.719 [Thread-3] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ebb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:21.882 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "14:53:21.892 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/illidan/proyecto_desde0/ETL/spark-warehouse'.\n",
      "14:53:21.914 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b7b3a0{/SQL,null,AVAILABLE,@Spark}\n",
      "14:53:21.915 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4967ea8{/SQL/json,null,AVAILABLE,@Spark}\n",
      "14:53:21.916 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@774d3745{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "14:53:21.916 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ded8a2c{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "14:53:21.928 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6fbbe68b{/static/sql,null,AVAILABLE,@Spark}\n",
      "14:53:23.225 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 107 ms to list leaf files for 1 paths.\n",
      "14:53:23.986 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:24.009 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:24.010 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:24.010 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:24.012 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:24.018 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:24.113 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "14:53:24.163 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "14:53:24.167 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.23.57.81:37441 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:53:24.173 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:24.196 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:24.198 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "14:53:24.277 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "14:53:24.298 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:24.830 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3053 bytes result sent to driver\n",
      "14:53:24.843 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 594 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:24.844 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "14:53:24.849 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.814 s\n",
      "14:53:24.852 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:24.853 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "14:53:24.854 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.867465 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:25.448 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.23.57.81:37441 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:53:25.745 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 1 paths.\n",
      "14:53:25.788 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:25.789 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:25.789 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:25.789 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:25.789 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:25.791 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:25.800 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "14:53:25.808 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "14:53:25.809 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.23.57.81:37441 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:53:25.810 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:25.811 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:25.811 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "14:53:25.814 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "14:53:25.815 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n",
      "14:53:25.841 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1662 bytes result sent to driver\n",
      "14:53:25.844 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 31 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:25.845 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "14:53:25.846 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.054 s\n",
      "14:53:25.847 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:25.847 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "14:53:25.847 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.059320 s\n",
      "14:53:25.872 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "14:53:25.926 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:25.928 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:25.928 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:25.928 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:25.929 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:25.930 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:25.940 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 102.6 KiB, free 434.2 MiB)\n",
      "14:53:25.951 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.1 MiB)\n",
      "14:53:25.955 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.23.57.81:37441 (size: 36.9 KiB, free: 434.3 MiB)\n",
      "14:53:25.956 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:25.957 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:25.958 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "14:53:25.960 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9225 bytes) \n",
      "14:53:25.962 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "14:53:25.992 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1945 bytes result sent to driver\n",
      "14:53:25.995 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 35 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:25.996 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "14:53:25.997 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.065 s\n",
      "14:53:25.997 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:25.997 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "14:53:25.998 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.071198 s\n",
      "14:53:26.131 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:37441 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:53:26.139 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:37441 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:53:26.624 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:53:26.625 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:53:27.169 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 286.266556 ms\n",
      "14:53:27.198 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)\n",
      "14:53:27.206 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "14:53:27.207 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.23.57.81:37441 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:53:27.208 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from count at NativeMethodAccessorImpl.java:0\n",
      "14:53:27.221 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:27.301 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "14:53:27.306 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:53:27.306 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:53:27.306 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:27.309 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:27.311 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:27.369 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)\n",
      "14:53:27.375 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "14:53:27.377 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.23.57.81:37441 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:53:27.378 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:27.381 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:53:27.382 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 8 tasks resource profile 0\n",
      "14:53:27.389 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.391 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 4) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.392 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 5) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.393 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 6) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.395 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 3.0 (TID 7) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.396 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 3.0 (TID 8) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.397 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 3.0 (TID 9) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.398 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 3.0 (TID 10) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:27.399 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)\n",
      "14:53:27.400 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 4)\n",
      "14:53:27.402 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 5)\n",
      "14:53:27.405 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 3.0 (TID 7)\n",
      "14:53:27.406 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 3.0 (TID 10)\n",
      "14:53:27.406 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 3.0 (TID 9)\n",
      "14:53:27.406 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 3.0 (TID 8)\n",
      "14:53:27.407 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 6)\n",
      "14:53:27.571 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.024552 ms\n",
      "14:53:27.594 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:53:27.597 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:53:27.599 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:53:27.599 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:53:27.601 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:53:27.605 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:53:27.607 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:53:27.609 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:53:27.902 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 3.0 (TID 7). 2222 bytes result sent to driver\n",
      "14:53:27.906 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 3.0 (TID 8). 2222 bytes result sent to driver\n",
      "14:53:27.906 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 4). 2222 bytes result sent to driver\n",
      "14:53:27.906 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 3.0 (TID 10). 2222 bytes result sent to driver\n",
      "14:53:27.907 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 6). 2179 bytes result sent to driver\n",
      "14:53:27.907 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 3.0 (TID 9). 2222 bytes result sent to driver\n",
      "14:53:27.909 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 5). 2222 bytes result sent to driver\n",
      "14:53:27.909 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2222 bytes result sent to driver\n",
      "14:53:27.919 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 3.0 (TID 7) in 524 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:53:27.920 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 6) in 528 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:53:27.922 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 3.0 (TID 9) in 524 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:53:27.924 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 4) in 534 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:53:27.924 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 3.0 (TID 10) in 527 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:53:27.927 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 3.0 (TID 8) in 531 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:53:27.929 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 5) in 538 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:53:27.937 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 553 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:53:27.938 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "14:53:27.939 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.620 s\n",
      "14:53:27.954 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:27.954 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:27.955 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:27.955 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:28.010 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.724447 ms\n",
      "14:53:28.048 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:53:28.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:28.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:53:28.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)\n",
      "14:53:28.052 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:28.055 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:28.067 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
      "14:53:28.082 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "14:53:28.085 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.23.57.81:37441 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:53:28.086 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:28.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:28.089 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "14:53:28.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.23.57.81:37441 in memory (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:53:28.099 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 11) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:28.100 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 11)\n",
      "14:53:28.187 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:28.190 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 15 ms\n",
      "14:53:28.216 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.904434 ms\n",
      "14:53:28.234 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 11). 4038 bytes result sent to driver\n",
      "14:53:28.236 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 11) in 139 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:28.237 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "14:53:28.238 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.174 s\n",
      "14:53:28.238 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:28.238 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished\n",
      "14:53:28.242 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.193042 s\n",
      "14:53:28.293 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:53:28.294 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:53:28.328 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)\n",
      "14:53:28.355 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.23.57.81:37441 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:53:28.355 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "14:53:28.356 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.23.57.81:37441 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:53:28.359 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 6 from count at NativeMethodAccessorImpl.java:0\n",
      "14:53:28.362 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:28.371 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "14:53:28.372 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:28.372 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:53:28.372 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:28.375 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:28.376 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:28.381 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "14:53:28.396 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "14:53:28.397 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.23.57.81:37441 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:53:28.398 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:28.402 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:28.402 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0\n",
      "14:53:28.404 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 12) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:28.405 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 12)\n",
      "14:53:28.420 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:53:28.442 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 12). 2222 bytes result sent to driver\n",
      "14:53:28.444 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 12) in 40 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:28.444 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "14:53:28.449 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.072 s\n",
      "14:53:28.449 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:28.449 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:28.449 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:28.450 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:28.506 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:53:28.508 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:28.508 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:53:28.508 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)\n",
      "14:53:28.508 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:28.510 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:28.513 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)\n",
      "14:53:28.514 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)\n",
      "14:53:28.515 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.23.57.81:37441 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:53:28.516 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:28.517 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:28.517 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks resource profile 0\n",
      "14:53:28.520 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 13) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:28.521 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 13)\n",
      "14:53:28.527 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:28.527 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:28.539 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.23.57.81:37441 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:53:28.539 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 13). 4038 bytes result sent to driver\n",
      "14:53:28.543 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 13) in 24 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:28.545 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "14:53:28.546 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.035 s\n",
      "14:53:28.547 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:28.547 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished\n",
      "14:53:28.548 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.042064 s\n",
      "14:53:28.594 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:53:28.594 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:53:28.628 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)\n",
      "14:53:28.647 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.23.57.81:37441 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:53:28.652 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "14:53:28.653 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.23.57.81:37441 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:53:28.654 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 9 from count at NativeMethodAccessorImpl.java:0\n",
      "14:53:28.655 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:28.660 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 172.23.57.81:37441 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:53:28.663 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 23 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "14:53:28.664 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:28.664 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:53:28.664 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:28.669 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:28.671 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:28.677 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "14:53:28.679 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "14:53:28.680 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.23.57.81:37441 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:53:28.682 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:28.684 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:28.693 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0\n",
      "14:53:28.700 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 14) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9684 bytes) \n",
      "14:53:28.701 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 14)\n",
      "14:53:28.707 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:53:28.731 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 14). 2222 bytes result sent to driver\n",
      "14:53:28.733 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 14) in 34 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:28.736 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "14:53:28.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.064 s\n",
      "14:53:28.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:28.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:28.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:28.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:28.793 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:53:28.795 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:28.796 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:53:28.796 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)\n",
      "14:53:28.796 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:28.797 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:28.800 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)\n",
      "14:53:28.803 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)\n",
      "14:53:28.804 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.23.57.81:37441 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:53:28.805 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:28.806 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:28.806 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0\n",
      "14:53:28.809 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 15) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:28.810 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 15)\n",
      "14:53:28.817 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:28.817 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:28.822 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 15). 3995 bytes result sent to driver\n",
      "14:53:28.825 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 15) in 16 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:28.825 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "14:53:28.826 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
      "14:53:28.827 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:28.827 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished\n",
      "14:53:28.828 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.033511 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1952a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #avg per fly\n",
    "    avg_flight = df_flights.join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                        .groupBy(df_flights.AIRLINE, df_airline.AIRLINE) \\\n",
    "                        .agg(round(avg(df_flights.DISTANCE), 2).alias(\"avg_DISTANCE\")) \\\n",
    "                        .orderBy(desc(\"avg_DISTANCE\")) \\\n",
    "                        .select(df_airline.AIRLINE.alias(\"AIRLINE_Name\"), \"avg_DISTANCE\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable avg_flight:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237699b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    #how many times a flight visit an airport\n",
    "    airline_in_airport = df_flights.join(broadcast(df_airports), df_flights.ORIGIN_AIRPORT == df_airports.IATA_CODE) \\\n",
    "                                .join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                                .repartition(df_flights.DESTINATION_AIRPORT) \\\n",
    "                                .groupBy(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE) \\\n",
    "                                    .agg(count(df_flights.AIRLINE).alias(\"Count_visit_per_airline\")) \\\n",
    "                                .orderBy(asc(df_flights.DESTINATION_AIRPORT)) \\\n",
    "                                .select(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE,\"Count_visit_per_airline\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airline_in_airport:\" + str(error))\n",
    "\n",
    "#a Rank of wich airline is the most visited in each airport\n",
    "try:\n",
    "    window_spec = Window.partitionBy(\"DESTINATION_AIRPORT\").orderBy(desc(\"Count_visit_per_airline\"))\n",
    "    \n",
    "    Rank_airline_in_airport = airline_in_airport.withColumn(\"Ranking\", rank().over(window_spec))\n",
    "\n",
    "\n",
    "except Exception as error:\n",
    "    logger.error(\"Error Rank_airline_in_airport:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad20a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #mention how many times an airline visit a destination and canceled this arrival\n",
    "    flights_per_cancell = df_flights.select(col(\"AIRLINE\"), col(\"CANCELLED\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .filter(col(\"CANCELLED\") == \"1\") \\\n",
    "                                .repartition(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .groupBy(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                    .agg(count(col(\"AIRLINE\")).alias(\"count_airlines_cancel\")) \\\n",
    "                                .orderBy(desc(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .limit(100)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable flights_per_cancell:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7929cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #i create this variable because i need to know how many airports have flights but are not in the list of airports\n",
    "    airport_notin_thelist = df_flights.join(broadcast(df_airports), df_flights.DESTINATION_AIRPORT == df_airports.IATA_CODE, \"left_anti\") \\\n",
    "                                        .select(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .repartition(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .groupBy(\"DESTINATION_AIRPORT\").agg(count(\"*\").alias(\"Count_airports\"))\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airport_notin_thelist:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7983760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:29.735 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 172.23.57.81:37441 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:53:29.741 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 172.23.57.81:37441 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:53:29.757 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "14:53:29.758 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "14:53:29.762 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:53:29.762 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "14:53:29.868 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 28.891761 ms\n",
      "14:53:29.877 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 200.3 KiB, free 433.7 MiB)\n",
      "14:53:29.886 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.7 MiB)\n",
      "14:53:29.888 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "14:53:29.890 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:29.893 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:29.914 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:29.915 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:53:29.915 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:53:29.915 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:29.916 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:29.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:53:29.920 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 15.1 KiB, free 433.7 MiB)\n",
      "14:53:29.923 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.7 MiB)\n",
      "14:53:29.925 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.23.57.81:37441 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "14:53:29.926 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:29.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:29.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks resource profile 0\n",
      "14:53:29.928 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "14:53:29.929 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)\n",
      "14:53:29.960 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.946828 ms\n",
      "14:53:29.962 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:53:29.986 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:53:30.135 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:53:30.299 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.23.57.81:37441 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:53:30.354 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 2241 bytes result sent to driver\n",
      "14:53:30.356 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 427 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:30.356 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "14:53:30.357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.438 s\n",
      "14:53:30.357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:30.357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished\n",
      "14:53:30.358 [broadcast-exchange-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.443235 s\n",
      "14:53:30.388 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.074441 ms\n",
      "14:53:30.394 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 2.0 MiB, free 431.9 MiB)\n",
      "14:53:30.400 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 686.0 B, free 431.9 MiB)\n",
      "14:53:30.400 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.23.57.81:37441 (size: 686.0 B, free: 434.3 MiB)\n",
      "14:53:30.401 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:30.413 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "14:53:30.413 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "14:53:30.596 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 90.599523 ms\n",
      "14:53:30.602 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 200.3 KiB, free 431.7 MiB)\n",
      "14:53:30.614 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 431.7 MiB)\n",
      "14:53:30.615 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 172.23.57.81:37441 (size: 34.7 KiB, free: 434.3 MiB)\n",
      "14:53:30.616 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 15 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:30.617 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:30.701 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 34 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "14:53:30.702 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 10 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:53:30.702 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:30.702 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:30.704 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:30.705 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:30.712 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 53.5 KiB, free 431.6 MiB)\n",
      "14:53:30.727 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 431.6 MiB)\n",
      "14:53:30.727 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 172.23.57.81:37441 (size: 23.2 KiB, free: 434.3 MiB)\n",
      "14:53:30.729 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:30.729 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 172.23.57.81:37441 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "14:53:30.730 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:53:30.730 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 8 tasks resource profile 0\n",
      "14:53:30.732 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 17) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.733 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 18) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.733 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 13.0 (TID 19) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.733 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 13.0 (TID 20) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.734 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 13.0 (TID 21) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.734 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 13.0 (TID 22) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.734 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 13.0 (TID 23) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.734 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 13.0 (TID 24) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:30.735 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 13.0 (TID 19)\n",
      "14:53:30.735 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 13.0 (TID 20)\n",
      "14:53:30.736 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 18)\n",
      "14:53:30.736 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 13.0 (TID 23)\n",
      "14:53:30.736 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 13.0 (TID 24)\n",
      "14:53:30.739 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 13.0 (TID 21)\n",
      "14:53:30.740 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 17)\n",
      "14:53:30.740 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 13.0 (TID 22)\n",
      "14:53:30.818 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.23.57.81:37441 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:53:30.883 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 108.002904 ms\n",
      "14:53:30.910 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.047237 ms\n",
      "14:53:30.924 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.335989 ms\n",
      "14:53:30.975 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.095781 ms\n",
      "14:53:31.025 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 42.308447 ms\n",
      "14:53:31.041 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:53:31.042 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:53:31.044 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:53:31.046 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:53:31.050 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:53:31.056 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:53:31.063 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:53:31.067 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.071 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:53:31.071 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.073 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.073 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.076 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.082 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.087 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.089 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:53:31.171 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:53:31.172 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:53:31.176 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:53:31.176 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:53:31.180 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:53:31.184 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:53:31.188 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:33.024 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 13.0 (TID 22). 3926 bytes result sent to driver\n",
      "14:53:33.024 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 13.0 (TID 23). 3926 bytes result sent to driver\n",
      "14:53:33.026 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 13.0 (TID 23) in 2292 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:53:33.026 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 13.0 (TID 22) in 2292 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:53:33.029 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 13.0 (TID 19). 3926 bytes result sent to driver\n",
      "14:53:33.032 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 13.0 (TID 19) in 2299 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:53:33.040 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 13.0 (TID 24). 3926 bytes result sent to driver\n",
      "14:53:33.041 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 13.0 (TID 24) in 2307 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:53:33.046 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 13.0 (TID 21). 3926 bytes result sent to driver\n",
      "14:53:33.048 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 13.0 (TID 21) in 2315 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:53:33.054 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 13.0 (TID 20). 3926 bytes result sent to driver\n",
      "14:53:33.057 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 13.0 (TID 20) in 2323 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:53:33.058 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 17). 3926 bytes result sent to driver\n",
      "14:53:33.060 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 17) in 2328 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:53:33.112 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 13.0 (TID 18). 3926 bytes result sent to driver\n",
      "14:53:33.113 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 13.0 (TID 18) in 2381 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:53:33.113 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "14:53:33.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.407 s\n",
      "14:53:33.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:33.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:33.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:33.114 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:33.130 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:53:33.154 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:53:33.188 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.787073 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:33.262 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.535816 ms\n",
      "14:53:33.319 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:33.320 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:33.320 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:33.320 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)\n",
      "14:53:33.321 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:33.322 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:33.332 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 54.3 KiB, free 431.8 MiB)\n",
      "14:53:33.333 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 431.8 MiB)\n",
      "14:53:33.334 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 172.23.57.81:37441 (size: 24.0 KiB, free: 434.3 MiB)\n",
      "14:53:33.334 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:33.338 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:33.338 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0\n",
      "14:53:33.340 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 25) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:33.341 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 25)\n",
      "14:53:33.356 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:33.357 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:53:33.387 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.654557 ms\n",
      "14:53:33.400 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.590885 ms\n",
      "14:53:33.427 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 25). 6934 bytes result sent to driver\n",
      "14:53:33.428 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 25) in 88 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:33.428 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "14:53:33.429 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.100 s\n",
      "14:53:33.430 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:33.430 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished\n",
      "14:53:33.431 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.111737 s\n",
      "14:53:33.442 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 40 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "14:53:33.442 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:33.442 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:33.442 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)\n",
      "14:53:33.444 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:33.445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:33.457 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 54.7 KiB, free 431.7 MiB)\n",
      "14:53:33.458 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 431.7 MiB)\n",
      "14:53:33.459 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 172.23.57.81:37441 (size: 24.3 KiB, free: 434.3 MiB)\n",
      "14:53:33.460 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:33.460 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:33.461 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks resource profile 0\n",
      "14:53:33.462 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 26) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:33.463 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 26)\n",
      "14:53:33.495 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 172.23.57.81:37441 in memory (size: 23.2 KiB, free: 434.3 MiB)\n",
      "14:53:33.511 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.485719 ms\n",
      "14:53:33.521 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:33.522 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "14:53:33.548 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 26). 6433 bytes result sent to driver\n",
      "14:53:33.549 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 26) in 87 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:33.549 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "14:53:33.550 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.100 s\n",
      "14:53:33.551 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:33.551 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:33.551 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:33.551 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:33.558 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:53:33.610 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:33.630 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:33.630 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:33.632 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:33.632 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:33.632 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:33.633 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:33.697 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.399234 ms\n",
      "14:53:33.713 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:33.714 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:33.715 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:33.715 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)\n",
      "14:53:33.717 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:33.719 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:33.746 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 250.3 KiB, free 431.5 MiB)\n",
      "14:53:33.750 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 431.5 MiB)\n",
      "14:53:33.750 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 172.23.57.81:37441 (size: 93.3 KiB, free: 434.2 MiB)\n",
      "14:53:33.751 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:33.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:33.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks resource profile 0\n",
      "14:53:33.753 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 27) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:33.754 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 27)\n",
      "14:53:33.790 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1323.0 B) non-empty blocks including 1 (1323.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:33.790 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:53:33.808 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.994363 ms\n",
      "14:53:33.821 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.935977 ms\n",
      "14:53:33.838 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.784966 ms\n",
      "14:53:33.848 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:33.848 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:33.848 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:33.849 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:33.849 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:33.849 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:33.854 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:33.857 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:33.878 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:53:33.886 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE_Name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_DISTANCE\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE_Name (STRING);\n",
      "  optional double avg_DISTANCE;\n",
      "}\n",
      "\n",
      "       \n",
      "14:53:33.930 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]\n",
      "14:53:34.034 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131453331927893630041150671_0020_m_000000_27' to file:/home/illidan/proyecto_desde0/archivos_parquet/avg_flight/_temporary/0/task_202506131453331927893630041150671_0020_m_000000\n",
      "14:53:34.035 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131453331927893630041150671_0020_m_000000_27: Committed. Elapsed time: 0 ms.\n",
      "14:53:34.042 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 27). 8767 bytes result sent to driver\n",
      "14:53:34.044 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 27) in 291 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:34.044 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "14:53:34.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.325 s\n",
      "14:53:34.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:34.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished\n",
      "14:53:34.046 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.333414 s\n",
      "14:53:34.048 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job f73b7e82-bd0f-45eb-b7aa-d92ad14f3558.\n",
      "14:53:34.075 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job f73b7e82-bd0f-45eb-b7aa-d92ad14f3558 committed. Elapsed time: 25 ms.\n",
      "14:53:34.078 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job f73b7e82-bd0f-45eb-b7aa-d92ad14f3558.\n",
      "14:53:34.233 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:53:34.233 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:53:34.235 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:53:34.236 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "14:53:34.237 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:53:34.237 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "14:53:34.316 [broadcast-exchange-1] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.090981 ms\n",
      "14:53:34.316 [broadcast-exchange-2] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.054159 ms\n",
      "14:53:34.321 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 200.2 KiB, free 431.3 MiB)\n",
      "14:53:34.321 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 200.3 KiB, free 431.1 MiB)\n",
      "14:53:34.332 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.0 MiB)\n",
      "14:53:34.332 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.0 MiB)\n",
      "14:53:34.332 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:53:34.333 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 434.1 MiB)\n",
      "14:53:34.333 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:34.333 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:34.335 [broadcast-exchange-1] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:34.335 [broadcast-exchange-2] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:34.349 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:34.350 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:53:34.350 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:53:34.350 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:34.351 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:34.353 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:53:34.351 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:34.356 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 15.1 KiB, free 431.0 MiB)\n",
      "14:53:34.358 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 431.0 MiB)\n",
      "14:53:34.360 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 172.23.57.81:37441 (size: 6.5 KiB, free: 434.1 MiB)\n",
      "14:53:34.361 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:34.361 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:34.362 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks resource profile 0\n",
      "14:53:34.363 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 28) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "14:53:34.363 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:53:34.363 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:53:34.363 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:34.363 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:34.365 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:53:34.365 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 28)\n",
      "14:53:34.367 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 14.5 KiB, free 431.0 MiB)\n",
      "14:53:34.379 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 431.0 MiB)\n",
      "14:53:34.380 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 172.23.57.81:37441 (size: 6.3 KiB, free: 434.1 MiB)\n",
      "14:53:34.381 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:34.382 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:34.382 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 1 tasks resource profile 0\n",
      "14:53:34.384 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 172.23.57.81:37441 in memory (size: 24.0 KiB, free: 434.1 MiB)\n",
      "14:53:34.393 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 172.23.57.81:37441 in memory (size: 93.3 KiB, free: 434.2 MiB)\n",
      "14:53:34.396 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 29) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "14:53:34.396 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.741909 ms\n",
      "14:53:34.398 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 29)\n",
      "14:53:34.398 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:53:34.409 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:53:34.419 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 28). 2241 bytes result sent to driver\n",
      "14:53:34.420 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 172.23.57.81:37441 in memory (size: 24.3 KiB, free: 434.3 MiB)\n",
      "14:53:34.424 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.354868 ms\n",
      "14:53:34.424 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 28) in 62 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:34.424 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "14:53:34.425 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.071 s\n",
      "14:53:34.425 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:34.426 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 21: Stage finished\n",
      "14:53:34.426 [broadcast-exchange-2] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.076351 s\n",
      "14:53:34.428 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:53:34.430 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 2.0 MiB, free 429.4 MiB)\n",
      "14:53:34.452 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:53:34.455 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 686.0 B, free 429.4 MiB)\n",
      "14:53:34.456 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 172.23.57.81:37441 (size: 686.0 B, free: 434.3 MiB)\n",
      "14:53:34.457 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 24 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:34.465 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 29). 3613 bytes result sent to driver\n",
      "14:53:34.468 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 29) in 76 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:34.469 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "14:53:34.469 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:53:34.470 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:53:34.470 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.105 s\n",
      "14:53:34.471 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:34.471 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished\n",
      "14:53:34.472 [broadcast-exchange-1] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.117099 s\n",
      "14:53:34.479 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 2.0 MiB, free 427.4 MiB)\n",
      "14:53:34.482 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 427.4 MiB)\n",
      "14:53:34.482 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 172.23.57.81:37441 (size: 4.9 KiB, free: 434.2 MiB)\n",
      "14:53:34.483 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:34.488 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:53:34.489 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:53:34.544 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.228264 ms\n",
      "14:53:34.549 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 200.5 KiB, free 427.2 MiB)\n",
      "14:53:34.557 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 427.2 MiB)\n",
      "14:53:34.558 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 172.23.57.81:37441 (size: 34.7 KiB, free: 434.2 MiB)\n",
      "14:53:34.559 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 26 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:34.560 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:34.565 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 55 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "14:53:34.566 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 16 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:53:34.566 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:34.566 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:34.566 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:34.567 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:34.575 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 22.1 KiB, free 427.2 MiB)\n",
      "14:53:34.576 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 427.2 MiB)\n",
      "14:53:34.577 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 172.23.57.81:37441 (size: 9.0 KiB, free: 434.2 MiB)\n",
      "14:53:34.577 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:34.578 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:53:34.578 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 8 tasks resource profile 0\n",
      "14:53:34.579 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 30) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.580 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 31) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.580 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 32) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.581 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 33) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.581 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 23.0 (TID 34) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.581 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 23.0 (TID 35) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.582 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 23.0 (TID 36) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.582 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 23.0 (TID 37) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:34.583 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 31)\n",
      "14:53:34.583 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 30)\n",
      "14:53:34.583 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 32)\n",
      "14:53:34.583 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 23.0 (TID 34)\n",
      "14:53:34.583 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 23.0 (TID 37)\n",
      "14:53:34.583 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 23.0 (TID 35)\n",
      "14:53:34.584 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 33)\n",
      "14:53:34.585 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 23.0 (TID 36)\n",
      "14:53:34.609 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.870182 ms\n",
      "14:53:34.624 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.977632 ms\n",
      "14:53:34.626 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:53:34.626 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:53:34.626 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:53:34.626 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:53:34.626 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:53:34.626 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:53:34.626 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:53:34.627 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:53:34.637 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.637 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.642 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.644 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.645 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.648 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.651 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.652 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:53:34.684 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 172.23.57.81:37441 in memory (size: 6.5 KiB, free: 434.2 MiB)\n",
      "14:53:34.723 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 172.23.57.81:37441 in memory (size: 6.3 KiB, free: 434.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:35.190 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 172.23.57.81:37441 in memory (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:53:35.247 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 172.23.57.81:37441 in memory (size: 686.0 B, free: 434.3 MiB)\n",
      "14:53:35.312 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 172.23.57.81:37441 in memory (size: 34.7 KiB, free: 434.3 MiB)\n",
      "14:53:38.767 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 23.0 (TID 36). 2451 bytes result sent to driver\n",
      "14:53:38.773 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 23.0 (TID 36) in 4191 ms on 172.23.57.81 (executor driver) (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:39.565 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 23.0 (TID 37). 2451 bytes result sent to driver\n",
      "14:53:39.567 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 23.0 (TID 37) in 4985 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:53:39.639 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 30). 2451 bytes result sent to driver\n",
      "14:53:39.641 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 30) in 5061 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:53:39.664 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 32). 2451 bytes result sent to driver\n",
      "14:53:39.666 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 32) in 5086 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:53:39.681 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 23.0 (TID 34). 2451 bytes result sent to driver\n",
      "14:53:39.682 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 23.0 (TID 34) in 5101 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:53:39.705 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 31). 2451 bytes result sent to driver\n",
      "14:53:39.706 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 31) in 5126 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:53:39.707 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 33). 2451 bytes result sent to driver\n",
      "14:53:39.708 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 33) in 5127 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:53:39.737 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 23.0 (TID 35). 2451 bytes result sent to driver\n",
      "14:53:39.738 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 23.0 (TID 35) in 5157 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:53:39.738 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "14:53:39.739 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0) finished in 5.170 s\n",
      "14:53:39.739 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:39.739 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:39.739 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:39.739 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:39.747 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(5), advisory target size: 67108864, actual target size 3493000, minimum partition size: 1048576\n",
      "14:53:39.762 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:53:39.812 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.136255 ms\n",
      "14:53:39.851 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.843314 ms\n",
      "14:53:39.878 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:39.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 17 (parquet at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "14:53:39.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:39.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)\n",
      "14:53:39.881 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:39.882 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[60] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:39.900 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 62.4 KiB, free 429.6 MiB)\n",
      "14:53:39.909 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.0 KiB, free 429.6 MiB)\n",
      "14:53:39.909 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 172.23.57.81:37441 (size: 26.0 KiB, free: 434.3 MiB)\n",
      "14:53:39.910 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:39.911 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 25 (MapPartitionsRDD[60] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "14:53:39.912 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 10 tasks resource profile 0\n",
      "14:53:39.914 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 38) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.914 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 25.0 (TID 39) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.915 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 25.0 (TID 40) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.915 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 25.0 (TID 41) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.916 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 25.0 (TID 42) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.916 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 25.0 (TID 43) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.916 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 25.0 (TID 44) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.917 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 25.0 (TID 45) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8999 bytes) \n",
      "14:53:39.919 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 25.0 (TID 40)\n",
      "14:53:39.919 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 25.0 (TID 41)\n",
      "14:53:39.921 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 25.0 (TID 42)\n",
      "14:53:39.921 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 25.0 (TID 45)\n",
      "14:53:39.921 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 38)\n",
      "14:53:39.925 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 25.0 (TID 44)\n",
      "14:53:39.925 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 25.0 (TID 43)\n",
      "14:53:39.925 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 25.0 (TID 39)\n",
      "14:53:39.946 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.946 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:53:39.954 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.954 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:39.954 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.955 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:53:39.956 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.956 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms\n",
      "14:53:39.951 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.951 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.957 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms\n",
      "14:53:39.957 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms\n",
      "14:53:39.948 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.947 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:39.959 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms\n",
      "14:53:39.959 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms\n",
      "14:53:39.999 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 172.23.57.81:37441 in memory (size: 9.0 KiB, free: 434.3 MiB)\n",
      "14:53:40.040 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 84.175138 ms\n",
      "14:53:40.052 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.105751 ms\n",
      "14:53:40.062 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.119081 ms\n",
      "14:53:40.072 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.459256 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                        (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:40.635 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 25.0 (TID 45). 9534 bytes result sent to driver\n",
      "14:53:40.637 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 25.0 (TID 45) in 721 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "14:53:40.638 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 25.0 (TID 46) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8999 bytes) \n",
      "14:53:40.642 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 25.0 (TID 46)\n",
      "14:53:40.654 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:40.654 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:40.792 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 25.0 (TID 43). 11058 bytes result sent to driver\n",
      "14:53:40.797 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 25.0 (TID 47) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8999 bytes) \n",
      "14:53:40.798 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 25.0 (TID 39). 13738 bytes result sent to driver\n",
      "14:53:40.798 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 25.0 (TID 47)\n",
      "14:53:40.799 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 25.0 (TID 43) in 883 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "14:53:40.801 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 25.0 (TID 39) in 887 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "14:53:40.814 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:40.814 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:40.828 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 25.0 (TID 44). 13408 bytes result sent to driver\n",
      "14:53:40.829 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 25.0 (TID 44) in 913 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "14:53:40.838 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 25.0 (TID 40). 12911 bytes result sent to driver\n",
      "14:53:40.839 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 25.0 (TID 40) in 924 ms on 172.23.57.81 (executor driver) (5/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:40.872 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 25.0 (TID 42). 12870 bytes result sent to driver\n",
      "14:53:40.873 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 25.0 (TID 42) in 958 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "14:53:40.901 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 25.0 (TID 41). 14066 bytes result sent to driver\n",
      "14:53:40.906 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 25.0 (TID 41) in 991 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "14:53:40.940 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 38). 17238 bytes result sent to driver\n",
      "14:53:40.942 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 38) in 1028 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "14:53:40.973 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 25.0 (TID 46). 9987 bytes result sent to driver\n",
      "14:53:40.974 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 25.0 (TID 46) in 337 ms on 172.23.57.81 (executor driver) (9/10)\n",
      "14:53:41.025 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 25.0 (TID 47). 11676 bytes result sent to driver\n",
      "14:53:41.026 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 25.0 (TID 47) in 229 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "14:53:41.027 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "14:53:41.027 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.130 s\n",
      "14:53:41.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:41.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 25: Stage finished\n",
      "14:53:41.028 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.149390 s\n",
      "14:53:41.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 61 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "14:53:41.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 18 (parquet at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "14:53:41.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:41.044 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)\n",
      "14:53:41.045 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:41.048 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 27 (MapPartitionsRDD[61] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:41.062 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 70.4 KiB, free 429.5 MiB)\n",
      "14:53:41.064 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 429.5 MiB)\n",
      "14:53:41.065 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 172.23.57.81:37441 (size: 27.5 KiB, free: 434.2 MiB)\n",
      "14:53:41.066 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:41.066 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[61] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "14:53:41.067 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 10 tasks resource profile 0\n",
      "14:53:41.069 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 48) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.069 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 49) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.070 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 50) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.070 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 27.0 (TID 51) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.070 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 27.0 (TID 52) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.071 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 27.0 (TID 53) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.071 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 27.0 (TID 54) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.072 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 27.0 (TID 55) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.072 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 50)\n",
      "14:53:41.072 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 27.0 (TID 54)\n",
      "14:53:41.073 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 27.0 (TID 52)\n",
      "14:53:41.072 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 49)\n",
      "14:53:41.074 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 27.0 (TID 51)\n",
      "14:53:41.072 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 48)\n",
      "14:53:41.075 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 27.0 (TID 55)\n",
      "14:53:41.077 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 27.0 (TID 53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:41.092 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.82895 ms\n",
      "14:53:41.099 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.099 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:41.100 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.100 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.100 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms\n",
      "14:53:41.100 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:41.102 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.103 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:41.103 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.103 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:41.105 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.105 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms\n",
      "14:53:41.106 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.106 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms\n",
      "14:53:41.110 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.110 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:41.271 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_28_piece0 on 172.23.57.81:37441 in memory (size: 26.0 KiB, free: 434.3 MiB)\n",
      "14:53:41.375 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 27.0 (TID 55). 7505 bytes result sent to driver\n",
      "14:53:41.376 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 27.0 (TID 56) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.379 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 27.0 (TID 55) in 307 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "14:53:41.385 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 27.0 (TID 56)\n",
      "14:53:41.393 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.394 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:41.539 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 27.0 (TID 53). 7548 bytes result sent to driver\n",
      "14:53:41.540 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 27.0 (TID 57) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8988 bytes) \n",
      "14:53:41.543 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 27.0 (TID 53) in 472 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "14:53:41.543 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 27.0 (TID 57)\n",
      "14:53:41.562 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:41.562 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:41.778 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 49). 7505 bytes result sent to driver\n",
      "14:53:41.794 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 49) in 725 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "14:53:41.806 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 48). 7505 bytes result sent to driver\n",
      "14:53:41.808 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 27.0 (TID 56). 7505 bytes result sent to driver\n",
      "14:53:41.818 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 27.0 (TID 56) in 443 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "14:53:41.821 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 48) in 753 ms on 172.23.57.81 (executor driver) (5/10)\n",
      "14:53:41.823 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 27.0 (TID 54). 7505 bytes result sent to driver\n",
      "14:53:41.829 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 50). 7505 bytes result sent to driver\n",
      "14:53:41.834 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 27.0 (TID 54) in 763 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "14:53:41.835 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 50) in 765 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "14:53:41.837 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 27.0 (TID 51). 7505 bytes result sent to driver\n",
      "14:53:41.843 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 27.0 (TID 51) in 773 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "14:53:41.861 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 27.0 (TID 52). 7505 bytes result sent to driver\n",
      "14:53:41.862 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 27.0 (TID 52) in 792 ms on 172.23.57.81 (executor driver) (9/10)\n",
      "14:53:41.940 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 27.0 (TID 57). 7505 bytes result sent to driver\n",
      "14:53:41.942 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 27.0 (TID 57) in 402 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "14:53:41.943 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.885 s\n",
      "14:53:41.943 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:41.943 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:41.943 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:41.943 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "14:53:41.943 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:41.950 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:53:41.976 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:41.977 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:41.977 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:41.978 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:41.978 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:41.978 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:41.978 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:42.051 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.821854 ms\n",
      "14:53:42.083 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:42.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 19 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:42.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 30 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:42.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 29)\n",
      "14:53:42.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:42.087 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 30 (MapPartitionsRDD[65] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:42.119 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 256.9 KiB, free 429.3 MiB)\n",
      "14:53:42.121 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 95.3 KiB, free 429.2 MiB)\n",
      "14:53:42.121 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 172.23.57.81:37441 (size: 95.3 KiB, free: 434.2 MiB)\n",
      "14:53:42.122 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:42.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[65] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:42.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 30.0 with 1 tasks resource profile 0\n",
      "14:53:42.124 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 30.0 (TID 58) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:42.125 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 30.0 (TID 58)\n",
      "14:53:42.163 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 10 (63.2 KiB) non-empty blocks including 10 (63.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:42.163 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:53:42.175 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.440672 ms\n",
      "14:53:42.185 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.58819 ms\n",
      "14:53:42.198 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.702813 ms\n",
      "14:53:42.216 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.031834 ms\n",
      "14:53:42.268 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.601124 ms\n",
      "14:53:42.287 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.757441 ms\n",
      "14:53:42.295 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.784702 ms\n",
      "14:53:42.299 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:42.299 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:42.300 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:42.300 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:42.300 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:42.300 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:42.300 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:42.301 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:42.303 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:53:42.305 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_visit_per_airline\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Ranking\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  optional binary AIRLINE (STRING);\n",
      "  required int64 Count_visit_per_airline;\n",
      "  required int32 Ranking;\n",
      "}\n",
      "\n",
      "       \n",
      "14:53:42.444 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131453424950518703728595444_0030_m_000000_58' to file:/home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/_temporary/0/task_202506131453424950518703728595444_0030_m_000000\n",
      "14:53:42.445 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131453424950518703728595444_0030_m_000000_58: Committed. Elapsed time: 0 ms.\n",
      "14:53:42.446 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 30.0 (TID 58). 9788 bytes result sent to driver\n",
      "14:53:42.447 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 30.0 (TID 58) in 323 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:42.447 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "14:53:42.448 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 30 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.354 s\n",
      "14:53:42.448 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:42.449 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 30: Stage finished\n",
      "14:53:42.449 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.365233 s\n",
      "14:53:42.450 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job b1b5a451-3bf7-4524-bcfe-b9eb73a66bdf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:42.480 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job b1b5a451-3bf7-4524-bcfe-b9eb73a66bdf committed. Elapsed time: 30 ms.\n",
      "14:53:42.481 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job b1b5a451-3bf7-4524-bcfe-b9eb73a66bdf.\n",
      "14:53:42.537 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(CANCELLED),EqualTo(CANCELLED,1)\n",
      "14:53:42.537 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(CANCELLED#24),(CANCELLED#24 = 1)\n",
      "14:53:42.583 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.749364 ms\n",
      "14:53:42.587 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 200.5 KiB, free 429.1 MiB)\n",
      "14:53:42.597 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 429.0 MiB)\n",
      "14:53:42.598 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 172.23.57.81:37441 (size: 34.8 KiB, free: 434.1 MiB)\n",
      "14:53:42.599 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 31 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:42.600 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:42.605 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 69 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "14:53:42.606 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 20 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:53:42.606 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:42.606 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:42.606 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:42.607 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 31 (MapPartitionsRDD[69] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:42.612 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 18.8 KiB, free 429.0 MiB)\n",
      "14:53:42.614 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 429.0 MiB)\n",
      "14:53:42.615 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 172.23.57.81:37441 (size: 8.2 KiB, free: 434.1 MiB)\n",
      "14:53:42.616 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:42.617 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[69] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:53:42.617 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 31.0 with 8 tasks resource profile 0\n",
      "14:53:42.620 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 31.0 (TID 59) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.620 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 31.0 (TID 60) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.621 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 31.0 (TID 61) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.621 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 31.0 (TID 62) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.621 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 31.0 (TID 63) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.621 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 31.0 (TID 64) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.622 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 31.0 (TID 65) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.622 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 31.0 (TID 66) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:42.623 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 31.0 (TID 63)\n",
      "14:53:42.623 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 31.0 (TID 59)\n",
      "14:53:42.624 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 31.0 (TID 60)\n",
      "14:53:42.626 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 31.0 (TID 66)\n",
      "14:53:42.625 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 31.0 (TID 62)\n",
      "14:53:42.625 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 31.0 (TID 64)\n",
      "14:53:42.625 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 31.0 (TID 65)\n",
      "14:53:42.624 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 31.0 (TID 61)\n",
      "14:53:42.646 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.520081 ms\n",
      "14:53:42.650 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:53:42.651 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:53:42.651 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:53:42.652 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:53:42.652 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:53:42.651 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:53:42.653 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:53:42.659 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:53:42.664 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:42.666 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:42.665 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:42.665 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:42.668 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:42.669 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:42.670 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:42.673 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:53:43.165 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 31.0 (TID 63). 2296 bytes result sent to driver\n",
      "14:53:43.174 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 31.0 (TID 63) in 553 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:53:43.185 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 31.0 (TID 65). 2296 bytes result sent to driver\n",
      "14:53:43.190 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 31.0 (TID 65) in 568 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:53:43.291 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 31.0 (TID 66). 2296 bytes result sent to driver\n",
      "14:53:43.293 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 31.0 (TID 66) in 671 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:53:43.346 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 31.0 (TID 59). 2296 bytes result sent to driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:43.367 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 31.0 (TID 59) in 748 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:53:43.454 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 31.0 (TID 64). 2296 bytes result sent to driver\n",
      "14:53:43.455 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 31.0 (TID 64) in 834 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:53:43.456 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 31.0 (TID 61). 2296 bytes result sent to driver\n",
      "14:53:43.458 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 31.0 (TID 61) in 838 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:53:43.462 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 31.0 (TID 62). 2296 bytes result sent to driver\n",
      "14:53:43.464 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 31.0 (TID 62) in 843 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:53:43.476 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 31.0 (TID 60). 2296 bytes result sent to driver\n",
      "14:53:43.477 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 31.0 (TID 60) in 857 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:53:43.477 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "14:53:43.478 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.869 s\n",
      "14:53:43.478 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:43.478 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:43.478 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:43.478 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:43.485 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:53:43.495 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:43.496 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:43.497 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:43.497 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:43.497 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:43.497 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:43.497 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:43.536 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.39883 ms\n",
      "14:53:43.537 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:53:43.584 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 36.956688 ms\n",
      "14:53:43.602 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:43.604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 21 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:43.604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:43.604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 32)\n",
      "14:53:43.604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:43.605 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 33 (MapPartitionsRDD[73] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:43.633 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 254.2 KiB, free 428.7 MiB)\n",
      "14:53:43.635 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 428.7 MiB)\n",
      "14:53:43.635 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on 172.23.57.81:37441 (size: 93.3 KiB, free: 434.0 MiB)\n",
      "14:53:43.636 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 33 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:43.637 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[73] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:43.637 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 33.0 with 1 tasks resource profile 0\n",
      "14:53:43.639 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 33.0 (TID 67) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:43.640 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 33.0 (TID 67)\n",
      "14:53:43.650 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.649055 ms\n",
      "14:53:43.670 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (350.4 KiB) non-empty blocks including 8 (350.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:43.671 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms\n",
      "14:53:43.706 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 34.179555 ms\n",
      "14:53:43.833 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:43.834 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:43.834 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:43.834 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:43.834 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:43.834 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:43.834 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:43.835 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:43.837 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:53:43.838 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"count_airlines_cancel\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE (STRING);\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 count_airlines_cancel;\n",
      "}\n",
      "\n",
      "       \n",
      "14:53:43.883 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20250613145343496034137616265825_0033_m_000000_67' to file:/home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/_temporary/0/task_20250613145343496034137616265825_0033_m_000000\n",
      "14:53:43.883 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20250613145343496034137616265825_0033_m_000000_67: Committed. Elapsed time: 0 ms.\n",
      "14:53:43.885 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 33.0 (TID 67). 7478 bytes result sent to driver\n",
      "14:53:43.886 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 33.0 (TID 67) in 248 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:43.886 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "14:53:43.887 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.277 s\n",
      "14:53:43.887 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:43.887 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 33: Stage finished\n",
      "14:53:43.888 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.285000 s\n",
      "14:53:43.889 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 8b1a0741-1957-4341-b3be-1dc492ae63d1.\n",
      "14:53:43.911 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 8b1a0741-1957-4341-b3be-1dc492ae63d1 committed. Elapsed time: 21 ms.\n",
      "14:53:43.911 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 8b1a0741-1957-4341-b3be-1dc492ae63d1.\n",
      "14:53:43.954 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:53:43.954 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:53:43.956 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:53:43.956 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "14:53:43.989 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34 stored as values in memory (estimated size 200.2 KiB, free 428.5 MiB)\n",
      "14:53:44.000 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 428.4 MiB)\n",
      "14:53:44.000 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_34_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 434.0 MiB)\n",
      "14:53:44.001 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:44.003 [broadcast-exchange-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:44.017 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_33_piece0 on 172.23.57.81:37441 in memory (size: 93.3 KiB, free: 434.1 MiB)\n",
      "14:53:44.022 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:44.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:53:44.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 34 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:53:44.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:44.023 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:44.025 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 34 (MapPartitionsRDD[77] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:53:44.027 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35 stored as values in memory (estimated size 14.5 KiB, free 428.8 MiB)\n",
      "14:53:44.028 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 428.7 MiB)\n",
      "14:53:44.028 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_35_piece0 in memory on 172.23.57.81:37441 (size: 6.3 KiB, free: 434.1 MiB)\n",
      "14:53:44.029 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:44.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[77] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:44.030 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 34.0 with 1 tasks resource profile 0\n",
      "14:53:44.031 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 34.0 (TID 68) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:44.032 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 34.0 (TID 68)\n",
      "14:53:44.037 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:53:44.044 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:53:44.049 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 34.0 (TID 68). 3613 bytes result sent to driver\n",
      "14:53:44.051 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 34.0 (TID 68) in 20 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:44.051 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "14:53:44.052 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 34 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.026 s\n",
      "14:53:44.052 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:44.052 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 34: Stage finished\n",
      "14:53:44.052 [broadcast-exchange-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.029527 s\n",
      "14:53:44.057 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36 stored as values in memory (estimated size 2.0 MiB, free 426.7 MiB)\n",
      "14:53:44.059 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 426.7 MiB)\n",
      "14:53:44.060 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_36_piece0 in memory on 172.23.57.81:37441 (size: 4.9 KiB, free: 434.1 MiB)\n",
      "14:53:44.061 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 36 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:53:44.070 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:53:44.070 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:53:44.102 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.204269 ms\n",
      "14:53:44.106 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37 stored as values in memory (estimated size 200.2 KiB, free 426.5 MiB)\n",
      "14:53:44.116 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 426.5 MiB)\n",
      "14:53:44.117 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_37_piece0 in memory on 172.23.57.81:37441 (size: 34.5 KiB, free: 434.1 MiB)\n",
      "14:53:44.118 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 37 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:44.118 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:53:44.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 81 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "14:53:44.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 23 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:53:44.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 35 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:44.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:53:44.123 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:44.124 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 35 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:44.130 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38 stored as values in memory (estimated size 18.9 KiB, free 426.5 MiB)\n",
      "14:53:44.132 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 426.5 MiB)\n",
      "14:53:44.132 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_38_piece0 in memory on 172.23.57.81:37441 (size: 8.4 KiB, free: 434.0 MiB)\n",
      "14:53:44.133 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 38 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:44.134 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:53:44.134 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 35.0 with 8 tasks resource profile 0\n",
      "14:53:44.135 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 35.0 (TID 69) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.136 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 35.0 (TID 70) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.136 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 35.0 (TID 71) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.137 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 35.0 (TID 72) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.138 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 35.0 (TID 73) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.138 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 35.0 (TID 74) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.139 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 35.0 (TID 75) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.139 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 35.0 (TID 76) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:53:44.140 [Executor task launch worker for task 1.0 in stage 35.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 35.0 (TID 70)\n",
      "14:53:44.140 [Executor task launch worker for task 0.0 in stage 35.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 35.0 (TID 69)\n",
      "14:53:44.140 [Executor task launch worker for task 6.0 in stage 35.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 35.0 (TID 75)\n",
      "14:53:44.140 [Executor task launch worker for task 7.0 in stage 35.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 35.0 (TID 76)\n",
      "14:53:44.140 [Executor task launch worker for task 5.0 in stage 35.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 35.0 (TID 74)\n",
      "14:53:44.141 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 35.0 (TID 71)\n",
      "14:53:44.143 [Executor task launch worker for task 3.0 in stage 35.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 35.0 (TID 72)\n",
      "14:53:44.143 [Executor task launch worker for task 4.0 in stage 35.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 35.0 (TID 73)\n",
      "14:53:44.156 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.259311 ms\n",
      "14:53:44.169 [Executor task launch worker for task 6.0 in stage 35.0 (TID 75)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.408665 ms\n",
      "14:53:44.170 [Executor task launch worker for task 6.0 in stage 35.0 (TID 75)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:53:44.170 [Executor task launch worker for task 3.0 in stage 35.0 (TID 72)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:53:44.170 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:53:44.170 [Executor task launch worker for task 5.0 in stage 35.0 (TID 74)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:53:44.170 [Executor task launch worker for task 4.0 in stage 35.0 (TID 73)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:53:44.170 [Executor task launch worker for task 7.0 in stage 35.0 (TID 76)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:53:44.170 [Executor task launch worker for task 0.0 in stage 35.0 (TID 69)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:53:44.170 [Executor task launch worker for task 1.0 in stage 35.0 (TID 70)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:53:44.354 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_35_piece0 on 172.23.57.81:37441 in memory (size: 6.3 KiB, free: 434.1 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:44.685 [Executor task launch worker for task 0.0 in stage 35.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 35.0 (TID 69). 2210 bytes result sent to driver\n",
      "14:53:44.687 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 35.0 (TID 69) in 552 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:53:44.693 [Executor task launch worker for task 3.0 in stage 35.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 35.0 (TID 72). 2210 bytes result sent to driver\n",
      "14:53:44.699 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 35.0 (TID 72) in 563 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:53:44.702 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 35.0 (TID 71). 2210 bytes result sent to driver\n",
      "14:53:44.703 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 35.0 (TID 71) in 567 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:53:44.763 [Executor task launch worker for task 1.0 in stage 35.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 35.0 (TID 70). 2210 bytes result sent to driver\n",
      "14:53:44.765 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 35.0 (TID 70) in 629 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:53:44.799 [Executor task launch worker for task 7.0 in stage 35.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 35.0 (TID 76). 2210 bytes result sent to driver\n",
      "14:53:44.802 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 35.0 (TID 76) in 662 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:53:44.862 [Executor task launch worker for task 5.0 in stage 35.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 35.0 (TID 74). 2339 bytes result sent to driver\n",
      "14:53:44.863 [Executor task launch worker for task 4.0 in stage 35.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 35.0 (TID 73). 2210 bytes result sent to driver\n",
      "14:53:44.864 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 35.0 (TID 74) in 726 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:53:44.865 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 35.0 (TID 73) in 726 ms on 172.23.57.81 (executor driver) (7/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:44.944 [Executor task launch worker for task 6.0 in stage 35.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 35.0 (TID 75). 2339 bytes result sent to driver\n",
      "14:53:44.945 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 35.0 (TID 75) in 806 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:53:44.945 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "14:53:44.945 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 35 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.819 s\n",
      "14:53:44.946 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:53:44.946 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:53:44.946 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:53:44.946 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:53:44.950 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:53:44.960 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:44.961 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:44.962 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:44.962 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:44.962 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:44.962 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:44.963 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:44.985 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:53:45.030 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.996441 ms\n",
      "14:53:45.039 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:53:45.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 24 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:53:45.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 37 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:53:45.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 36)\n",
      "14:53:45.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:53:45.041 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 37 (MapPartitionsRDD[84] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:53:45.065 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39 stored as values in memory (estimated size 251.0 KiB, free 426.2 MiB)\n",
      "14:53:45.068 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39_piece0 stored as bytes in memory (estimated size 92.4 KiB, free 426.2 MiB)\n",
      "14:53:45.068 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_39_piece0 in memory on 172.23.57.81:37441 (size: 92.4 KiB, free: 434.0 MiB)\n",
      "14:53:45.069 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
      "14:53:45.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[84] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:53:45.070 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 37.0 with 1 tasks resource profile 0\n",
      "14:53:45.071 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 37.0 (TID 77) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:53:45.072 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 37.0 (TID 77)\n",
      "14:53:45.090 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (457.0 KiB) non-empty blocks including 2 (457.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:53:45.090 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:53:45.122 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.581936 ms\n",
      "14:53:45.125 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:45.125 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:45.126 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:45.126 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:53:45.126 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:53:45.126 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:53:45.126 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:45.127 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:53:45.129 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:53:45.130 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_airports\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 Count_airports;\n",
      "}\n",
      "\n",
      "       \n",
      "14:53:45.320 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131453452579057947162649007_0037_m_000000_77' to file:/home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/_temporary/0/task_202506131453452579057947162649007_0037_m_000000\n",
      "14:53:45.321 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131453452579057947162649007_0037_m_000000_77: Committed. Elapsed time: 0 ms.\n",
      "14:53:45.322 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 37.0 (TID 77). 7199 bytes result sent to driver\n",
      "14:53:45.323 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 37.0 (TID 77) in 252 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:53:45.323 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "14:53:45.324 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 37 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.279 s\n",
      "14:53:45.324 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:53:45.324 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 37: Stage finished\n",
      "14:53:45.324 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.285006 s\n",
      "14:53:45.325 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 6c3909e9-c279-4f6f-8fb9-40c57cf43227.\n",
      "14:53:45.342 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 6c3909e9-c279-4f6f-8fb9-40c57cf43227 committed. Elapsed time: 17 ms.\n",
      "14:53:45.343 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 6c3909e9-c279-4f6f-8fb9-40c57cf43227.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create parquet files\n",
    "\n",
    "try:\n",
    "    avg_flight.write.parquet(config[\"Parquet_file\"][\"avg_flight\"], mode=\"overwrite\")\n",
    "\n",
    "    Rank_airline_in_airport.write.parquet(config[\"Parquet_file\"][\"airline_in_airport\"], mode=\"overwrite\")\n",
    "\n",
    "    flights_per_cancell.write.parquet(config[\"Parquet_file\"][\"flights_per_cancell\"], mode=\"overwrite\")\n",
    "\n",
    "    airport_notin_thelist.write.parquet(config[\"Parquet_file\"][\"airport_notin_thelist\"], mode=\"overwrite\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error write parquet files:\" + str(error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
