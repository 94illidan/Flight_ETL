{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a3abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, max, min, count, broadcast, desc, asc, when, lit,row_number, rank, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2650fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cfebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:34.098 [Thread-3] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ebb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b3eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:34.253 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "14:02:34.263 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/illidan/proyecto_desde0/ETL/spark-warehouse'.\n",
      "14:02:34.280 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11ca1834{/SQL,null,AVAILABLE,@Spark}\n",
      "14:02:34.283 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c10dc7e{/SQL/json,null,AVAILABLE,@Spark}\n",
      "14:02:34.286 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47b8caef{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "14:02:34.288 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b3a2190{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "14:02:34.304 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16749697{/static/sql,null,AVAILABLE,@Spark}\n",
      "14:02:35.428 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 101 ms to list leaf files for 1 paths.\n",
      "14:02:36.313 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:36.341 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:36.342 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:36.343 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:36.344 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:36.350 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:36.474 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "14:02:36.536 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "14:02:36.543 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.23.57.81:35957 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:02:36.551 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:36.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:36.591 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "14:02:36.686 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "14:02:36.712 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:37.219 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3053 bytes result sent to driver\n",
      "14:02:37.234 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 582 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:37.236 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "14:02:37.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.871 s\n",
      "14:02:37.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:37.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "14:02:37.250 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.935921 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:37.531 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.23.57.81:35957 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:02:38.182 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 6 ms to list leaf files for 1 paths.\n",
      "14:02:38.255 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:38.257 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:38.257 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:38.257 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:38.257 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:38.259 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:38.272 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "14:02:38.282 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "14:02:38.283 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.23.57.81:35957 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:02:38.285 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:38.287 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:38.287 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "14:02:38.290 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "14:02:38.292 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n",
      "14:02:38.319 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1662 bytes result sent to driver\n",
      "14:02:38.323 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 33 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:38.323 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "14:02:38.325 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.063 s\n",
      "14:02:38.326 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:38.326 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "14:02:38.327 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.071205 s\n",
      "14:02:38.356 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 5 ms to list leaf files for 1 paths.\n",
      "14:02:38.417 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:38.419 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:38.419 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:38.419 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:38.420 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:38.421 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:38.434 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 102.6 KiB, free 434.2 MiB)\n",
      "14:02:38.442 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.1 MiB)\n",
      "14:02:38.444 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.23.57.81:35957 (size: 36.9 KiB, free: 434.3 MiB)\n",
      "14:02:38.445 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:38.446 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:38.446 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "14:02:38.449 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9225 bytes) \n",
      "14:02:38.450 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "14:02:38.478 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1945 bytes result sent to driver\n",
      "14:02:38.481 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 33 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:38.481 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "14:02:38.482 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.060 s\n",
      "14:02:38.483 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:38.483 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "14:02:38.484 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.066370 s\n",
      "14:02:38.909 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:35957 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:02:38.915 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:35957 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "14:02:39.106 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:02:39.107 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:02:39.740 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 351.087644 ms\n",
      "14:02:39.774 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)\n",
      "14:02:39.785 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
      "14:02:39.786 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.23.57.81:35957 (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:02:39.788 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from count at NativeMethodAccessorImpl.java:0\n",
      "14:02:39.807 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:39.875 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "14:02:39.881 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:02:39.881 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:02:39.882 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:39.884 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:39.886 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:39.977 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)\n",
      "14:02:39.980 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "14:02:39.982 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.23.57.81:35957 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:02:39.983 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:39.987 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:02:39.987 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 8 tasks resource profile 0\n",
      "14:02:39.993 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:39.994 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 4) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:39.994 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 5) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:39.995 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 6) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:39.996 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 3.0 (TID 7) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:39.996 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 3.0 (TID 8) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:39.998 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 3.0 (TID 9) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:39.999 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 3.0 (TID 10) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:40.000 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)\n",
      "14:02:40.001 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 4)\n",
      "14:02:40.002 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 5)\n",
      "14:02:40.007 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 6)\n",
      "14:02:40.010 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 3.0 (TID 8)\n",
      "14:02:40.009 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 3.0 (TID 7)\n",
      "14:02:40.014 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 3.0 (TID 10)\n",
      "14:02:40.018 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 3.0 (TID 9)\n",
      "14:02:40.174 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.893432 ms\n",
      "14:02:40.198 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:02:40.198 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:02:40.200 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:02:40.203 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:02:40.204 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:02:40.209 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:02:40.211 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:02:40.216 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:40.512 [Executor task launch worker for task 5.0 in stage 3.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 3.0 (TID 8). 2222 bytes result sent to driver\n",
      "14:02:40.518 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2222 bytes result sent to driver\n",
      "14:02:40.518 [Executor task launch worker for task 2.0 in stage 3.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 5). 2222 bytes result sent to driver\n",
      "14:02:40.519 [Executor task launch worker for task 3.0 in stage 3.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 6). 2222 bytes result sent to driver\n",
      "14:02:40.523 [Executor task launch worker for task 7.0 in stage 3.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 3.0 (TID 10). 2222 bytes result sent to driver\n",
      "14:02:40.524 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 3.0 (TID 8) in 528 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:02:40.525 [Executor task launch worker for task 6.0 in stage 3.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 3.0 (TID 9). 2222 bytes result sent to driver\n",
      "14:02:40.530 [Executor task launch worker for task 4.0 in stage 3.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 3.0 (TID 7). 2222 bytes result sent to driver\n",
      "14:02:40.531 [Executor task launch worker for task 1.0 in stage 3.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 4). 2179 bytes result sent to driver\n",
      "14:02:40.537 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 6) in 542 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:02:40.538 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 549 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:02:40.539 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 3.0 (TID 7) in 544 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:02:40.542 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 5) in 548 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:02:40.543 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 3.0 (TID 10) in 544 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:02:40.544 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 4) in 551 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:02:40.544 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 3.0 (TID 9) in 547 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:02:40.544 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "14:02:40.552 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.660 s\n",
      "14:02:40.553 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:40.554 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:40.554 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:40.555 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:40.645 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.652294 ms\n",
      "14:02:40.702 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:02:40.705 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:40.705 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:02:40.705 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)\n",
      "14:02:40.705 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:40.707 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:40.728 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
      "14:02:40.749 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "14:02:40.754 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.23.57.81:35957 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:02:40.755 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.23.57.81:35957 in memory (size: 7.8 KiB, free: 434.4 MiB)\n",
      "14:02:40.756 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:40.757 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:40.757 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "14:02:40.769 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 11) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:40.771 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 11)\n",
      "14:02:40.856 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:40.859 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms\n",
      "14:02:40.887 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.558872 ms\n",
      "14:02:40.912 [Executor task launch worker for task 0.0 in stage 5.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 11). 4038 bytes result sent to driver\n",
      "14:02:40.915 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 11) in 148 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:40.915 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "14:02:40.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.193 s\n",
      "14:02:40.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:40.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished\n",
      "14:02:40.919 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.216828 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:40.966 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:02:40.966 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:02:41.000 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)\n",
      "14:02:41.026 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.23.57.81:35957 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "14:02:41.030 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
      "14:02:41.031 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.23.57.81:35957 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:02:41.033 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 6 from count at NativeMethodAccessorImpl.java:0\n",
      "14:02:41.035 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:41.046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "14:02:41.046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:41.046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:02:41.047 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:41.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:41.052 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:41.059 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "14:02:41.074 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "14:02:41.075 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.23.57.81:35957 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:02:41.076 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:41.077 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:41.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0\n",
      "14:02:41.080 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 12) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:41.081 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 12)\n",
      "14:02:41.092 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:02:41.116 [Executor task launch worker for task 0.0 in stage 6.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 12). 2222 bytes result sent to driver\n",
      "14:02:41.118 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 12) in 39 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:41.119 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "14:02:41.121 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.066 s\n",
      "14:02:41.124 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:41.124 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:41.125 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:41.125 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:41.206 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:02:41.209 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:41.209 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:02:41.210 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)\n",
      "14:02:41.210 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:41.212 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:41.217 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 12.5 KiB, free 433.9 MiB)\n",
      "14:02:41.219 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)\n",
      "14:02:41.220 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.23.57.81:35957 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:02:41.221 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:41.222 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:41.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks resource profile 0\n",
      "14:02:41.226 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 13) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:41.227 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 13)\n",
      "14:02:41.235 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:41.236 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "14:02:41.255 [Executor task launch worker for task 0.0 in stage 8.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 13). 4038 bytes result sent to driver\n",
      "14:02:41.258 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 13) in 33 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:41.258 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "14:02:41.260 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.23.57.81:35957 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:02:41.261 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.046 s\n",
      "14:02:41.261 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:41.261 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished\n",
      "14:02:41.263 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.055513 s\n",
      "14:02:41.327 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:02:41.328 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:02:41.368 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)\n",
      "14:02:41.386 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)\n",
      "14:02:41.387 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.23.57.81:35957 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:02:41.388 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 9 from count at NativeMethodAccessorImpl.java:0\n",
      "14:02:41.390 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:41.402 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 23 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "14:02:41.403 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:41.403 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:02:41.403 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:41.405 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:41.407 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:41.414 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 17.0 KiB, free 433.7 MiB)\n",
      "14:02:41.416 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.7 MiB)\n",
      "14:02:41.419 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.23.57.81:35957 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:02:41.421 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:41.423 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:41.423 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0\n",
      "14:02:41.425 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 14) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9684 bytes) \n",
      "14:02:41.427 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 14)\n",
      "14:02:41.437 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:02:41.459 [Executor task launch worker for task 0.0 in stage 9.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 14). 2136 bytes result sent to driver\n",
      "14:02:41.461 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 14) in 36 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:41.461 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "14:02:41.463 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.055 s\n",
      "14:02:41.464 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:41.464 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:41.464 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:41.464 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:41.523 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "14:02:41.526 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:41.526 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (count at NativeMethodAccessorImpl.java:0)\n",
      "14:02:41.526 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)\n",
      "14:02:41.527 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:41.528 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:41.533 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 12.5 KiB, free 433.7 MiB)\n",
      "14:02:41.539 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.7 MiB)\n",
      "14:02:41.542 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.23.57.81:35957 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:02:41.543 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:41.544 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:41.544 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0\n",
      "14:02:41.547 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 15) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:41.548 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 15)\n",
      "14:02:41.558 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:41.559 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:41.582 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 172.23.57.81:35957 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "14:02:41.584 [Executor task launch worker for task 0.0 in stage 11.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 15). 4038 bytes result sent to driver\n",
      "14:02:41.587 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 15) in 41 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:41.588 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "14:02:41.590 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.059 s\n",
      "14:02:41.590 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:41.590 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished\n",
      "14:02:41.592 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: count at NativeMethodAccessorImpl.java:0, took 0.068086 s\n",
      "14:02:41.592 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 172.23.57.81:35957 in memory (size: 5.9 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flight_row_count = df_flights.count()\n",
    "\n",
    "airline_row_count = df_airline.count()\n",
    "\n",
    "airports_row_count = df_airports.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1952a178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:41.618 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.23.57.81:35957 in memory (size: 34.5 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #avg per fly\n",
    "    avg_flight = df_flights.join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                        .groupBy(df_flights.AIRLINE, df_airline.AIRLINE) \\\n",
    "                        .agg(round(avg(df_flights.DISTANCE), 2).alias(\"avg_DISTANCE\")) \\\n",
    "                        .orderBy(desc(\"avg_DISTANCE\")) \\\n",
    "                        .select(df_airline.AIRLINE.alias(\"AIRLINE_Name\"), \"avg_DISTANCE\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable avg_flight:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237699b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #how many times a flight visit an airport\n",
    "    airline_in_airport = df_flights.join(broadcast(df_airports), df_flights.ORIGIN_AIRPORT == df_airports.IATA_CODE) \\\n",
    "                                .join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                                .repartition(df_flights.DESTINATION_AIRPORT) \\\n",
    "                                .groupBy(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE) \\\n",
    "                                    .agg(count(df_flights.AIRLINE).alias(\"Count_visit_per_airline\")) \\\n",
    "                                .orderBy(asc(df_flights.DESTINATION_AIRPORT)) \\\n",
    "                                .select(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE,\"Count_visit_per_airline\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airline_in_airport:\" + str(error))\n",
    "\n",
    "#a Rank of wich airline is the most visited in each airport\n",
    "try:\n",
    "    window_spec = Window.partitionBy(\"DESTINATION_AIRPORT\").orderBy(desc(\"Count_visit_per_airline\"))\n",
    "    \n",
    "    Rank_airline_in_airport = airline_in_airport.withColumn(\"Ranking\", rank().over(window_spec))\n",
    "\n",
    "\n",
    "except Exception as error:\n",
    "    logger.error(\"Error Rank_airline_in_airport:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad20a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #mention how many times an airline visit a destination and canceled this arrival\n",
    "    flights_per_cancell = df_flights.select(col(\"AIRLINE\"), col(\"CANCELLED\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .filter(col(\"CANCELLED\") == \"1\") \\\n",
    "                                .repartition(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .groupBy(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                    .agg(count(col(\"AIRLINE\")).alias(\"count_airlines_cancel\")) \\\n",
    "                                .orderBy(desc(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .limit(100)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable flights_per_cancell:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7929cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #i create this variable because i need to know how many airports have flights but are not in the list of airports\n",
    "    airport_notin_thelist = df_flights.join(broadcast(df_airports), df_flights.DESTINATION_AIRPORT == df_airports.IATA_CODE, \"left_anti\") \\\n",
    "                                        .select(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .repartition(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .groupBy(\"DESTINATION_AIRPORT\").agg(count(\"*\").alias(\"Count_airports\"))\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airport_notin_thelist:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7983760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:42.704 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "14:02:42.704 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "14:02:42.709 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:02:42.709 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "14:02:42.764 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 172.23.57.81:35957 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "14:02:42.772 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.23.57.81:35957 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
      "14:02:42.851 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 36.932516 ms\n",
      "14:02:42.862 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 200.3 KiB, free 434.0 MiB)\n",
      "14:02:42.872 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.9 MiB)\n",
      "14:02:42.875 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.23.57.81:35957 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "14:02:42.876 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:42.880 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:42.916 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:42.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:02:42.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:02:42.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:42.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:42.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:02:42.923 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 15.1 KiB, free 433.9 MiB)\n",
      "14:02:42.927 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.9 MiB)\n",
      "14:02:42.928 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.23.57.81:35957 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "14:02:42.929 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:42.930 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[30] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:42.931 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 1 tasks resource profile 0\n",
      "14:02:42.933 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "14:02:42.935 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)\n",
      "14:02:42.971 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.718737 ms\n",
      "14:02:42.973 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:02:43.001 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:02:43.117 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:02:43.369 [Executor task launch worker for task 0.0 in stage 12.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 2241 bytes result sent to driver\n",
      "14:02:43.371 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 438 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:43.372 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "14:02:43.373 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.451 s\n",
      "14:02:43.373 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:43.373 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 12: Stage finished\n",
      "14:02:43.374 [broadcast-exchange-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.457274 s\n",
      "14:02:43.403 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.551929 ms\n",
      "14:02:43.408 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 2.0 MiB, free 431.9 MiB)\n",
      "14:02:43.414 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 686.0 B, free 431.9 MiB)\n",
      "14:02:43.415 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.23.57.81:35957 (size: 686.0 B, free: 434.3 MiB)\n",
      "14:02:43.416 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:43.430 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "14:02:43.430 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#4)\n",
      "14:02:43.653 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 110.614466 ms\n",
      "14:02:43.658 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 200.3 KiB, free 431.7 MiB)\n",
      "14:02:43.666 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 431.7 MiB)\n",
      "14:02:43.667 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 172.23.57.81:35957 (size: 34.7 KiB, free: 434.3 MiB)\n",
      "14:02:43.669 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 15 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:43.671 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:43.771 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 34 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "14:02:43.772 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 10 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:02:43.772 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:43.772 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:43.774 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:43.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:43.783 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 53.5 KiB, free 431.6 MiB)\n",
      "14:02:43.800 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 431.6 MiB)\n",
      "14:02:43.802 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 172.23.57.81:35957 (size: 23.2 KiB, free: 434.3 MiB)\n",
      "14:02:43.803 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 172.23.57.81:35957 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "14:02:43.803 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:43.804 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:02:43.804 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 8 tasks resource profile 0\n",
      "14:02:43.806 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 17) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.807 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 18) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.807 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 13.0 (TID 19) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.808 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 13.0 (TID 20) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.808 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 13.0 (TID 21) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.808 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 13.0 (TID 22) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.809 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 13.0 (TID 23) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.810 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 13.0 (TID 24) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:43.811 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 17)\n",
      "14:02:43.811 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 13.0 (TID 19)\n",
      "14:02:43.811 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 13.0 (TID 20)\n",
      "14:02:43.811 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 18)\n",
      "14:02:43.811 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 13.0 (TID 22)\n",
      "14:02:43.814 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 13.0 (TID 24)\n",
      "14:02:43.811 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 13.0 (TID 21)\n",
      "14:02:43.832 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 13.0 (TID 23)\n",
      "14:02:43.893 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.23.57.81:35957 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "14:02:43.975 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 123.703361 ms\n",
      "14:02:44.010 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.543687 ms\n",
      "14:02:44.030 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.639443 ms\n",
      "14:02:44.079 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.514469 ms\n",
      "14:02:44.124 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.671955 ms\n",
      "14:02:44.144 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:02:44.144 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:02:44.144 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:02:44.146 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:02:44.147 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:02:44.150 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:02:44.153 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:02:44.154 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:02:44.162 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.168 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.169 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.197 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.199 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.199 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.199 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.203 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "14:02:44.211 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:02:44.219 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:02:44.219 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:02:44.223 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:02:44.224 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:02:44.229 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:02:44.238 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:45.893 [Executor task launch worker for task 1.0 in stage 13.0 (TID 18)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 13.0 (TID 18). 3926 bytes result sent to driver\n",
      "14:02:45.897 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 13.0 (TID 18) in 2091 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:02:45.900 [Executor task launch worker for task 0.0 in stage 13.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 17). 3926 bytes result sent to driver\n",
      "14:02:45.905 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 17) in 2098 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:02:45.908 [Executor task launch worker for task 2.0 in stage 13.0 (TID 19)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 13.0 (TID 19). 3969 bytes result sent to driver\n",
      "14:02:45.914 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 13.0 (TID 19) in 2107 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:02:45.916 [Executor task launch worker for task 7.0 in stage 13.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 13.0 (TID 24). 3926 bytes result sent to driver\n",
      "14:02:45.918 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 13.0 (TID 24) in 2108 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:02:45.921 [Executor task launch worker for task 5.0 in stage 13.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 13.0 (TID 22). 3926 bytes result sent to driver\n",
      "14:02:45.923 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 13.0 (TID 22) in 2115 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:02:45.925 [Executor task launch worker for task 6.0 in stage 13.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 13.0 (TID 23). 3926 bytes result sent to driver\n",
      "14:02:45.929 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 13.0 (TID 23) in 2120 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:02:45.933 [Executor task launch worker for task 3.0 in stage 13.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 13.0 (TID 20). 3926 bytes result sent to driver\n",
      "14:02:45.935 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 13.0 (TID 20) in 2127 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:02:45.937 [Executor task launch worker for task 4.0 in stage 13.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 13.0 (TID 21). 3926 bytes result sent to driver\n",
      "14:02:45.939 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 13.0 (TID 21) in 2131 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:02:45.939 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "14:02:45.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.163 s\n",
      "14:02:45.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:45.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:45.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:45.941 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:45.969 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:02:46.003 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:02:46.052 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.301831 ms\n",
      "14:02:46.136 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 36.724897 ms\n",
      "14:02:46.189 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:46.191 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:46.192 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:46.192 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)\n",
      "14:02:46.192 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:46.194 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:46.206 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 54.3 KiB, free 431.8 MiB)\n",
      "14:02:46.209 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 431.8 MiB)\n",
      "14:02:46.210 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 172.23.57.81:35957 (size: 24.1 KiB, free: 434.3 MiB)\n",
      "14:02:46.211 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:46.212 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[39] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:46.212 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks resource profile 0\n",
      "14:02:46.217 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 25) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:46.219 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 25)\n",
      "14:02:46.243 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:46.244 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms\n",
      "14:02:46.289 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 42.592231 ms\n",
      "14:02:46.305 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.915776 ms\n",
      "14:02:46.334 [Executor task launch worker for task 0.0 in stage 15.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 25). 6891 bytes result sent to driver\n",
      "14:02:46.337 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 25) in 119 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:46.337 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "14:02:46.338 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.136 s\n",
      "14:02:46.339 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:46.339 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 15: Stage finished\n",
      "14:02:46.340 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.149923 s\n",
      "14:02:46.355 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 40 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "14:02:46.355 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:46.355 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:46.356 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)\n",
      "14:02:46.357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:46.359 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:46.377 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 54.7 KiB, free 431.7 MiB)\n",
      "14:02:46.379 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 431.7 MiB)\n",
      "14:02:46.380 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 172.23.57.81:35957 (size: 24.3 KiB, free: 434.3 MiB)\n",
      "14:02:46.381 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:46.382 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[40] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:46.382 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks resource profile 0\n",
      "14:02:46.384 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 26) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "14:02:46.385 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 26)\n",
      "14:02:46.418 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.814202 ms\n",
      "14:02:46.428 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:46.429 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:46.459 [Executor task launch worker for task 0.0 in stage 17.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 26). 6433 bytes result sent to driver\n",
      "14:02:46.461 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 26) in 76 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:46.461 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "14:02:46.462 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 17 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.097 s\n",
      "14:02:46.463 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:46.463 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:46.463 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:46.463 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:46.473 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:02:46.535 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:46.556 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:46.557 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:46.559 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:46.559 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:46.559 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:46.561 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:46.635 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.697462 ms\n",
      "14:02:46.657 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:46.660 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:46.660 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:46.660 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)\n",
      "14:02:46.662 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:46.664 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:46.697 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 250.3 KiB, free 431.5 MiB)\n",
      "14:02:46.721 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 431.4 MiB)\n",
      "14:02:46.723 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 172.23.57.81:35957 (size: 93.3 KiB, free: 434.2 MiB)\n",
      "14:02:46.724 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 172.23.57.81:35957 in memory (size: 24.1 KiB, free: 434.2 MiB)\n",
      "14:02:46.726 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:46.727 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:46.728 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks resource profile 0\n",
      "14:02:46.731 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 172.23.57.81:35957 in memory (size: 24.3 KiB, free: 434.2 MiB)\n",
      "14:02:46.731 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 27) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:46.738 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 27)\n",
      "14:02:46.811 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 172.23.57.81:35957 in memory (size: 23.2 KiB, free: 434.2 MiB)\n",
      "14:02:46.848 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1323.0 B) non-empty blocks including 1 (1323.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:46.848 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:46.875 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.690713 ms\n",
      "14:02:46.894 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.546702 ms\n",
      "14:02:46.931 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.670303 ms\n",
      "14:02:46.950 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:46.950 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:46.951 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:46.951 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:46.951 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:46.952 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:46.961 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:46.966 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:47.010 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:02:47.028 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE_Name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_DISTANCE\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE_Name (STRING);\n",
      "  optional double avg_DISTANCE;\n",
      "}\n",
      "\n",
      "       \n",
      "14:02:47.096 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new compressor [.snappy]\n",
      "14:02:47.255 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131402462577111245517342877_0020_m_000000_27' to file:/home/illidan/proyecto_desde0/archivos_parquet/avg_flight/_temporary/0/task_202506131402462577111245517342877_0020_m_000000\n",
      "14:02:47.257 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131402462577111245517342877_0020_m_000000_27: Committed. Elapsed time: 2 ms.\n",
      "14:02:47.278 [Executor task launch worker for task 0.0 in stage 20.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 27). 8767 bytes result sent to driver\n",
      "14:02:47.282 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 27) in 550 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:47.282 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "14:02:47.284 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.617 s\n",
      "14:02:47.285 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:47.285 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished\n",
      "14:02:47.288 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 13 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.629767 s\n",
      "14:02:47.291 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 2cdf9281-4720-430a-b901-3e69479abc82.\n",
      "14:02:47.334 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 2cdf9281-4720-430a-b901-3e69479abc82 committed. Elapsed time: 41 ms.\n",
      "14:02:47.342 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 2cdf9281-4720-430a-b901-3e69479abc82.\n",
      "14:02:47.497 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:02:47.498 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:02:47.501 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:02:47.502 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "14:02:47.503 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:02:47.504 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#62)\n",
      "14:02:47.586 [broadcast-exchange-1] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.781869 ms\n",
      "14:02:47.591 [broadcast-exchange-2] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.194262 ms\n",
      "14:02:47.594 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 200.2 KiB, free 431.4 MiB)\n",
      "14:02:47.598 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 200.3 KiB, free 431.2 MiB)\n",
      "14:02:47.611 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.2 MiB)\n",
      "14:02:47.611 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 172.23.57.81:35957 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:02:47.612 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.1 MiB)\n",
      "14:02:47.613 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 20 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:47.613 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 172.23.57.81:35957 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:02:47.614 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 21 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:47.614 [broadcast-exchange-1] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:47.615 [broadcast-exchange-2] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:47.651 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:47.653 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 14 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:02:47.653 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:02:47.653 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:47.653 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:47.653 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:47.655 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:02:47.658 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 15.1 KiB, free 431.1 MiB)\n",
      "14:02:47.660 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 431.1 MiB)\n",
      "14:02:47.661 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 172.23.57.81:35957 (size: 6.5 KiB, free: 434.2 MiB)\n",
      "14:02:47.662 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:47.663 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:47.663 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks resource profile 0\n",
      "14:02:47.665 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 28) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "14:02:47.665 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:02:47.665 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:02:47.665 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:47.666 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:47.667 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 28)\n",
      "14:02:47.667 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:02:47.670 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 14.5 KiB, free 431.1 MiB)\n",
      "14:02:47.672 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 431.1 MiB)\n",
      "14:02:47.673 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 172.23.57.81:35957 (size: 6.3 KiB, free: 434.2 MiB)\n",
      "14:02:47.674 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:47.676 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:47.676 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 1 tasks resource profile 0\n",
      "14:02:47.678 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 29) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "14:02:47.680 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 29)\n",
      "14:02:47.692 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.966652 ms\n",
      "14:02:47.695 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "14:02:47.700 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.408878 ms\n",
      "14:02:47.701 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:02:47.703 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:02:47.709 [Executor task launch worker for task 0.0 in stage 21.0 (TID 28)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 28). 2198 bytes result sent to driver\n",
      "14:02:47.710 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:02:47.711 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 28) in 46 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:47.711 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "14:02:47.716 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.059 s\n",
      "14:02:47.717 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:47.717 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 21: Stage finished\n",
      "14:02:47.718 [broadcast-exchange-2] INFO  org.apache.spark.scheduler.DAGScheduler - Job 14 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.066408 s\n",
      "14:02:47.724 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 2.0 MiB, free 429.1 MiB)\n",
      "14:02:47.725 [Executor task launch worker for task 0.0 in stage 22.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 29). 3613 bytes result sent to driver\n",
      "14:02:47.727 [broadcast-exchange-2] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 686.0 B, free 429.1 MiB)\n",
      "14:02:47.728 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 172.23.57.81:35957 (size: 686.0 B, free: 434.2 MiB)\n",
      "14:02:47.728 [broadcast-exchange-2] INFO  org.apache.spark.SparkContext - Created broadcast 24 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:47.732 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 29) in 54 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:47.732 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "14:02:47.734 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.065 s\n",
      "14:02:47.734 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:47.734 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished\n",
      "14:02:47.735 [broadcast-exchange-1] INFO  org.apache.spark.scheduler.DAGScheduler - Job 15 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.081223 s\n",
      "14:02:47.746 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 2.0 MiB, free 427.1 MiB)\n",
      "14:02:47.747 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:02:47.747 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:02:47.752 [broadcast-exchange-1] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 427.1 MiB)\n",
      "14:02:47.753 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 172.23.57.81:35957 (size: 4.9 KiB, free: 434.2 MiB)\n",
      "14:02:47.754 [broadcast-exchange-1] INFO  org.apache.spark.SparkContext - Created broadcast 25 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:47.765 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "14:02:47.766 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#7),isnotnull(AIRLINE#4)\n",
      "14:02:47.829 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.597453 ms\n",
      "14:02:47.835 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 200.5 KiB, free 426.9 MiB)\n",
      "14:02:47.850 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 426.9 MiB)\n",
      "14:02:47.851 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 172.23.57.81:35957 (size: 34.8 KiB, free: 434.1 MiB)\n",
      "14:02:47.852 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 26 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:47.854 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:47.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 55 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "14:02:47.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 16 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:02:47.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:47.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:47.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:47.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:47.874 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 22.1 KiB, free 426.8 MiB)\n",
      "14:02:47.877 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 426.8 MiB)\n",
      "14:02:47.878 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 172.23.57.81:35957 (size: 9.0 KiB, free: 434.1 MiB)\n",
      "14:02:47.879 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:47.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[55] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:02:47.881 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 8 tasks resource profile 0\n",
      "14:02:47.883 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 30) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.883 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 31) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.884 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 32) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.885 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 33) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.885 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 23.0 (TID 34) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.885 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 23.0 (TID 35) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.886 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 23.0 (TID 36) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.886 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 23.0 (TID 37) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:47.887 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 32)\n",
      "14:02:47.887 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 23.0 (TID 35)\n",
      "14:02:47.887 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 23.0 (TID 36)\n",
      "14:02:47.887 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 33)\n",
      "14:02:47.888 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 30)\n",
      "14:02:47.888 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 31)\n",
      "14:02:47.892 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 23.0 (TID 34)\n",
      "14:02:47.894 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 23.0 (TID 37)\n",
      "14:02:47.918 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.861338 ms\n",
      "14:02:47.944 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.572478 ms\n",
      "14:02:47.946 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:02:47.946 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:02:47.946 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:02:47.946 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:02:47.946 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:02:47.947 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:02:47.950 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:02:47.946 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:02:47.961 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:47.962 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:47.970 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:47.971 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:47.976 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:47.976 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:47.980 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:47.990 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "14:02:48.194 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 172.23.57.81:35957 in memory (size: 93.3 KiB, free: 434.2 MiB)\n",
      "14:02:48.231 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 172.23.57.81:35957 in memory (size: 6.5 KiB, free: 434.2 MiB)\n",
      "14:02:48.297 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 172.23.57.81:35957 in memory (size: 6.3 KiB, free: 434.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:48.716 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 172.23.57.81:35957 in memory (size: 686.0 B, free: 434.2 MiB)\n",
      "14:02:48.743 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 172.23.57.81:35957 in memory (size: 34.6 KiB, free: 434.3 MiB)\n",
      "14:02:48.787 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 172.23.57.81:35957 in memory (size: 34.7 KiB, free: 434.3 MiB)\n",
      "14:02:49.408 [Executor task launch worker for task 6.0 in stage 23.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 23.0 (TID 36). 2451 bytes result sent to driver\n",
      "14:02:49.414 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 23.0 (TID 36) in 1529 ms on 172.23.57.81 (executor driver) (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:50.478 [Executor task launch worker for task 5.0 in stage 23.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 23.0 (TID 35). 2494 bytes result sent to driver\n",
      "14:02:50.489 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 23.0 (TID 35) in 2604 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:02:50.535 [Executor task launch worker for task 1.0 in stage 23.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 31). 2451 bytes result sent to driver\n",
      "14:02:50.537 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 31) in 2654 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:02:50.579 [Executor task launch worker for task 0.0 in stage 23.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 30). 2451 bytes result sent to driver\n",
      "14:02:50.581 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 30) in 2699 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:02:50.589 [Executor task launch worker for task 7.0 in stage 23.0 (TID 37)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 23.0 (TID 37). 2451 bytes result sent to driver\n",
      "14:02:50.592 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 23.0 (TID 37) in 2706 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:02:50.594 [Executor task launch worker for task 3.0 in stage 23.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 33). 2494 bytes result sent to driver\n",
      "14:02:50.595 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 33) in 2710 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:02:50.616 [Executor task launch worker for task 2.0 in stage 23.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 32). 2451 bytes result sent to driver\n",
      "14:02:50.618 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 32) in 2734 ms on 172.23.57.81 (executor driver) (7/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:50.690 [Executor task launch worker for task 4.0 in stage 23.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 23.0 (TID 34). 2451 bytes result sent to driver\n",
      "14:02:50.692 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 23.0 (TID 34) in 2807 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:02:50.692 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "14:02:50.693 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 23 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.824 s\n",
      "14:02:50.693 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:50.693 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:50.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:50.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:50.705 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(5), advisory target size: 67108864, actual target size 3493000, minimum partition size: 1048576\n",
      "14:02:50.726 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:02:50.788 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 48.592274 ms\n",
      "14:02:50.828 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.68541 ms\n",
      "14:02:50.844 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:50.847 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 17 (parquet at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "14:02:50.847 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:50.847 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)\n",
      "14:02:50.847 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:50.849 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[60] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:50.869 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 62.4 KiB, free 429.6 MiB)\n",
      "14:02:50.871 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.0 KiB, free 429.6 MiB)\n",
      "14:02:50.872 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 172.23.57.81:35957 (size: 26.0 KiB, free: 434.3 MiB)\n",
      "14:02:50.873 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:50.875 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 25 (MapPartitionsRDD[60] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "14:02:50.875 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 10 tasks resource profile 0\n",
      "14:02:50.877 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 38) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.878 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 25.0 (TID 39) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.878 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 25.0 (TID 40) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.879 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 25.0 (TID 41) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.879 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 25.0 (TID 42) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.880 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 25.0 (TID 43) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.880 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 25.0 (TID 44) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.880 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 25.0 (TID 45) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8999 bytes) \n",
      "14:02:50.881 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 25.0 (TID 39)\n",
      "14:02:50.881 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 25.0 (TID 40)\n",
      "14:02:50.881 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 25.0 (TID 44)\n",
      "14:02:50.881 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 25.0 (TID 43)\n",
      "14:02:50.882 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 25.0 (TID 41)\n",
      "14:02:50.882 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 25.0 (TID 45)\n",
      "14:02:50.881 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 38)\n",
      "14:02:50.886 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 25.0 (TID 42)\n",
      "14:02:50.899 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.901 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "14:02:50.903 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.906 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms\n",
      "14:02:50.903 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.909 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms\n",
      "14:02:50.912 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.912 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms\n",
      "14:02:50.915 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.915 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:50.915 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.915 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:50.919 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.919 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:50.923 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:50.923 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:50.954 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 51.933383 ms\n",
      "14:02:50.967 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.424002 ms\n",
      "14:02:50.978 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.707484 ms\n",
      "14:02:50.996 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.618127 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                        (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:51.741 [Executor task launch worker for task 7.0 in stage 25.0 (TID 45)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 25.0 (TID 45). 9534 bytes result sent to driver\n",
      "14:02:51.751 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 25.0 (TID 46) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8999 bytes) \n",
      "14:02:51.752 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 25.0 (TID 46)\n",
      "14:02:51.761 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 25.0 (TID 45) in 881 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "14:02:51.771 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:51.771 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:51.911 [Executor task launch worker for task 6.0 in stage 25.0 (TID 44)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 25.0 (TID 44). 13408 bytes result sent to driver\n",
      "14:02:51.916 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 25.0 (TID 47) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8999 bytes) \n",
      "14:02:51.917 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 25.0 (TID 47)\n",
      "14:02:51.917 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 25.0 (TID 44) in 1037 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "14:02:51.927 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:51.927 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:51.998 [Executor task launch worker for task 1.0 in stage 25.0 (TID 39)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 25.0 (TID 39). 13738 bytes result sent to driver\n",
      "14:02:52.001 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 25.0 (TID 39) in 1124 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "14:02:52.004 [Executor task launch worker for task 5.0 in stage 25.0 (TID 43)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 25.0 (TID 43). 11058 bytes result sent to driver\n",
      "14:02:52.007 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 25.0 (TID 43) in 1128 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "14:02:52.031 [Executor task launch worker for task 3.0 in stage 25.0 (TID 41)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 25.0 (TID 41). 14109 bytes result sent to driver\n",
      "14:02:52.037 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 25.0 (TID 41) in 1157 ms on 172.23.57.81 (executor driver) (5/10)\n",
      "14:02:52.040 [Executor task launch worker for task 2.0 in stage 25.0 (TID 40)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 25.0 (TID 40). 12911 bytes result sent to driver\n",
      "14:02:52.046 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 25.0 (TID 40) in 1168 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "14:02:52.081 [Executor task launch worker for task 0.0 in stage 25.0 (TID 38)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 38). 17238 bytes result sent to driver\n",
      "14:02:52.083 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 38) in 1206 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "14:02:52.105 [Executor task launch worker for task 4.0 in stage 25.0 (TID 42)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 25.0 (TID 42). 12913 bytes result sent to driver\n",
      "14:02:52.107 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 25.0 (TID 42) in 1228 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "14:02:52.136 [Executor task launch worker for task 8.0 in stage 25.0 (TID 46)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 25.0 (TID 46). 10030 bytes result sent to driver\n",
      "14:02:52.139 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 25.0 (TID 46) in 388 ms on 172.23.57.81 (executor driver) (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:52.253 [Executor task launch worker for task 9.0 in stage 25.0 (TID 47)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 25.0 (TID 47). 11676 bytes result sent to driver\n",
      "14:02:52.254 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 25.0 (TID 47) in 339 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "14:02:52.255 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "14:02:52.256 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.391 s\n",
      "14:02:52.257 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:52.257 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 25: Stage finished\n",
      "14:02:52.258 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 17 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.413148 s\n",
      "14:02:52.280 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 61 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "14:02:52.281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 18 (parquet at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "14:02:52.281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:52.281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)\n",
      "14:02:52.281 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:52.291 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 27 (MapPartitionsRDD[61] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:52.314 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 70.4 KiB, free 429.5 MiB)\n",
      "14:02:52.334 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 429.5 MiB)\n",
      "14:02:52.337 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 172.23.57.81:35957 (size: 27.5 KiB, free: 434.2 MiB)\n",
      "14:02:52.339 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:52.340 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[61] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "14:02:52.340 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 10 tasks resource profile 0\n",
      "14:02:52.344 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 48) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.345 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 49) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.346 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 50) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.346 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 27.0 (TID 51) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.347 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 27.0 (TID 52) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.347 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 27.0 (TID 53) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.348 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 27.0 (TID 54) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.349 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 27.0 (TID 55) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.352 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 27.0 (TID 51)\n",
      "14:02:52.352 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 27.0 (TID 55)\n",
      "14:02:52.352 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 50)\n",
      "14:02:52.352 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 27.0 (TID 52)\n",
      "14:02:52.351 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 49)\n",
      "14:02:52.351 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 48)\n",
      "14:02:52.354 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 27.0 (TID 53)\n",
      "14:02:52.355 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 27.0 (TID 54)\n",
      "14:02:52.392 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.31997 ms\n",
      "14:02:52.402 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.402 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.403 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:52.403 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:52.403 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.404 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:52.405 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.405 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:52.406 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.406 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.406 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms\n",
      "14:02:52.406 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms\n",
      "14:02:52.408 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.408 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:52.411 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.411 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:52.795 [Executor task launch worker for task 7.0 in stage 27.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 27.0 (TID 55). 7505 bytes result sent to driver\n",
      "14:02:52.797 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 27.0 (TID 56) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8988 bytes) \n",
      "14:02:52.798 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 27.0 (TID 56)\n",
      "14:02:52.798 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 27.0 (TID 55) in 450 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "14:02:52.816 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:52.817 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=====>                                                   (1 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:53.048 [Executor task launch worker for task 5.0 in stage 27.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 27.0 (TID 53). 7505 bytes result sent to driver\n",
      "14:02:53.050 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 27.0 (TID 53) in 703 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "14:02:53.054 [Executor task launch worker for task 2.0 in stage 27.0 (TID 50)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 50). 7505 bytes result sent to driver\n",
      "14:02:53.061 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 27.0 (TID 57) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8988 bytes) \n",
      "14:02:53.065 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 50) in 719 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "14:02:53.067 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 27.0 (TID 57)\n",
      "14:02:53.082 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:53.082 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:53.085 [Executor task launch worker for task 6.0 in stage 27.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 27.0 (TID 54). 7505 bytes result sent to driver\n",
      "14:02:53.087 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 27.0 (TID 54) in 739 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "14:02:53.148 [Executor task launch worker for task 0.0 in stage 27.0 (TID 48)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 48). 7505 bytes result sent to driver\n",
      "14:02:53.155 [Executor task launch worker for task 3.0 in stage 27.0 (TID 51)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 27.0 (TID 51). 7548 bytes result sent to driver\n",
      "14:02:53.161 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 27.0 (TID 51) in 815 ms on 172.23.57.81 (executor driver) (5/10)\n",
      "14:02:53.163 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 48) in 820 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "14:02:53.191 [Executor task launch worker for task 1.0 in stage 27.0 (TID 49)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 49). 7505 bytes result sent to driver\n",
      "14:02:53.195 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 49) in 850 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "14:02:53.250 [Executor task launch worker for task 8.0 in stage 27.0 (TID 56)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 27.0 (TID 56). 7505 bytes result sent to driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=======================================>                 (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:53.251 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 27.0 (TID 56) in 454 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "14:02:53.303 [Executor task launch worker for task 4.0 in stage 27.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 27.0 (TID 52). 7548 bytes result sent to driver\n",
      "14:02:53.305 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 27.0 (TID 52) in 958 ms on 172.23.57.81 (executor driver) (9/10)\n",
      "14:02:53.355 [Executor task launch worker for task 9.0 in stage 27.0 (TID 57)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 27.0 (TID 57). 7548 bytes result sent to driver\n",
      "14:02:53.356 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 27.0 (TID 57) in 297 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "14:02:53.357 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "14:02:53.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 27 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.053 s\n",
      "14:02:53.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:53.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:53.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:53.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:53.369 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:02:53.390 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:53.391 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:53.391 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:53.392 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:53.392 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:53.392 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:53.392 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:53.456 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.802127 ms\n",
      "14:02:53.507 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:53.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 19 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:53.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 30 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:53.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 29)\n",
      "14:02:53.510 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:53.510 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 30 (MapPartitionsRDD[65] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:53.548 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 256.9 KiB, free 429.2 MiB)\n",
      "14:02:53.561 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 95.3 KiB, free 429.1 MiB)\n",
      "14:02:53.562 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 172.23.57.81:35957 (size: 95.3 KiB, free: 434.1 MiB)\n",
      "14:02:53.563 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:53.564 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[65] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:53.564 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 30.0 with 1 tasks resource profile 0\n",
      "14:02:53.566 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 30.0 (TID 58) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:53.567 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 30.0 (TID 58)\n",
      "14:02:53.622 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 10 (63.2 KiB) non-empty blocks including 10 (63.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:53.623 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "14:02:53.637 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.650844 ms\n",
      "14:02:53.646 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.838084 ms\n",
      "14:02:53.662 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.779219 ms\n",
      "14:02:53.689 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.817143 ms\n",
      "14:02:53.767 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.906976 ms\n",
      "14:02:53.788 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.320077 ms\n",
      "14:02:53.798 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.015776 ms\n",
      "14:02:53.804 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:53.804 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:53.804 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:53.805 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:53.805 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:53.805 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:53.805 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:53.806 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:53.807 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:02:53.810 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_visit_per_airline\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Ranking\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  optional binary AIRLINE (STRING);\n",
      "  required int64 Count_visit_per_airline;\n",
      "  required int32 Ranking;\n",
      "}\n",
      "\n",
      "       \n",
      "14:02:53.970 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131402531254957245424965813_0030_m_000000_58' to file:/home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/_temporary/0/task_202506131402531254957245424965813_0030_m_000000\n",
      "14:02:53.970 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131402531254957245424965813_0030_m_000000_58: Committed. Elapsed time: 1 ms.\n",
      "14:02:53.976 [Executor task launch worker for task 0.0 in stage 30.0 (TID 58)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 30.0 (TID 58). 9831 bytes result sent to driver\n",
      "14:02:53.978 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 30.0 (TID 58) in 412 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:53.979 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "14:02:53.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 30 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.461 s\n",
      "14:02:53.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:53.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 30: Stage finished\n",
      "14:02:53.981 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 19 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.473266 s\n",
      "14:02:53.982 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 5485f31d-b129-4cac-a2f8-d24e8485697f.\n",
      "14:02:54.007 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 5485f31d-b129-4cac-a2f8-d24e8485697f committed. Elapsed time: 25 ms.\n",
      "14:02:54.008 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 5485f31d-b129-4cac-a2f8-d24e8485697f.\n",
      "14:02:54.064 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(CANCELLED),EqualTo(CANCELLED,1)\n",
      "14:02:54.064 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(CANCELLED#24),(CANCELLED#24 = 1)\n",
      "14:02:54.107 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.964494 ms\n",
      "14:02:54.111 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 200.5 KiB, free 428.9 MiB)\n",
      "14:02:54.129 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 428.9 MiB)\n",
      "14:02:54.129 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 172.23.57.81:35957 (size: 34.8 KiB, free: 434.1 MiB)\n",
      "14:02:54.130 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 31 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:54.132 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:54.139 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 69 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "14:02:54.140 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 20 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:02:54.140 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:54.140 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:54.140 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:54.141 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 31 (MapPartitionsRDD[69] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:54.147 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 18.8 KiB, free 428.9 MiB)\n",
      "14:02:54.150 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 428.9 MiB)\n",
      "14:02:54.150 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 172.23.57.81:35957 (size: 8.2 KiB, free: 434.1 MiB)\n",
      "14:02:54.151 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:54.152 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[69] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:02:54.152 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 31.0 with 8 tasks resource profile 0\n",
      "14:02:54.153 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 31.0 (TID 59) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.154 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 31.0 (TID 60) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.154 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 31.0 (TID 61) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.155 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 31.0 (TID 62) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.156 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 31.0 (TID 63) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.156 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 31.0 (TID 64) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.157 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 31.0 (TID 65) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.158 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 31.0 (TID 66) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:54.159 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 31.0 (TID 61)\n",
      "14:02:54.159 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 31.0 (TID 60)\n",
      "14:02:54.159 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 31.0 (TID 64)\n",
      "14:02:54.160 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 31.0 (TID 62)\n",
      "14:02:54.160 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 31.0 (TID 65)\n",
      "14:02:54.160 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 31.0 (TID 63)\n",
      "14:02:54.162 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 31.0 (TID 66)\n",
      "14:02:54.159 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 31.0 (TID 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:54.185 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_30_piece0 on 172.23.57.81:35957 in memory (size: 95.3 KiB, free: 434.2 MiB)\n",
      "14:02:54.196 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.885705 ms\n",
      "14:02:54.199 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_29_piece0 on 172.23.57.81:35957 in memory (size: 27.5 KiB, free: 434.2 MiB)\n",
      "14:02:54.202 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:02:54.203 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:02:54.204 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:02:54.206 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:02:54.211 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:02:54.213 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:02:54.217 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:02:54.220 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:02:54.226 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:02:54.226 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:02:54.226 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:02:54.226 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:02:54.230 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:02:54.237 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:02:54.243 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "14:02:54.246 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:54.732 [Executor task launch worker for task 3.0 in stage 31.0 (TID 62)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 31.0 (TID 62). 2339 bytes result sent to driver\n",
      "14:02:54.745 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 31.0 (TID 62) in 591 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:02:54.815 [Executor task launch worker for task 7.0 in stage 31.0 (TID 66)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 31.0 (TID 66). 2296 bytes result sent to driver\n",
      "14:02:54.820 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 31.0 (TID 66) in 661 ms on 172.23.57.81 (executor driver) (2/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:54.940 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_28_piece0 on 172.23.57.81:35957 in memory (size: 26.0 KiB, free: 434.2 MiB)\n",
      "14:02:54.966 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 172.23.57.81:35957 in memory (size: 686.0 B, free: 434.2 MiB)\n",
      "14:02:54.976 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 172.23.57.81:35957 in memory (size: 9.0 KiB, free: 434.3 MiB)\n",
      "14:02:54.986 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 172.23.57.81:35957 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
      "14:02:54.997 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 172.23.57.81:35957 in memory (size: 34.6 KiB, free: 434.3 MiB)\n",
      "14:02:55.007 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on 172.23.57.81:35957 in memory (size: 4.9 KiB, free: 434.3 MiB)\n",
      "14:02:55.021 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 172.23.57.81:35957 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "14:02:55.050 [Executor task launch worker for task 5.0 in stage 31.0 (TID 64)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 31.0 (TID 64). 2339 bytes result sent to driver\n",
      "14:02:55.052 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 31.0 (TID 64) in 896 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:02:55.056 [Executor task launch worker for task 1.0 in stage 31.0 (TID 60)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 31.0 (TID 60). 2296 bytes result sent to driver\n",
      "14:02:55.057 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 31.0 (TID 60) in 903 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:02:55.063 [Executor task launch worker for task 6.0 in stage 31.0 (TID 65)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 31.0 (TID 65). 2339 bytes result sent to driver\n",
      "14:02:55.067 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 31.0 (TID 65) in 910 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:02:55.073 [Executor task launch worker for task 2.0 in stage 31.0 (TID 61)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 31.0 (TID 61). 2296 bytes result sent to driver\n",
      "14:02:55.075 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 31.0 (TID 61) in 921 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:02:55.077 [Executor task launch worker for task 0.0 in stage 31.0 (TID 59)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 31.0 (TID 59). 2339 bytes result sent to driver\n",
      "14:02:55.077 [Executor task launch worker for task 4.0 in stage 31.0 (TID 63)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 31.0 (TID 63). 2296 bytes result sent to driver\n",
      "14:02:55.078 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 31.0 (TID 59) in 925 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:02:55.079 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 31.0 (TID 63) in 922 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:02:55.079 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "14:02:55.080 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 31 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.936 s\n",
      "14:02:55.080 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:55.080 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:55.080 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:55.080 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:55.093 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:02:55.103 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:55.105 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:55.105 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:55.105 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:55.106 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:55.106 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:55.106 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:55.159 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.5555 ms\n",
      "14:02:55.160 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "14:02:55.215 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.206403 ms\n",
      "14:02:55.239 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:55.241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 21 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:55.241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:55.241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 32)\n",
      "14:02:55.241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:55.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 33 (MapPartitionsRDD[73] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:55.273 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 254.2 KiB, free 433.9 MiB)\n",
      "14:02:55.281 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 433.8 MiB)\n",
      "14:02:55.281 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on 172.23.57.81:35957 (size: 93.3 KiB, free: 434.3 MiB)\n",
      "14:02:55.282 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 33 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:55.283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[73] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:55.283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 33.0 with 1 tasks resource profile 0\n",
      "14:02:55.284 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 33.0 (TID 67) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:55.285 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 33.0 (TID 67)\n",
      "14:02:55.300 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.240501 ms\n",
      "14:02:55.320 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (350.4 KiB) non-empty blocks including 8 (350.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:55.321 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "14:02:55.355 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.034602 ms\n",
      "14:02:55.486 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:55.486 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:55.486 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:55.486 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:55.486 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:55.487 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:55.487 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:55.488 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:55.489 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:02:55.491 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"count_airlines_cancel\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE (STRING);\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 count_airlines_cancel;\n",
      "}\n",
      "\n",
      "       \n",
      "14:02:55.537 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131402557003735477615642022_0033_m_000000_67' to file:/home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/_temporary/0/task_202506131402557003735477615642022_0033_m_000000\n",
      "14:02:55.537 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131402557003735477615642022_0033_m_000000_67: Committed. Elapsed time: 1 ms.\n",
      "14:02:55.539 [Executor task launch worker for task 0.0 in stage 33.0 (TID 67)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 33.0 (TID 67). 7521 bytes result sent to driver\n",
      "14:02:55.540 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 33.0 (TID 67) in 256 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:55.541 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "14:02:55.541 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 33 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.295 s\n",
      "14:02:55.541 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:55.542 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 33: Stage finished\n",
      "14:02:55.542 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.302790 s\n",
      "14:02:55.543 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 3fb83b86-f070-4350-b76c-11cd5856badb.\n",
      "14:02:55.565 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 3fb83b86-f070-4350-b76c-11cd5856badb committed. Elapsed time: 21 ms.\n",
      "14:02:55.565 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 3fb83b86-f070-4350-b76c-11cd5856badb.\n",
      "14:02:55.606 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:02:55.607 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:02:55.608 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "14:02:55.608 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#66)\n",
      "14:02:55.642 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34 stored as values in memory (estimated size 200.2 KiB, free 433.6 MiB)\n",
      "14:02:55.653 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.6 MiB)\n",
      "14:02:55.654 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_34_piece0 in memory on 172.23.57.81:35957 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "14:02:55.655 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:55.656 [broadcast-exchange-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:55.675 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:55.675 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "14:02:55.676 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 34 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "14:02:55.676 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:55.676 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:55.676 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 34 (MapPartitionsRDD[77] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "14:02:55.678 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35 stored as values in memory (estimated size 14.5 KiB, free 433.6 MiB)\n",
      "14:02:55.680 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 433.6 MiB)\n",
      "14:02:55.681 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_35_piece0 in memory on 172.23.57.81:35957 (size: 6.3 KiB, free: 434.2 MiB)\n",
      "14:02:55.682 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:55.682 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[77] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:55.682 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 34.0 with 1 tasks resource profile 0\n",
      "14:02:55.683 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 34.0 (TID 68) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "14:02:55.684 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 34.0 (TID 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:55.688 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "14:02:55.694 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "14:02:55.718 [Executor task launch worker for task 0.0 in stage 34.0 (TID 68)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 34.0 (TID 68). 3656 bytes result sent to driver\n",
      "14:02:55.719 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 34.0 (TID 68) in 36 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:55.719 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "14:02:55.720 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 34 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.043 s\n",
      "14:02:55.720 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:55.721 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 34: Stage finished\n",
      "14:02:55.721 [broadcast-exchange-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.046107 s\n",
      "14:02:55.726 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36 stored as values in memory (estimated size 2.0 MiB, free 431.5 MiB)\n",
      "14:02:55.731 [broadcast-exchange-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 431.5 MiB)\n",
      "14:02:55.732 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_36_piece0 in memory on 172.23.57.81:35957 (size: 4.9 KiB, free: 434.2 MiB)\n",
      "14:02:55.733 [broadcast-exchange-3] INFO  org.apache.spark.SparkContext - Created broadcast 36 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "14:02:55.739 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:02:55.740 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:02:55.775 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.266501 ms\n",
      "14:02:55.781 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37 stored as values in memory (estimated size 200.2 KiB, free 431.3 MiB)\n",
      "14:02:55.797 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 431.3 MiB)\n",
      "14:02:55.798 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_37_piece0 in memory on 172.23.57.81:35957 (size: 34.5 KiB, free: 434.2 MiB)\n",
      "14:02:55.801 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 37 from parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:55.808 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:02:55.816 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 81 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "14:02:55.816 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 23 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "14:02:55.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 35 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:55.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:02:55.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:55.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 35 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:55.824 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38 stored as values in memory (estimated size 18.9 KiB, free 431.3 MiB)\n",
      "14:02:55.826 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 431.3 MiB)\n",
      "14:02:55.827 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_38_piece0 in memory on 172.23.57.81:35957 (size: 8.4 KiB, free: 434.2 MiB)\n",
      "14:02:55.827 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 38 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:55.828 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[81] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "14:02:55.828 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 35.0 with 8 tasks resource profile 0\n",
      "14:02:55.830 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 35.0 (TID 69) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.830 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 35.0 (TID 70) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.830 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 35.0 (TID 71) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.831 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 35.0 (TID 72) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.831 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 35.0 (TID 73) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.831 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 35.0 (TID 74) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.832 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 35.0 (TID 75) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.832 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 35.0 (TID 76) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "14:02:55.833 [Executor task launch worker for task 0.0 in stage 35.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 35.0 (TID 69)\n",
      "14:02:55.833 [Executor task launch worker for task 1.0 in stage 35.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 35.0 (TID 70)\n",
      "14:02:55.833 [Executor task launch worker for task 3.0 in stage 35.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 35.0 (TID 72)\n",
      "14:02:55.833 [Executor task launch worker for task 7.0 in stage 35.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 35.0 (TID 76)\n",
      "14:02:55.834 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 35.0 (TID 71)\n",
      "14:02:55.834 [Executor task launch worker for task 5.0 in stage 35.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 35.0 (TID 74)\n",
      "14:02:55.838 [Executor task launch worker for task 4.0 in stage 35.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 35.0 (TID 73)\n",
      "14:02:55.839 [Executor task launch worker for task 6.0 in stage 35.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 35.0 (TID 75)\n",
      "14:02:55.856 [Executor task launch worker for task 7.0 in stage 35.0 (TID 76)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.733301 ms\n",
      "14:02:55.870 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.167301 ms\n",
      "14:02:55.871 [Executor task launch worker for task 6.0 in stage 35.0 (TID 75)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "14:02:55.871 [Executor task launch worker for task 4.0 in stage 35.0 (TID 73)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "14:02:55.871 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "14:02:55.872 [Executor task launch worker for task 7.0 in stage 35.0 (TID 76)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "14:02:55.872 [Executor task launch worker for task 1.0 in stage 35.0 (TID 70)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "14:02:55.872 [Executor task launch worker for task 0.0 in stage 35.0 (TID 69)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "14:02:55.872 [Executor task launch worker for task 3.0 in stage 35.0 (TID 72)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "14:02:55.877 [Executor task launch worker for task 5.0 in stage 35.0 (TID 74)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "14:02:56.091 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_35_piece0 on 172.23.57.81:35957 in memory (size: 6.3 KiB, free: 434.2 MiB)\n",
      "14:02:56.321 [Executor task launch worker for task 7.0 in stage 35.0 (TID 76)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 35.0 (TID 76). 2210 bytes result sent to driver\n",
      "14:02:56.327 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 35.0 (TID 76) in 495 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "14:02:56.362 [Executor task launch worker for task 3.0 in stage 35.0 (TID 72)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 35.0 (TID 72). 2210 bytes result sent to driver\n",
      "14:02:56.363 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 35.0 (TID 72) in 532 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "14:02:56.380 [Executor task launch worker for task 4.0 in stage 35.0 (TID 73)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 35.0 (TID 73). 2210 bytes result sent to driver\n",
      "14:02:56.381 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 35.0 (TID 73) in 550 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "14:02:56.393 [Executor task launch worker for task 2.0 in stage 35.0 (TID 71)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 35.0 (TID 71). 2210 bytes result sent to driver\n",
      "14:02:56.394 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 35.0 (TID 71) in 564 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "14:02:56.419 [Executor task launch worker for task 1.0 in stage 35.0 (TID 70)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 35.0 (TID 70). 2210 bytes result sent to driver\n",
      "14:02:56.421 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 35.0 (TID 70) in 591 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "14:02:56.424 [Executor task launch worker for task 0.0 in stage 35.0 (TID 69)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 35.0 (TID 69). 2210 bytes result sent to driver\n",
      "14:02:56.424 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 35.0 (TID 69) in 595 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "14:02:56.448 [Executor task launch worker for task 5.0 in stage 35.0 (TID 74)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 35.0 (TID 74). 2339 bytes result sent to driver\n",
      "14:02:56.449 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 35.0 (TID 74) in 618 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "14:02:56.473 [Executor task launch worker for task 6.0 in stage 35.0 (TID 75)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 35.0 (TID 75). 2339 bytes result sent to driver\n",
      "14:02:56.474 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 35.0 (TID 75) in 642 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "14:02:56.475 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "14:02:56.475 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 35 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.656 s\n",
      "14:02:56.475 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "14:02:56.475 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "14:02:56.475 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "14:02:56.475 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "14:02:56.480 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "14:02:56.490 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:56.490 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:56.491 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:56.491 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:56.491 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:56.491 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:56.491 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:56.513 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:02:56.548 [Thread-3] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.979602 ms\n",
      "14:02:56.559 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:02:56.561 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 24 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:02:56.561 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 37 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:02:56.561 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 36)\n",
      "14:02:56.561 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:02:56.562 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 37 (MapPartitionsRDD[84] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:02:56.584 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39 stored as values in memory (estimated size 251.0 KiB, free 431.1 MiB)\n",
      "14:02:56.587 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39_piece0 stored as bytes in memory (estimated size 92.4 KiB, free 431.0 MiB)\n",
      "14:02:56.587 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_39_piece0 in memory on 172.23.57.81:35957 (size: 92.4 KiB, free: 434.1 MiB)\n",
      "14:02:56.588 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
      "14:02:56.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[84] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:02:56.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 37.0 with 1 tasks resource profile 0\n",
      "14:02:56.590 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 37.0 (TID 77) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "14:02:56.591 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 37.0 (TID 77)\n",
      "14:02:56.605 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (457.0 KiB) non-empty blocks including 2 (457.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "14:02:56.605 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "14:02:56.633 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.193202 ms\n",
      "14:02:56.635 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:56.635 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:56.635 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:56.635 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "14:02:56.635 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "14:02:56.636 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "14:02:56.636 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:56.636 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "14:02:56.638 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "14:02:56.639 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_airports\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 Count_airports;\n",
      "}\n",
      "\n",
      "       \n",
      "14:02:56.908 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131402567463055890038158189_0037_m_000000_77' to file:/home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/_temporary/0/task_202506131402567463055890038158189_0037_m_000000\n",
      "14:02:56.908 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131402567463055890038158189_0037_m_000000_77: Committed. Elapsed time: 0 ms.\n",
      "14:02:56.909 [Executor task launch worker for task 0.0 in stage 37.0 (TID 77)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 37.0 (TID 77). 7199 bytes result sent to driver\n",
      "14:02:56.910 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 37.0 (TID 77) in 320 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:02:56.911 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "14:02:56.911 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 37 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.347 s\n",
      "14:02:56.912 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:02:56.912 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 37: Stage finished\n",
      "14:02:56.912 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.352386 s\n",
      "14:02:56.913 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job dbf17c81-1a33-4d60-a70f-39677aa15df5.\n",
      "14:02:56.929 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job dbf17c81-1a33-4d60-a70f-39677aa15df5 committed. Elapsed time: 15 ms.\n",
      "14:02:56.929 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job dbf17c81-1a33-4d60-a70f-39677aa15df5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Create parquet files\n",
    "\n",
    "try:\n",
    "    avg_flight.write.parquet(config[\"Parquet_file\"][\"avg_flight\"], mode=\"overwrite\")\n",
    "\n",
    "    Rank_airline_in_airport.write.parquet(config[\"Parquet_file\"][\"airline_in_airport\"], mode=\"overwrite\")\n",
    "\n",
    "    flights_per_cancell.write.parquet(config[\"Parquet_file\"][\"flights_per_cancell\"], mode=\"overwrite\")\n",
    "\n",
    "    airport_notin_thelist.write.parquet(config[\"Parquet_file\"][\"airport_notin_thelist\"], mode=\"overwrite\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error write parquet files:\" + str(error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
