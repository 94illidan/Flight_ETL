{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a3abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, max, min, count, broadcast, desc, asc, when, rank, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2650fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31cfebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:49.667 [Thread-3] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ebb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4b3eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:49.730 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "15:20:49.773 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:49.774 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 25 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:49.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 38 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:49.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:49.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:49.775 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 38 (MapPartitionsRDD[86] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:49.791 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_40 stored as values in memory (estimated size 102.6 KiB, free 426.1 MiB)\n",
      "15:20:49.896 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_40_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 426.0 MiB)\n",
      "15:20:49.897 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_40_piece0 in memory on 172.23.57.81:37441 (size: 36.9 KiB, free: 433.9 MiB)\n",
      "15:20:49.898 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:49.898 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[86] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:49.898 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 38.0 with 1 tasks resource profile 0\n",
      "15:20:49.899 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 38.0 (TID 78) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "15:20:49.901 [Executor task launch worker for task 0.0 in stage 38.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 38.0 (TID 78)\n",
      "15:20:49.917 [Executor task launch worker for task 0.0 in stage 38.0 (TID 78)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 38.0 (TID 78). 3010 bytes result sent to driver\n",
      "15:20:49.918 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 38.0 (TID 78) in 19 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:49.918 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "15:20:49.919 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 38 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.142 s\n",
      "15:20:49.919 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:49.919 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 38: Stage finished\n",
      "15:20:49.919 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 25 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.146364 s\n",
      "15:20:49.939 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "15:20:49.974 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:49.975 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 26 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:49.975 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 39 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:49.975 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:49.975 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:49.976 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 39 (MapPartitionsRDD[88] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:49.986 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_41 stored as values in memory (estimated size 102.6 KiB, free 425.9 MiB)\n",
      "15:20:50.004 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_41_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 425.9 MiB)\n",
      "15:20:50.005 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_41_piece0 in memory on 172.23.57.81:37441 (size: 36.9 KiB, free: 433.9 MiB)\n",
      "15:20:50.005 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 41 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:50.005 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[88] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:50.005 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 39.0 with 1 tasks resource profile 0\n",
      "15:20:50.007 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 39.0 (TID 79) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "15:20:50.007 [Executor task launch worker for task 0.0 in stage 39.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 39.0 (TID 79)\n",
      "15:20:50.031 [Executor task launch worker for task 0.0 in stage 39.0 (TID 79)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 39.0 (TID 79). 1705 bytes result sent to driver\n",
      "15:20:50.033 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 39.0 (TID 79) in 25 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:50.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 39 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.056 s\n",
      "15:20:50.034 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "15:20:50.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:50.034 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 39: Stage finished\n",
      "15:20:50.035 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 26 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.059947 s\n",
      "15:20:50.053 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 1 ms to list leaf files for 1 paths.\n",
      "15:20:50.091 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:50.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 27 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:50.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 40 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:50.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:50.092 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:50.093 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 40 (MapPartitionsRDD[90] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:50.100 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_42 stored as values in memory (estimated size 102.6 KiB, free 425.8 MiB)\n",
      "15:20:50.131 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_42_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 425.7 MiB)\n",
      "15:20:50.131 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_42_piece0 in memory on 172.23.57.81:37441 (size: 36.9 KiB, free: 433.9 MiB)\n",
      "15:20:50.132 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 42 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:50.133 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[90] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:50.133 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 40.0 with 1 tasks resource profile 0\n",
      "15:20:50.134 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 40.0 (TID 80) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9225 bytes) \n",
      "15:20:50.135 [Executor task launch worker for task 0.0 in stage 40.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 40.0 (TID 80)\n",
      "15:20:50.161 [Executor task launch worker for task 0.0 in stage 40.0 (TID 80)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 40.0 (TID 80). 1945 bytes result sent to driver\n",
      "15:20:50.163 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 40.0 (TID 80) in 28 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:50.163 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "15:20:50.163 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 40 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.070 s\n",
      "15:20:50.164 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:50.164 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 40: Stage finished\n",
      "15:20:50.164 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 27 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.072688 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1952a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #avg per fly\n",
    "    avg_flight = df_flights.join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                        .groupBy(df_flights.AIRLINE, df_airline.AIRLINE) \\\n",
    "                        .agg(round(avg(df_flights.DISTANCE), 2).alias(\"avg_DISTANCE\")) \\\n",
    "                        .orderBy(desc(\"avg_DISTANCE\")) \\\n",
    "                        .select(df_airline.AIRLINE.alias(\"AIRLINE_Name\"), \"avg_DISTANCE\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable avg_flight:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "237699b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:50.333 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_40_piece0 on 172.23.57.81:37441 in memory (size: 36.9 KiB, free: 433.9 MiB)\n",
      "15:20:50.338 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_42_piece0 on 172.23.57.81:37441 in memory (size: 36.9 KiB, free: 433.9 MiB)\n",
      "15:20:50.343 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_41_piece0 on 172.23.57.81:37441 in memory (size: 36.9 KiB, free: 434.0 MiB)\n",
      "15:20:50.350 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_39_piece0 on 172.23.57.81:37441 in memory (size: 92.4 KiB, free: 434.1 MiB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    #how many times a flight visit an airport\n",
    "    airline_in_airport = df_flights.join(broadcast(df_airports), df_flights.ORIGIN_AIRPORT == df_airports.IATA_CODE) \\\n",
    "                                .join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                                .repartition(df_flights.DESTINATION_AIRPORT) \\\n",
    "                                .groupBy(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE) \\\n",
    "                                    .agg(count(df_flights.AIRLINE).alias(\"Count_visit_per_airline\")) \\\n",
    "                                .orderBy(asc(df_flights.DESTINATION_AIRPORT)) \\\n",
    "                                .select(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE,\"Count_visit_per_airline\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airline_in_airport:\" + str(error))\n",
    "\n",
    "#a Rank of wich airline is the most visited in each airport\n",
    "try:\n",
    "    window_spec = Window.partitionBy(\"DESTINATION_AIRPORT\").orderBy(desc(\"Count_visit_per_airline\"))\n",
    "    \n",
    "    Rank_airline_in_airport = airline_in_airport.withColumn(\"ranking\", rank().over(window_spec))\n",
    "\n",
    "\n",
    "except Exception as error:\n",
    "    logger.error(\"Error Rank_airline_in_airport:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ad20a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #mention how many times an airline visit a destination and canceled this arrival\n",
    "    flights_per_cancell = df_flights.select(col(\"AIRLINE\"), col(\"CANCELLED\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .filter(col(\"CANCELLED\") == \"1\") \\\n",
    "                                .repartition(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .groupBy(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                    .agg(count(col(\"AIRLINE\")).alias(\"count_airlines_cancel\")) \\\n",
    "                                .orderBy(desc(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .limit(100)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable flights_per_cancell:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7929cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #i create this variable because i need to know how many airports have flights but are not in the list of airports\n",
    "    airport_notin_thelist = df_flights.join(broadcast(df_airports), df_flights.DESTINATION_AIRPORT == df_airports.IATA_CODE, \"left_anti\") \\\n",
    "                                        .select(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .repartition(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .groupBy(\"DESTINATION_AIRPORT\").agg(count(\"*\").alias(\"Count_airports\"))\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airport_notin_thelist:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7983760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:50.639 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "15:20:50.639 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#807)\n",
      "15:20:50.640 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "15:20:50.640 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#865)\n",
      "15:20:50.673 [broadcast-exchange-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_43 stored as values in memory (estimated size 200.3 KiB, free 426.3 MiB)\n",
      "15:20:50.703 [broadcast-exchange-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 426.3 MiB)\n",
      "15:20:50.704 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_43_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 434.0 MiB)\n",
      "15:20:50.704 [broadcast-exchange-4] INFO  org.apache.spark.SparkContext - Created broadcast 43 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:50.705 [broadcast-exchange-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:50.714 [broadcast-exchange-4] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:50.715 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 28 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "15:20:50.715 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 41 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "15:20:50.715 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:50.715 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:50.716 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 41 (MapPartitionsRDD[94] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "15:20:50.717 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_44 stored as values in memory (estimated size 15.1 KiB, free 426.2 MiB)\n",
      "15:20:50.727 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_44_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 426.2 MiB)\n",
      "15:20:50.728 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_44_piece0 in memory on 172.23.57.81:37441 (size: 6.5 KiB, free: 434.0 MiB)\n",
      "15:20:50.728 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 44 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:50.729 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[94] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:50.729 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 41.0 with 1 tasks resource profile 0\n",
      "15:20:50.730 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 41.0 (TID 81) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "15:20:50.730 [Executor task launch worker for task 0.0 in stage 41.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 41.0 (TID 81)\n",
      "15:20:50.732 [Executor task launch worker for task 0.0 in stage 41.0 (TID 81)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "15:20:50.736 [Executor task launch worker for task 0.0 in stage 41.0 (TID 81)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "15:20:50.751 [Executor task launch worker for task 0.0 in stage 41.0 (TID 81)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 41.0 (TID 81). 2241 bytes result sent to driver\n",
      "15:20:50.751 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 41.0 (TID 81) in 22 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:50.751 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "15:20:50.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 41 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.036 s\n",
      "15:20:50.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:50.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 41: Stage finished\n",
      "15:20:50.752 [broadcast-exchange-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 28 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.038165 s\n",
      "15:20:50.756 [broadcast-exchange-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_45 stored as values in memory (estimated size 2.0 MiB, free 424.2 MiB)\n",
      "15:20:50.765 [broadcast-exchange-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_45_piece0 stored as bytes in memory (estimated size 686.0 B, free 424.2 MiB)\n",
      "15:20:50.766 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_45_piece0 in memory on 172.23.57.81:37441 (size: 686.0 B, free: 434.0 MiB)\n",
      "15:20:50.767 [broadcast-exchange-4] INFO  org.apache.spark.SparkContext - Created broadcast 45 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:50.773 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(AIRLINE)\n",
      "15:20:50.773 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(AIRLINE#807)\n",
      "15:20:50.797 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_46 stored as values in memory (estimated size 200.3 KiB, free 424.0 MiB)\n",
      "15:20:50.812 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_46_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 424.0 MiB)\n",
      "15:20:50.813 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_46_piece0 in memory on 172.23.57.81:37441 (size: 34.7 KiB, free: 434.0 MiB)\n",
      "15:20:50.814 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 46 from parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:50.815 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:50.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 98 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 9\n",
      "15:20:50.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 29 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "15:20:50.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 42 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:50.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:50.825 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:50.825 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 42 (MapPartitionsRDD[98] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:50.829 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_47 stored as values in memory (estimated size 53.5 KiB, free 424.0 MiB)\n",
      "15:20:50.835 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_47_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 423.9 MiB)\n",
      "15:20:50.836 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_47_piece0 in memory on 172.23.57.81:37441 (size: 23.2 KiB, free: 434.0 MiB)\n",
      "15:20:50.836 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 47 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:50.837 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[98] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "15:20:50.837 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 42.0 with 8 tasks resource profile 0\n",
      "15:20:50.838 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 42.0 (TID 82) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.838 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 42.0 (TID 83) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.838 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 42.0 (TID 84) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.839 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 42.0 (TID 85) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.839 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 42.0 (TID 86) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.839 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 42.0 (TID 87) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.840 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 42.0 (TID 88) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.840 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 42.0 (TID 89) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:50.840 [Executor task launch worker for task 0.0 in stage 42.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 42.0 (TID 82)\n",
      "15:20:50.842 [Executor task launch worker for task 1.0 in stage 42.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 42.0 (TID 83)\n",
      "15:20:50.843 [Executor task launch worker for task 2.0 in stage 42.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 42.0 (TID 84)\n",
      "15:20:50.848 [Executor task launch worker for task 6.0 in stage 42.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 42.0 (TID 88)\n",
      "15:20:50.848 [Executor task launch worker for task 4.0 in stage 42.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 42.0 (TID 86)\n",
      "15:20:50.852 [Executor task launch worker for task 7.0 in stage 42.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 42.0 (TID 89)\n",
      "15:20:50.856 [Executor task launch worker for task 3.0 in stage 42.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 42.0 (TID 85)\n",
      "15:20:50.856 [Executor task launch worker for task 5.0 in stage 42.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 42.0 (TID 87)\n",
      "15:20:50.864 [Executor task launch worker for task 2.0 in stage 42.0 (TID 84)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "15:20:50.867 [Executor task launch worker for task 1.0 in stage 42.0 (TID 83)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "15:20:50.876 [Executor task launch worker for task 0.0 in stage 42.0 (TID 82)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "15:20:50.879 [Executor task launch worker for task 7.0 in stage 42.0 (TID 89)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "15:20:50.886 [Executor task launch worker for task 0.0 in stage 42.0 (TID 82)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:50.888 [Executor task launch worker for task 6.0 in stage 42.0 (TID 88)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "15:20:50.887 [Executor task launch worker for task 2.0 in stage 42.0 (TID 84)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:50.890 [Executor task launch worker for task 3.0 in stage 42.0 (TID 85)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "15:20:50.891 [Executor task launch worker for task 1.0 in stage 42.0 (TID 83)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:50.891 [Executor task launch worker for task 7.0 in stage 42.0 (TID 89)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:50.894 [Executor task launch worker for task 4.0 in stage 42.0 (TID 86)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "15:20:50.902 [Executor task launch worker for task 5.0 in stage 42.0 (TID 87)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "15:20:50.906 [Executor task launch worker for task 3.0 in stage 42.0 (TID 85)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:50.909 [Executor task launch worker for task 6.0 in stage 42.0 (TID 88)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:50.916 [Executor task launch worker for task 4.0 in stage 42.0 (TID 86)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:50.920 [Executor task launch worker for task 5.0 in stage 42.0 (TID 87)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(AIRLINE, null)\n",
      "15:20:51.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_44_piece0 on 172.23.57.81:37441 in memory (size: 6.5 KiB, free: 434.0 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:51.879 [Executor task launch worker for task 6.0 in stage 42.0 (TID 88)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 42.0 (TID 88). 3926 bytes result sent to driver\n",
      "15:20:51.884 [Executor task launch worker for task 7.0 in stage 42.0 (TID 89)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 42.0 (TID 89). 3926 bytes result sent to driver\n",
      "15:20:51.886 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 42.0 (TID 88) in 1047 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "15:20:51.886 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 42.0 (TID 89) in 1046 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "15:20:51.902 [Executor task launch worker for task 3.0 in stage 42.0 (TID 85)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 42.0 (TID 85). 3926 bytes result sent to driver\n",
      "15:20:51.903 [Executor task launch worker for task 1.0 in stage 42.0 (TID 83)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 42.0 (TID 83). 3926 bytes result sent to driver\n",
      "15:20:51.903 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 42.0 (TID 85) in 1064 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "15:20:51.904 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 42.0 (TID 83) in 1066 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "15:20:51.923 [Executor task launch worker for task 0.0 in stage 42.0 (TID 82)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 42.0 (TID 82). 3926 bytes result sent to driver\n",
      "15:20:51.924 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 42.0 (TID 82) in 1086 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "15:20:51.929 [Executor task launch worker for task 2.0 in stage 42.0 (TID 84)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 42.0 (TID 84). 3926 bytes result sent to driver\n",
      "15:20:51.929 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 42.0 (TID 84) in 1091 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "15:20:51.934 [Executor task launch worker for task 4.0 in stage 42.0 (TID 86)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 42.0 (TID 86). 3926 bytes result sent to driver\n",
      "15:20:51.935 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 42.0 (TID 86) in 1096 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "15:20:51.958 [Executor task launch worker for task 5.0 in stage 42.0 (TID 87)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 42.0 (TID 87). 3926 bytes result sent to driver\n",
      "15:20:51.959 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 42.0 (TID 87) in 1120 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "15:20:51.959 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 42.0, whose tasks have all completed, from pool \n",
      "15:20:51.960 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 42 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.133 s\n",
      "15:20:51.960 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:20:51.960 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "15:20:51.960 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:20:51.960 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "15:20:51.967 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "15:20:51.973 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "15:20:51.998 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:51.999 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 30 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:52.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 44 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:52.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 43)\n",
      "15:20:52.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:52.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 44 (MapPartitionsRDD[103] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:52.005 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_48 stored as values in memory (estimated size 54.3 KiB, free 423.9 MiB)\n",
      "15:20:52.006 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_48_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 423.9 MiB)\n",
      "15:20:52.007 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_48_piece0 in memory on 172.23.57.81:37441 (size: 24.1 KiB, free: 433.9 MiB)\n",
      "15:20:52.007 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:52.008 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[103] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:52.008 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 44.0 with 1 tasks resource profile 0\n",
      "15:20:52.010 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 44.0 (TID 90) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "15:20:52.011 [Executor task launch worker for task 0.0 in stage 44.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 44.0 (TID 90)\n",
      "15:20:52.017 [Executor task launch worker for task 0.0 in stage 44.0 (TID 90)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:52.017 [Executor task launch worker for task 0.0 in stage 44.0 (TID 90)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:52.026 [Executor task launch worker for task 0.0 in stage 44.0 (TID 90)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 44.0 (TID 90). 6848 bytes result sent to driver\n",
      "15:20:52.027 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 44.0 (TID 90) in 18 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:52.027 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "15:20:52.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 44 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.026 s\n",
      "15:20:52.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:52.028 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 44: Stage finished\n",
      "15:20:52.028 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 30 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.029945 s\n",
      "15:20:52.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 104 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 10\n",
      "15:20:52.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 31 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:52.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 46 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:52.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 45)\n",
      "15:20:52.032 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:52.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 46 (MapPartitionsRDD[104] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:52.038 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_49 stored as values in memory (estimated size 54.7 KiB, free 423.8 MiB)\n",
      "15:20:52.056 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_49_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 423.8 MiB)\n",
      "15:20:52.057 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_49_piece0 in memory on 172.23.57.81:37441 (size: 24.3 KiB, free: 433.9 MiB)\n",
      "15:20:52.057 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 49 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:52.058 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[104] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:52.058 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 46.0 with 1 tasks resource profile 0\n",
      "15:20:52.059 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 46.0 (TID 91) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "15:20:52.059 [Executor task launch worker for task 0.0 in stage 46.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 46.0 (TID 91)\n",
      "15:20:52.066 [Executor task launch worker for task 0.0 in stage 46.0 (TID 91)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (11.2 KiB) non-empty blocks including 8 (11.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:52.066 [Executor task launch worker for task 0.0 in stage 46.0 (TID 91)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:52.077 [Executor task launch worker for task 0.0 in stage 46.0 (TID 91)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 46.0 (TID 91). 6433 bytes result sent to driver\n",
      "15:20:52.078 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 46.0 (TID 91) in 19 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:52.078 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "15:20:52.079 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 46 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.045 s\n",
      "15:20:52.079 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:20:52.079 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "15:20:52.079 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:20:52.079 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:52.085 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "15:20:52.099 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:52.101 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:52.101 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:52.102 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:52.102 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:52.102 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:52.102 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:52.135 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:52.136 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 32 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:52.136 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 49 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:52.136 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 48)\n",
      "15:20:52.136 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:52.137 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 49 (MapPartitionsRDD[107] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:52.154 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_50 stored as values in memory (estimated size 250.3 KiB, free 423.6 MiB)\n",
      "15:20:52.172 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_50_piece0 stored as bytes in memory (estimated size 93.4 KiB, free 423.5 MiB)\n",
      "15:20:52.173 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_50_piece0 in memory on 172.23.57.81:37441 (size: 93.4 KiB, free: 433.8 MiB)\n",
      "15:20:52.174 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 50 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:52.174 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[107] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:52.174 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 49.0 with 1 tasks resource profile 0\n",
      "15:20:52.177 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 49.0 (TID 92) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "15:20:52.178 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 49.0 (TID 92)\n",
      "15:20:52.190 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1323.0 B) non-empty blocks including 1 (1323.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:52.190 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:52.193 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:52.194 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:52.194 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:52.194 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:52.194 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:52.194 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:52.194 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:20:52.195 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:20:52.197 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "15:20:52.198 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE_Name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_DISTANCE\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE_Name (STRING);\n",
      "  optional double avg_DISTANCE;\n",
      "}\n",
      "\n",
      "       \n",
      "15:20:52.226 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131520523776502878379403663_0049_m_000000_92' to file:/home/illidan/proyecto_desde0/archivos_parquet/avg_flight/_temporary/0/task_202506131520523776502878379403663_0049_m_000000\n",
      "15:20:52.226 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131520523776502878379403663_0049_m_000000_92: Committed. Elapsed time: 0 ms.\n",
      "15:20:52.238 [Executor task launch worker for task 0.0 in stage 49.0 (TID 92)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 49.0 (TID 92). 8810 bytes result sent to driver\n",
      "15:20:52.240 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_48_piece0 on 172.23.57.81:37441 in memory (size: 24.1 KiB, free: 433.8 MiB)\n",
      "15:20:52.240 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 49.0 (TID 92) in 64 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:52.240 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "15:20:52.241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 49 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.103 s\n",
      "15:20:52.241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:52.241 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 49: Stage finished\n",
      "15:20:52.241 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 32 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.106482 s\n",
      "15:20:52.242 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job f2078a4d-9d9b-45de-b6d5-d20742399c9e.\n",
      "15:20:52.247 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_49_piece0 on 172.23.57.81:37441 in memory (size: 24.3 KiB, free: 433.9 MiB)\n",
      "15:20:52.262 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job f2078a4d-9d9b-45de-b6d5-d20742399c9e committed. Elapsed time: 20 ms.\n",
      "15:20:52.263 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job f2078a4d-9d9b-45de-b6d5-d20742399c9e.\n",
      "15:20:52.340 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_36_piece0 on 172.23.57.81:37441 in memory (size: 4.9 KiB, free: 433.9 MiB)\n",
      "15:20:52.364 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 172.23.57.81:37441 in memory (size: 34.6 KiB, free: 433.9 MiB)\n",
      "15:20:52.373 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 172.23.57.81:37441 in memory (size: 686.0 B, free: 433.9 MiB)\n",
      "15:20:52.377 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "15:20:52.377 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#810),isnotnull(AIRLINE#807)\n",
      "15:20:52.378 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_34_piece0 on 172.23.57.81:37441 in memory (size: 34.6 KiB, free: 433.9 MiB)\n",
      "15:20:52.379 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "15:20:52.379 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#869)\n",
      "15:20:52.381 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "15:20:52.381 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#865)\n",
      "15:20:52.384 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_47_piece0 on 172.23.57.81:37441 in memory (size: 23.2 KiB, free: 434.0 MiB)\n",
      "15:20:52.390 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_32_piece0 on 172.23.57.81:37441 in memory (size: 8.2 KiB, free: 434.0 MiB)\n",
      "15:20:52.395 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_37_piece0 on 172.23.57.81:37441 in memory (size: 34.5 KiB, free: 434.0 MiB)\n",
      "15:20:52.402 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 172.23.57.81:37441 in memory (size: 34.7 KiB, free: 434.0 MiB)\n",
      "15:20:52.408 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on 172.23.57.81:37441 in memory (size: 4.9 KiB, free: 434.0 MiB)\n",
      "15:20:52.415 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_29_piece0 on 172.23.57.81:37441 in memory (size: 27.5 KiB, free: 434.1 MiB)\n",
      "15:20:52.426 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_38_piece0 on 172.23.57.81:37441 in memory (size: 8.4 KiB, free: 434.1 MiB)\n",
      "15:20:52.431 [broadcast-exchange-5] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_52 stored as values in memory (estimated size 200.2 KiB, free 430.6 MiB)\n",
      "15:20:52.432 [broadcast-exchange-6] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_51 stored as values in memory (estimated size 200.3 KiB, free 430.4 MiB)\n",
      "15:20:52.437 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 172.23.57.81:37441 in memory (size: 34.6 KiB, free: 434.1 MiB)\n",
      "15:20:52.451 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_31_piece0 on 172.23.57.81:37441 in memory (size: 34.8 KiB, free: 434.1 MiB)\n",
      "15:20:52.457 [broadcast-exchange-5] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_52_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 430.9 MiB)\n",
      "15:20:52.457 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_30_piece0 on 172.23.57.81:37441 in memory (size: 95.3 KiB, free: 434.2 MiB)\n",
      "15:20:52.457 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_52_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "15:20:52.458 [broadcast-exchange-5] INFO  org.apache.spark.SparkContext - Created broadcast 52 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:52.458 [broadcast-exchange-6] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_51_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 431.1 MiB)\n",
      "15:20:52.458 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_51_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 434.2 MiB)\n",
      "15:20:52.459 [broadcast-exchange-5] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:52.459 [broadcast-exchange-6] INFO  org.apache.spark.SparkContext - Created broadcast 51 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:52.460 [broadcast-exchange-6] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:52.470 [broadcast-exchange-5] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:52.471 [broadcast-exchange-6] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:52.471 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 33 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "15:20:52.472 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 50 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "15:20:52.472 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:52.472 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:52.472 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 50 (MapPartitionsRDD[114] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "15:20:52.474 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_53 stored as values in memory (estimated size 14.5 KiB, free 431.1 MiB)\n",
      "15:20:52.488 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_53_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 431.1 MiB)\n",
      "15:20:52.488 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_53_piece0 in memory on 172.23.57.81:37441 (size: 6.3 KiB, free: 434.2 MiB)\n",
      "15:20:52.489 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 53 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:52.489 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[114] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:52.490 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 50.0 with 1 tasks resource profile 0\n",
      "15:20:52.491 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 34 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "15:20:52.491 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 51 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "15:20:52.491 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:52.491 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:52.491 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 50.0 (TID 93) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "15:20:52.491 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 51 (MapPartitionsRDD[115] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "15:20:52.491 [Executor task launch worker for task 0.0 in stage 50.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 50.0 (TID 93)\n",
      "15:20:52.493 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_54 stored as values in memory (estimated size 15.1 KiB, free 431.1 MiB)\n",
      "15:20:52.494 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_54_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 431.1 MiB)\n",
      "15:20:52.494 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_54_piece0 in memory on 172.23.57.81:37441 (size: 6.5 KiB, free: 434.2 MiB)\n",
      "15:20:52.495 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 54 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:52.495 [Executor task launch worker for task 0.0 in stage 50.0 (TID 93)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "15:20:52.495 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[115] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:52.495 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 51.0 with 1 tasks resource profile 0\n",
      "15:20:52.496 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 51.0 (TID 94) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9694 bytes) \n",
      "15:20:52.497 [Executor task launch worker for task 0.0 in stage 51.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 51.0 (TID 94)\n",
      "15:20:52.499 [Executor task launch worker for task 0.0 in stage 50.0 (TID 93)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "15:20:52.499 [Executor task launch worker for task 0.0 in stage 51.0 (TID 94)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "15:20:52.503 [Executor task launch worker for task 0.0 in stage 51.0 (TID 94)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "15:20:52.511 [Executor task launch worker for task 0.0 in stage 50.0 (TID 93)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 50.0 (TID 93). 3613 bytes result sent to driver\n",
      "15:20:52.511 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 50.0 (TID 93) in 21 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:52.512 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "15:20:52.512 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 50 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.039 s\n",
      "15:20:52.512 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:52.512 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 50: Stage finished\n",
      "15:20:52.513 [broadcast-exchange-5] INFO  org.apache.spark.scheduler.DAGScheduler - Job 33 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.042118 s\n",
      "15:20:52.513 [Executor task launch worker for task 0.0 in stage 51.0 (TID 94)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 51.0 (TID 94). 2241 bytes result sent to driver\n",
      "15:20:52.514 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 51.0 (TID 94) in 18 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:52.514 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "15:20:52.515 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 51 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.023 s\n",
      "15:20:52.515 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:52.515 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 51: Stage finished\n",
      "15:20:52.515 [broadcast-exchange-6] INFO  org.apache.spark.scheduler.DAGScheduler - Job 34 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.043620 s\n",
      "15:20:52.516 [broadcast-exchange-5] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_55 stored as values in memory (estimated size 2.0 MiB, free 429.1 MiB)\n",
      "15:20:52.518 [broadcast-exchange-6] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_56 stored as values in memory (estimated size 2.0 MiB, free 427.1 MiB)\n",
      "15:20:52.518 [broadcast-exchange-5] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_55_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 427.1 MiB)\n",
      "15:20:52.518 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_55_piece0 in memory on 172.23.57.81:37441 (size: 4.9 KiB, free: 434.2 MiB)\n",
      "15:20:52.519 [broadcast-exchange-6] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_56_piece0 stored as bytes in memory (estimated size 686.0 B, free 427.1 MiB)\n",
      "15:20:52.520 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_56_piece0 in memory on 172.23.57.81:37441 (size: 686.0 B, free: 434.2 MiB)\n",
      "15:20:52.520 [broadcast-exchange-5] INFO  org.apache.spark.SparkContext - Created broadcast 55 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:52.521 [broadcast-exchange-6] INFO  org.apache.spark.SparkContext - Created broadcast 56 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:52.527 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "15:20:52.527 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#810),isnotnull(AIRLINE#807)\n",
      "15:20:52.538 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(ORIGIN_AIRPORT),IsNotNull(AIRLINE)\n",
      "15:20:52.539 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ORIGIN_AIRPORT#810),isnotnull(AIRLINE#807)\n",
      "15:20:52.573 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_57 stored as values in memory (estimated size 200.5 KiB, free 426.9 MiB)\n",
      "15:20:52.583 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_57_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 426.9 MiB)\n",
      "15:20:52.583 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_57_piece0 in memory on 172.23.57.81:37441 (size: 34.7 KiB, free: 434.1 MiB)\n",
      "15:20:52.584 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 57 from parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:52.585 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:52.588 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 119 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 11\n",
      "15:20:52.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 35 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "15:20:52.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 52 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:52.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:52.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:52.589 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 52 (MapPartitionsRDD[119] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:52.592 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_58 stored as values in memory (estimated size 22.1 KiB, free 426.8 MiB)\n",
      "15:20:52.593 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_58_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 426.8 MiB)\n",
      "15:20:52.594 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_58_piece0 in memory on 172.23.57.81:37441 (size: 9.0 KiB, free: 434.1 MiB)\n",
      "15:20:52.595 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 58 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:52.595 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[119] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "15:20:52.595 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 52.0 with 8 tasks resource profile 0\n",
      "15:20:52.596 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 52.0 (TID 95) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.597 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 52.0 (TID 96) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.597 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 52.0 (TID 97) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.597 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 52.0 (TID 98) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.597 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 52.0 (TID 99) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.598 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 52.0 (TID 100) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.598 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 52.0 (TID 101) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.598 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 52.0 (TID 102) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:52.599 [Executor task launch worker for task 0.0 in stage 52.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 52.0 (TID 95)\n",
      "15:20:52.599 [Executor task launch worker for task 1.0 in stage 52.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 52.0 (TID 96)\n",
      "15:20:52.599 [Executor task launch worker for task 3.0 in stage 52.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 52.0 (TID 98)\n",
      "15:20:52.599 [Executor task launch worker for task 2.0 in stage 52.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 52.0 (TID 97)\n",
      "15:20:52.599 [Executor task launch worker for task 5.0 in stage 52.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 52.0 (TID 100)\n",
      "15:20:52.599 [Executor task launch worker for task 4.0 in stage 52.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 52.0 (TID 99)\n",
      "15:20:52.599 [Executor task launch worker for task 6.0 in stage 52.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 52.0 (TID 101)\n",
      "15:20:52.599 [Executor task launch worker for task 7.0 in stage 52.0 (TID 102)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 52.0 (TID 102)\n",
      "15:20:52.606 [Executor task launch worker for task 3.0 in stage 52.0 (TID 98)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "15:20:52.606 [Executor task launch worker for task 0.0 in stage 52.0 (TID 95)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "15:20:52.606 [Executor task launch worker for task 5.0 in stage 52.0 (TID 100)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "15:20:52.606 [Executor task launch worker for task 6.0 in stage 52.0 (TID 101)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "15:20:52.606 [Executor task launch worker for task 2.0 in stage 52.0 (TID 97)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "15:20:52.607 [Executor task launch worker for task 7.0 in stage 52.0 (TID 102)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "15:20:52.611 [Executor task launch worker for task 1.0 in stage 52.0 (TID 96)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "15:20:52.611 [Executor task launch worker for task 4.0 in stage 52.0 (TID 99)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "15:20:52.612 [Executor task launch worker for task 0.0 in stage 52.0 (TID 95)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.613 [Executor task launch worker for task 3.0 in stage 52.0 (TID 98)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.613 [Executor task launch worker for task 6.0 in stage 52.0 (TID 101)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.614 [Executor task launch worker for task 2.0 in stage 52.0 (TID 97)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.617 [Executor task launch worker for task 5.0 in stage 52.0 (TID 100)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.618 [Executor task launch worker for task 7.0 in stage 52.0 (TID 102)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.621 [Executor task launch worker for task 4.0 in stage 52.0 (TID 99)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.621 [Executor task launch worker for task 1.0 in stage 52.0 (TID 96)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(ORIGIN_AIRPORT, null), noteq(AIRLINE, null))\n",
      "15:20:52.725 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_54_piece0 on 172.23.57.81:37441 in memory (size: 6.5 KiB, free: 434.1 MiB)\n",
      "15:20:52.773 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_50_piece0 on 172.23.57.81:37441 in memory (size: 93.4 KiB, free: 434.2 MiB)\n",
      "15:20:52.800 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_53_piece0 on 172.23.57.81:37441 in memory (size: 6.3 KiB, free: 434.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:53.612 [Executor task launch worker for task 6.0 in stage 52.0 (TID 101)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 52.0 (TID 101). 2494 bytes result sent to driver\n",
      "15:20:53.620 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 52.0 (TID 101) in 1022 ms on 172.23.57.81 (executor driver) (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:53.967 [Executor task launch worker for task 7.0 in stage 52.0 (TID 102)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 52.0 (TID 102). 2451 bytes result sent to driver\n",
      "15:20:53.969 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 52.0 (TID 102) in 1370 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "15:20:54.087 [Executor task launch worker for task 4.0 in stage 52.0 (TID 99)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 52.0 (TID 99). 2451 bytes result sent to driver\n",
      "15:20:54.089 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 52.0 (TID 99) in 1492 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "15:20:54.142 [Executor task launch worker for task 1.0 in stage 52.0 (TID 96)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 52.0 (TID 96). 2494 bytes result sent to driver\n",
      "15:20:54.143 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 52.0 (TID 96) in 1547 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "15:20:54.150 [Executor task launch worker for task 5.0 in stage 52.0 (TID 100)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 52.0 (TID 100). 2451 bytes result sent to driver\n",
      "15:20:54.151 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 52.0 (TID 100) in 1554 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "15:20:54.158 [Executor task launch worker for task 3.0 in stage 52.0 (TID 98)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 52.0 (TID 98). 2451 bytes result sent to driver\n",
      "15:20:54.159 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 52.0 (TID 98) in 1562 ms on 172.23.57.81 (executor driver) (6/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:54.182 [Executor task launch worker for task 2.0 in stage 52.0 (TID 97)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 52.0 (TID 97). 2451 bytes result sent to driver\n",
      "15:20:54.183 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 52.0 (TID 97) in 1586 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "15:20:54.197 [Executor task launch worker for task 0.0 in stage 52.0 (TID 95)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 52.0 (TID 95). 2494 bytes result sent to driver\n",
      "15:20:54.198 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 52.0 (TID 95) in 1602 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "15:20:54.198 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "15:20:54.199 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 52 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.608 s\n",
      "15:20:54.199 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:20:54.199 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "15:20:54.199 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:20:54.199 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "15:20:54.208 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(11), advisory target size: 67108864, actual target size 3493000, minimum partition size: 1048576\n",
      "15:20:54.222 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "15:20:54.273 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:54.275 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 36 (parquet at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "15:20:54.275 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 54 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:54.275 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 53)\n",
      "15:20:54.275 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:54.275 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 54 (MapPartitionsRDD[124] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:54.286 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_59 stored as values in memory (estimated size 62.5 KiB, free 427.1 MiB)\n",
      "15:20:54.291 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_59_piece0 stored as bytes in memory (estimated size 26.0 KiB, free 427.1 MiB)\n",
      "15:20:54.291 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_59_piece0 in memory on 172.23.57.81:37441 (size: 26.0 KiB, free: 434.2 MiB)\n",
      "15:20:54.292 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 59 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:54.293 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ResultStage 54 (MapPartitionsRDD[124] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "15:20:54.293 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 54.0 with 10 tasks resource profile 0\n",
      "15:20:54.294 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 54.0 (TID 103) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.294 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 54.0 (TID 104) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.294 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 54.0 (TID 105) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.295 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 54.0 (TID 106) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.295 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 54.0 (TID 107) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.295 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 54.0 (TID 108) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.295 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 54.0 (TID 109) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.296 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 54.0 (TID 110) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.297 [Executor task launch worker for task 0.0 in stage 54.0 (TID 103)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 54.0 (TID 103)\n",
      "15:20:54.297 [Executor task launch worker for task 2.0 in stage 54.0 (TID 105)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 54.0 (TID 105)\n",
      "15:20:54.297 [Executor task launch worker for task 3.0 in stage 54.0 (TID 106)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 54.0 (TID 106)\n",
      "15:20:54.297 [Executor task launch worker for task 4.0 in stage 54.0 (TID 107)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 54.0 (TID 107)\n",
      "15:20:54.297 [Executor task launch worker for task 5.0 in stage 54.0 (TID 108)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 54.0 (TID 108)\n",
      "15:20:54.298 [Executor task launch worker for task 6.0 in stage 54.0 (TID 109)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 54.0 (TID 109)\n",
      "15:20:54.298 [Executor task launch worker for task 1.0 in stage 54.0 (TID 104)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 54.0 (TID 104)\n",
      "15:20:54.298 [Executor task launch worker for task 7.0 in stage 54.0 (TID 110)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 54.0 (TID 110)\n",
      "15:20:54.305 [Executor task launch worker for task 1.0 in stage 54.0 (TID 104)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.305 [Executor task launch worker for task 1.0 in stage 54.0 (TID 104)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.305 [Executor task launch worker for task 3.0 in stage 54.0 (TID 106)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.306 [Executor task launch worker for task 3.0 in stage 54.0 (TID 106)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "15:20:54.306 [Executor task launch worker for task 7.0 in stage 54.0 (TID 110)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.306 [Executor task launch worker for task 7.0 in stage 54.0 (TID 110)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.309 [Executor task launch worker for task 5.0 in stage 54.0 (TID 108)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.309 [Executor task launch worker for task 5.0 in stage 54.0 (TID 108)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.313 [Executor task launch worker for task 2.0 in stage 54.0 (TID 105)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.313 [Executor task launch worker for task 2.0 in stage 54.0 (TID 105)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "15:20:54.314 [Executor task launch worker for task 0.0 in stage 54.0 (TID 103)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.314 [Executor task launch worker for task 0.0 in stage 54.0 (TID 103)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "15:20:54.317 [Executor task launch worker for task 4.0 in stage 54.0 (TID 107)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.317 [Executor task launch worker for task 4.0 in stage 54.0 (TID 107)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.321 [Executor task launch worker for task 6.0 in stage 54.0 (TID 109)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.321 [Executor task launch worker for task 6.0 in stage 54.0 (TID 109)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.473 [Executor task launch worker for task 7.0 in stage 54.0 (TID 110)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 54.0 (TID 110). 9534 bytes result sent to driver\n",
      "15:20:54.474 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 54.0 (TID 111) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.475 [Executor task launch worker for task 8.0 in stage 54.0 (TID 111)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 54.0 (TID 111)\n",
      "15:20:54.477 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 54.0 (TID 110) in 181 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "15:20:54.486 [Executor task launch worker for task 8.0 in stage 54.0 (TID 111)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.487 [Executor task launch worker for task 8.0 in stage 54.0 (TID 111)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.615 [Executor task launch worker for task 5.0 in stage 54.0 (TID 108)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 54.0 (TID 108). 11058 bytes result sent to driver\n",
      "15:20:54.617 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 54.0 (TID 112) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8999 bytes) \n",
      "15:20:54.618 [Executor task launch worker for task 9.0 in stage 54.0 (TID 112)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 54.0 (TID 112)\n",
      "15:20:54.619 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 54.0 (TID 108) in 324 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "15:20:54.639 [Executor task launch worker for task 9.0 in stage 54.0 (TID 112)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.640 [Executor task launch worker for task 9.0 in stage 54.0 (TID 112)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms\n",
      "15:20:54.674 [Executor task launch worker for task 2.0 in stage 54.0 (TID 105)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 54.0 (TID 105). 12911 bytes result sent to driver\n",
      "15:20:54.674 [Executor task launch worker for task 1.0 in stage 54.0 (TID 104)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 54.0 (TID 104). 13695 bytes result sent to driver\n",
      "15:20:54.678 [Executor task launch worker for task 3.0 in stage 54.0 (TID 106)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 54.0 (TID 106). 14109 bytes result sent to driver\n",
      "15:20:54.682 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 54.0 (TID 106) in 388 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "15:20:54.682 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 54.0 (TID 105) in 388 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "15:20:54.685 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 54.0 (TID 104) in 391 ms on 172.23.57.81 (executor driver) (5/10)\n",
      "15:20:54.707 [Executor task launch worker for task 6.0 in stage 54.0 (TID 109)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 54.0 (TID 109). 13408 bytes result sent to driver\n",
      "15:20:54.708 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 54.0 (TID 109) in 413 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "15:20:54.751 [Executor task launch worker for task 8.0 in stage 54.0 (TID 111)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 54.0 (TID 111). 9987 bytes result sent to driver\n",
      "15:20:54.753 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 54.0 (TID 111) in 279 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "15:20:54.757 [Executor task launch worker for task 4.0 in stage 54.0 (TID 107)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 54.0 (TID 107). 12870 bytes result sent to driver\n",
      "15:20:54.758 [Executor task launch worker for task 0.0 in stage 54.0 (TID 103)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 54.0 (TID 103). 17238 bytes result sent to driver\n",
      "15:20:54.759 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 54.0 (TID 107) in 464 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "15:20:54.761 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 54.0 (TID 103) in 467 ms on 172.23.57.81 (executor driver) (9/10)\n",
      "15:20:54.816 [Executor task launch worker for task 9.0 in stage 54.0 (TID 112)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 54.0 (TID 112). 11676 bytes result sent to driver\n",
      "15:20:54.816 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 54.0 (TID 112) in 200 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "15:20:54.817 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "15:20:54.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 54 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.537 s\n",
      "15:20:54.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:54.817 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 54: Stage finished\n",
      "15:20:54.818 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 36 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.543978 s\n",
      "15:20:54.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 125 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 12\n",
      "15:20:54.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 37 (parquet at NativeMethodAccessorImpl.java:0) with 10 output partitions\n",
      "15:20:54.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 56 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:54.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 55)\n",
      "15:20:54.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:54.824 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 56 (MapPartitionsRDD[125] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:54.829 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_60 stored as values in memory (estimated size 70.4 KiB, free 427.0 MiB)\n",
      "15:20:54.830 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_60_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 427.0 MiB)\n",
      "15:20:54.831 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_60_piece0 in memory on 172.23.57.81:37441 (size: 27.5 KiB, free: 434.2 MiB)\n",
      "15:20:54.831 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 60 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:54.832 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[125] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "15:20:54.832 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 56.0 with 10 tasks resource profile 0\n",
      "15:20:54.833 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 56.0 (TID 113) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.833 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 56.0 (TID 114) (172.23.57.81, executor driver, partition 1, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.833 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 56.0 (TID 115) (172.23.57.81, executor driver, partition 2, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.834 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 56.0 (TID 116) (172.23.57.81, executor driver, partition 3, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.834 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 56.0 (TID 117) (172.23.57.81, executor driver, partition 4, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.834 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 56.0 (TID 118) (172.23.57.81, executor driver, partition 5, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.834 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 56.0 (TID 119) (172.23.57.81, executor driver, partition 6, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.835 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 56.0 (TID 120) (172.23.57.81, executor driver, partition 7, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.835 [Executor task launch worker for task 1.0 in stage 56.0 (TID 114)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 56.0 (TID 114)\n",
      "15:20:54.835 [Executor task launch worker for task 0.0 in stage 56.0 (TID 113)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 56.0 (TID 113)\n",
      "15:20:54.835 [Executor task launch worker for task 2.0 in stage 56.0 (TID 115)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 56.0 (TID 115)\n",
      "15:20:54.835 [Executor task launch worker for task 4.0 in stage 56.0 (TID 117)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 56.0 (TID 117)\n",
      "15:20:54.835 [Executor task launch worker for task 5.0 in stage 56.0 (TID 118)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 56.0 (TID 118)\n",
      "15:20:54.835 [Executor task launch worker for task 7.0 in stage 56.0 (TID 120)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 56.0 (TID 120)\n",
      "15:20:54.837 [Executor task launch worker for task 6.0 in stage 56.0 (TID 119)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 56.0 (TID 119)\n",
      "15:20:54.837 [Executor task launch worker for task 3.0 in stage 56.0 (TID 116)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 56.0 (TID 116)\n",
      "15:20:54.844 [Executor task launch worker for task 6.0 in stage 56.0 (TID 119)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.7 MiB) non-empty blocks including 8 (2.7 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.844 [Executor task launch worker for task 0.0 in stage 56.0 (TID 113)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.844 [Executor task launch worker for task 6.0 in stage 56.0 (TID 119)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.844 [Executor task launch worker for task 0.0 in stage 56.0 (TID 113)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.844 [Executor task launch worker for task 3.0 in stage 56.0 (TID 116)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.3 MiB) non-empty blocks including 8 (3.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.844 [Executor task launch worker for task 3.0 in stage 56.0 (TID 116)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.844 [Executor task launch worker for task 5.0 in stage 56.0 (TID 118)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.6 MiB) non-empty blocks including 8 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.844 [Executor task launch worker for task 5.0 in stage 56.0 (TID 118)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.844 [Executor task launch worker for task 4.0 in stage 56.0 (TID 117)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.845 [Executor task launch worker for task 4.0 in stage 56.0 (TID 117)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.844 [Executor task launch worker for task 7.0 in stage 56.0 (TID 120)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (1233.1 KiB) non-empty blocks including 8 (1233.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.847 [Executor task launch worker for task 7.0 in stage 56.0 (TID 120)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "15:20:54.849 [Executor task launch worker for task 1.0 in stage 56.0 (TID 114)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.9 MiB) non-empty blocks including 8 (2.9 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.849 [Executor task launch worker for task 1.0 in stage 56.0 (TID 114)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:54.849 [Executor task launch worker for task 2.0 in stage 56.0 (TID 115)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.8 MiB) non-empty blocks including 8 (2.8 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.849 [Executor task launch worker for task 2.0 in stage 56.0 (TID 115)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:54.938 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_59_piece0 on 172.23.57.81:37441 in memory (size: 26.0 KiB, free: 434.2 MiB)\n",
      "15:20:54.981 [Executor task launch worker for task 7.0 in stage 56.0 (TID 120)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 56.0 (TID 120). 7505 bytes result sent to driver\n",
      "15:20:54.982 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 56.0 (TID 121) (172.23.57.81, executor driver, partition 8, NODE_LOCAL, 8988 bytes) \n",
      "15:20:54.982 [Executor task launch worker for task 8.0 in stage 56.0 (TID 121)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 56.0 (TID 121)\n",
      "15:20:54.985 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 56.0 (TID 120) in 151 ms on 172.23.57.81 (executor driver) (1/10)\n",
      "15:20:54.996 [Executor task launch worker for task 8.0 in stage 56.0 (TID 121)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:54.996 [Executor task launch worker for task 8.0 in stage 56.0 (TID 121)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:20:55.196 [Executor task launch worker for task 5.0 in stage 56.0 (TID 118)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 56.0 (TID 118). 7505 bytes result sent to driver\n",
      "15:20:55.197 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 56.0 (TID 122) (172.23.57.81, executor driver, partition 9, NODE_LOCAL, 8988 bytes) \n",
      "15:20:55.198 [Executor task launch worker for task 9.0 in stage 56.0 (TID 122)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 56.0 (TID 122)\n",
      "15:20:55.201 [Executor task launch worker for task 1.0 in stage 56.0 (TID 114)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 56.0 (TID 114). 7505 bytes result sent to driver\n",
      "15:20:55.205 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 56.0 (TID 114) in 371 ms on 172.23.57.81 (executor driver) (2/10)\n",
      "15:20:55.211 [Executor task launch worker for task 9.0 in stage 56.0 (TID 122)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (2.2 MiB) non-empty blocks including 8 (2.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:55.211 [Executor task launch worker for task 9.0 in stage 56.0 (TID 122)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "15:20:55.214 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 56.0 (TID 118) in 380 ms on 172.23.57.81 (executor driver) (3/10)\n",
      "15:20:55.253 [Executor task launch worker for task 6.0 in stage 56.0 (TID 119)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 56.0 (TID 119). 7505 bytes result sent to driver\n",
      "15:20:55.254 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 56.0 (TID 119) in 420 ms on 172.23.57.81 (executor driver) (4/10)\n",
      "15:20:55.261 [Executor task launch worker for task 2.0 in stage 56.0 (TID 115)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 56.0 (TID 115). 7505 bytes result sent to driver\n",
      "15:20:55.261 [Executor task launch worker for task 4.0 in stage 56.0 (TID 117)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 56.0 (TID 117). 7505 bytes result sent to driver\n",
      "15:20:55.263 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 56.0 (TID 115) in 430 ms on 172.23.57.81 (executor driver) (5/10)\n",
      "15:20:55.266 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 56.0 (TID 117) in 432 ms on 172.23.57.81 (executor driver) (6/10)\n",
      "15:20:55.289 [Executor task launch worker for task 3.0 in stage 56.0 (TID 116)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 56.0 (TID 116). 7505 bytes result sent to driver\n",
      "15:20:55.291 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 56.0 (TID 116) in 457 ms on 172.23.57.81 (executor driver) (7/10)\n",
      "15:20:55.304 [Executor task launch worker for task 0.0 in stage 56.0 (TID 113)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 56.0 (TID 113). 7505 bytes result sent to driver\n",
      "15:20:55.306 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 56.0 (TID 113) in 473 ms on 172.23.57.81 (executor driver) (8/10)\n",
      "15:20:55.323 [Executor task launch worker for task 8.0 in stage 56.0 (TID 121)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 56.0 (TID 121). 7505 bytes result sent to driver\n",
      "15:20:55.324 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 56.0 (TID 121) in 343 ms on 172.23.57.81 (executor driver) (9/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:58.390 [Executor task launch worker for task 9.0 in stage 56.0 (TID 122)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 56.0 (TID 122). 7505 bytes result sent to driver\n",
      "15:20:58.391 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 56.0 (TID 122) in 3193 ms on 172.23.57.81 (executor driver) (10/10)\n",
      "15:20:58.391 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "15:20:58.391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 56 (parquet at NativeMethodAccessorImpl.java:0) finished in 3.565 s\n",
      "15:20:58.391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:20:58.391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "15:20:58.391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:20:58.391 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "15:20:58.398 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "15:20:58.408 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:58.410 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:58.410 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:58.410 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:58.410 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:58.410 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:58.411 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:58.484 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:58.485 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 38 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:58.485 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 59 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:58.485 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 58)\n",
      "15:20:58.485 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:58.486 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 59 (MapPartitionsRDD[129] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:58.524 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_61 stored as values in memory (estimated size 256.9 KiB, free 426.9 MiB)\n",
      "15:20:58.527 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_61_piece0 stored as bytes in memory (estimated size 95.3 KiB, free 426.8 MiB)\n",
      "15:20:58.528 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_61_piece0 in memory on 172.23.57.81:37441 (size: 95.3 KiB, free: 434.1 MiB)\n",
      "15:20:58.528 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 61 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:58.528 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[129] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:58.529 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 59.0 with 1 tasks resource profile 0\n",
      "15:20:58.530 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 59.0 (TID 123) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "15:20:58.531 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 59.0 (TID 123)\n",
      "15:20:58.545 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 10 (63.2 KiB) non-empty blocks including 10 (63.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:58.546 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms\n",
      "15:20:58.568 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:58.568 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:58.568 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:58.569 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:58.569 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:58.569 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:58.569 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:20:58.570 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:20:58.571 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "15:20:58.573 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_visit_per_airline\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ranking\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  optional binary AIRLINE (STRING);\n",
      "  required int64 Count_visit_per_airline;\n",
      "  required int32 ranking;\n",
      "}\n",
      "\n",
      "       \n",
      "15:20:58.646 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_2025061315205857805666663536221_0059_m_000000_123' to file:/home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/_temporary/0/task_2025061315205857805666663536221_0059_m_000000\n",
      "15:20:58.646 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_2025061315205857805666663536221_0059_m_000000_123: Committed. Elapsed time: 0 ms.\n",
      "15:20:58.650 [Executor task launch worker for task 0.0 in stage 59.0 (TID 123)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 59.0 (TID 123). 9788 bytes result sent to driver\n",
      "15:20:58.653 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 59.0 (TID 123) in 123 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:58.654 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
      "15:20:58.656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 59 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.167 s\n",
      "15:20:58.656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:58.656 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 59: Stage finished\n",
      "15:20:58.657 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 38 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.172691 s\n",
      "15:20:58.658 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 155d9a58-acc7-47e8-8b38-440ce019fad9.\n",
      "15:20:58.677 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 155d9a58-acc7-47e8-8b38-440ce019fad9 committed. Elapsed time: 18 ms.\n",
      "15:20:58.677 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 155d9a58-acc7-47e8-8b38-440ce019fad9.\n",
      "15:20:58.734 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(CANCELLED),EqualTo(CANCELLED,1)\n",
      "15:20:58.734 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(CANCELLED#827),(CANCELLED#827 = 1)\n",
      "15:20:58.757 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_62 stored as values in memory (estimated size 200.5 KiB, free 426.6 MiB)\n",
      "15:20:58.764 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_62_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 426.5 MiB)\n",
      "15:20:58.765 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_62_piece0 in memory on 172.23.57.81:37441 (size: 34.8 KiB, free: 434.1 MiB)\n",
      "15:20:58.766 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 62 from parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:58.766 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:58.770 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 133 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 13\n",
      "15:20:58.770 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 39 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "15:20:58.771 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 60 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:58.771 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:58.771 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:58.771 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 60 (MapPartitionsRDD[133] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:58.776 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_63 stored as values in memory (estimated size 18.8 KiB, free 426.5 MiB)\n",
      "15:20:58.777 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_63_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 426.5 MiB)\n",
      "15:20:58.777 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_63_piece0 in memory on 172.23.57.81:37441 (size: 8.2 KiB, free: 434.1 MiB)\n",
      "15:20:58.778 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 63 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:58.778 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[133] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "15:20:58.778 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 60.0 with 8 tasks resource profile 0\n",
      "15:20:58.780 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 60.0 (TID 124) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.780 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 60.0 (TID 125) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.780 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 60.0 (TID 126) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.780 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 60.0 (TID 127) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.780 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 60.0 (TID 128) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.781 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 60.0 (TID 129) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.781 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 60.0 (TID 130) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.781 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 60.0 (TID 131) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:58.782 [Executor task launch worker for task 0.0 in stage 60.0 (TID 124)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 60.0 (TID 124)\n",
      "15:20:58.782 [Executor task launch worker for task 2.0 in stage 60.0 (TID 126)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 60.0 (TID 126)\n",
      "15:20:58.782 [Executor task launch worker for task 1.0 in stage 60.0 (TID 125)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 60.0 (TID 125)\n",
      "15:20:58.782 [Executor task launch worker for task 3.0 in stage 60.0 (TID 127)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 60.0 (TID 127)\n",
      "15:20:58.782 [Executor task launch worker for task 5.0 in stage 60.0 (TID 129)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 60.0 (TID 129)\n",
      "15:20:58.782 [Executor task launch worker for task 7.0 in stage 60.0 (TID 131)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 60.0 (TID 131)\n",
      "15:20:58.782 [Executor task launch worker for task 4.0 in stage 60.0 (TID 128)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 60.0 (TID 128)\n",
      "15:20:58.783 [Executor task launch worker for task 6.0 in stage 60.0 (TID 130)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 60.0 (TID 130)\n",
      "15:20:58.788 [Executor task launch worker for task 2.0 in stage 60.0 (TID 126)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "15:20:58.788 [Executor task launch worker for task 0.0 in stage 60.0 (TID 124)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "15:20:58.789 [Executor task launch worker for task 5.0 in stage 60.0 (TID 129)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "15:20:58.789 [Executor task launch worker for task 1.0 in stage 60.0 (TID 125)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "15:20:58.790 [Executor task launch worker for task 4.0 in stage 60.0 (TID 128)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "15:20:58.792 [Executor task launch worker for task 7.0 in stage 60.0 (TID 131)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "15:20:58.792 [Executor task launch worker for task 3.0 in stage 60.0 (TID 127)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "15:20:58.794 [Executor task launch worker for task 6.0 in stage 60.0 (TID 130)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "15:20:58.795 [Executor task launch worker for task 2.0 in stage 60.0 (TID 126)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "15:20:58.796 [Executor task launch worker for task 0.0 in stage 60.0 (TID 124)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "15:20:58.796 [Executor task launch worker for task 1.0 in stage 60.0 (TID 125)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "15:20:58.796 [Executor task launch worker for task 4.0 in stage 60.0 (TID 128)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "15:20:58.796 [Executor task launch worker for task 5.0 in stage 60.0 (TID 129)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "15:20:58.797 [Executor task launch worker for task 7.0 in stage 60.0 (TID 131)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "15:20:58.799 [Executor task launch worker for task 6.0 in stage 60.0 (TID 130)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n",
      "15:20:58.799 [Executor task launch worker for task 3.0 in stage 60.0 (TID 127)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(CANCELLED, null), eq(CANCELLED, 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:20:59.266 [Executor task launch worker for task 4.0 in stage 60.0 (TID 128)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 60.0 (TID 128). 2296 bytes result sent to driver\n",
      "15:20:59.268 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 60.0 (TID 128) in 488 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "15:20:59.290 [Executor task launch worker for task 2.0 in stage 60.0 (TID 126)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 60.0 (TID 126). 2296 bytes result sent to driver\n",
      "15:20:59.291 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 60.0 (TID 126) in 511 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "15:20:59.306 [Executor task launch worker for task 7.0 in stage 60.0 (TID 131)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 60.0 (TID 131). 2296 bytes result sent to driver\n",
      "15:20:59.308 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 60.0 (TID 131) in 527 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "15:20:59.318 [Executor task launch worker for task 3.0 in stage 60.0 (TID 127)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 60.0 (TID 127). 2296 bytes result sent to driver\n",
      "15:20:59.318 [Executor task launch worker for task 0.0 in stage 60.0 (TID 124)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 60.0 (TID 124). 2296 bytes result sent to driver\n",
      "15:20:59.320 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 60.0 (TID 124) in 541 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "15:20:59.321 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 60.0 (TID 127) in 541 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "15:20:59.324 [Executor task launch worker for task 5.0 in stage 60.0 (TID 129)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 60.0 (TID 129). 2296 bytes result sent to driver\n",
      "15:20:59.325 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 60.0 (TID 129) in 544 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "15:20:59.328 [Executor task launch worker for task 1.0 in stage 60.0 (TID 125)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 60.0 (TID 125). 2296 bytes result sent to driver\n",
      "15:20:59.329 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 60.0 (TID 125) in 549 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "15:20:59.335 [Executor task launch worker for task 6.0 in stage 60.0 (TID 130)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 60.0 (TID 130). 2296 bytes result sent to driver\n",
      "15:20:59.336 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 60.0 (TID 130) in 555 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "15:20:59.336 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
      "15:20:59.337 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 60 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.565 s\n",
      "15:20:59.337 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:20:59.337 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "15:20:59.337 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:20:59.337 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "15:20:59.346 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(13), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "15:20:59.355 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:59.358 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:59.358 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:59.359 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:59.359 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:59.359 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:59.360 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:59.399 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "15:20:59.422 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:59.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 40 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:20:59.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 62 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:59.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 61)\n",
      "15:20:59.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:59.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 62 (MapPartitionsRDD[137] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:59.454 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_64 stored as values in memory (estimated size 254.3 KiB, free 426.3 MiB)\n",
      "15:20:59.457 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_64_piece0 stored as bytes in memory (estimated size 93.3 KiB, free 426.2 MiB)\n",
      "15:20:59.457 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_64_piece0 in memory on 172.23.57.81:37441 (size: 93.3 KiB, free: 434.0 MiB)\n",
      "15:20:59.458 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 64 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:59.459 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[137] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:59.459 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 62.0 with 1 tasks resource profile 0\n",
      "15:20:59.460 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 62.0 (TID 132) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "15:20:59.462 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 62.0 (TID 132)\n",
      "15:20:59.483 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 8 (350.4 KiB) non-empty blocks including 8 (350.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:20:59.483 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms\n",
      "15:20:59.554 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:59.555 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:59.555 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:59.555 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:20:59.555 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:20:59.555 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:20:59.556 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:20:59.556 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:20:59.560 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "15:20:59.561 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"AIRLINE\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"count_airlines_cancel\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary AIRLINE (STRING);\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 count_airlines_cancel;\n",
      "}\n",
      "\n",
      "       \n",
      "15:20:59.596 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131520597562926050341297271_0062_m_000000_132' to file:/home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/_temporary/0/task_202506131520597562926050341297271_0062_m_000000\n",
      "15:20:59.596 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131520597562926050341297271_0062_m_000000_132: Committed. Elapsed time: 0 ms.\n",
      "15:20:59.598 [Executor task launch worker for task 0.0 in stage 62.0 (TID 132)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 62.0 (TID 132). 7478 bytes result sent to driver\n",
      "15:20:59.598 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 62.0 (TID 132) in 138 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:59.599 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 62.0, whose tasks have all completed, from pool \n",
      "15:20:59.599 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 62 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.172 s\n",
      "15:20:59.599 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:59.599 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 62: Stage finished\n",
      "15:20:59.600 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 40 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.177024 s\n",
      "15:20:59.600 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job d3077e8f-75ef-47a2-85ea-8598f140f87c.\n",
      "15:20:59.615 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job d3077e8f-75ef-47a2-85ea-8598f140f87c committed. Elapsed time: 15 ms.\n",
      "15:20:59.616 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job d3077e8f-75ef-47a2-85ea-8598f140f87c.\n",
      "15:20:59.641 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "15:20:59.641 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "15:20:59.642 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(IATA_CODE)\n",
      "15:20:59.642 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(IATA_CODE#869)\n",
      "15:20:59.663 [broadcast-exchange-7] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_65 stored as values in memory (estimated size 200.2 KiB, free 426.0 MiB)\n",
      "15:20:59.670 [broadcast-exchange-7] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_65_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 425.9 MiB)\n",
      "15:20:59.671 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_65_piece0 in memory on 172.23.57.81:37441 (size: 34.6 KiB, free: 433.9 MiB)\n",
      "15:20:59.671 [broadcast-exchange-7] INFO  org.apache.spark.SparkContext - Created broadcast 65 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:59.672 [broadcast-exchange-7] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:59.680 [broadcast-exchange-7] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:59.680 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 41 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "15:20:59.680 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 63 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "15:20:59.680 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:59.681 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:59.681 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 63 (MapPartitionsRDD[141] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "15:20:59.683 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_66 stored as values in memory (estimated size 14.5 KiB, free 425.9 MiB)\n",
      "15:20:59.684 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_66_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 425.9 MiB)\n",
      "15:20:59.685 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_66_piece0 in memory on 172.23.57.81:37441 (size: 6.3 KiB, free: 433.9 MiB)\n",
      "15:20:59.685 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 66 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:59.685 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[141] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "15:20:59.686 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 63.0 with 1 tasks resource profile 0\n",
      "15:20:59.687 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 63.0 (TID 133) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9695 bytes) \n",
      "15:20:59.687 [Executor task launch worker for task 0.0 in stage 63.0 (TID 133)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 63.0 (TID 133)\n",
      "15:20:59.690 [Executor task launch worker for task 0.0 in stage 63.0 (TID 133)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airports/part-00000-10922ef7-e1bf-447c-b966-c26b9b507f33-c000.snappy.parquet, range: 0-17990, partition values: [empty row]\n",
      "15:20:59.694 [Executor task launch worker for task 0.0 in stage 63.0 (TID 133)] INFO  org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: noteq(IATA_CODE, null)\n",
      "15:20:59.698 [Executor task launch worker for task 0.0 in stage 63.0 (TID 133)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 63.0 (TID 133). 3613 bytes result sent to driver\n",
      "15:20:59.699 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 63.0 (TID 133) in 13 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:20:59.699 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 63.0, whose tasks have all completed, from pool \n",
      "15:20:59.700 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 63 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.017 s\n",
      "15:20:59.700 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 41 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:20:59.700 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 63: Stage finished\n",
      "15:20:59.700 [broadcast-exchange-7] INFO  org.apache.spark.scheduler.DAGScheduler - Job 41 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.020493 s\n",
      "15:20:59.703 [broadcast-exchange-7] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_67 stored as values in memory (estimated size 2.0 MiB, free 423.9 MiB)\n",
      "15:20:59.705 [broadcast-exchange-7] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_67_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 423.9 MiB)\n",
      "15:20:59.706 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_67_piece0 in memory on 172.23.57.81:37441 (size: 4.9 KiB, free: 433.9 MiB)\n",
      "15:20:59.706 [broadcast-exchange-7] INFO  org.apache.spark.SparkContext - Created broadcast 67 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:20:59.710 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "15:20:59.710 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "15:20:59.723 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_68 stored as values in memory (estimated size 200.2 KiB, free 423.7 MiB)\n",
      "15:20:59.731 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_68_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 423.7 MiB)\n",
      "15:20:59.732 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_68_piece0 in memory on 172.23.57.81:37441 (size: 34.5 KiB, free: 433.9 MiB)\n",
      "15:20:59.733 [Thread-3] INFO  org.apache.spark.SparkContext - Created broadcast 68 from parquet at NativeMethodAccessorImpl.java:0\n",
      "15:20:59.733 [Thread-3] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 21880221 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:20:59.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 145 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
      "15:20:59.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 42 (parquet at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "15:20:59.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 64 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:20:59.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:20:59.737 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:20:59.738 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 64 (MapPartitionsRDD[145] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:20:59.743 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_69 stored as values in memory (estimated size 18.9 KiB, free 423.6 MiB)\n",
      "15:20:59.744 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_69_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 423.6 MiB)\n",
      "15:20:59.745 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_69_piece0 in memory on 172.23.57.81:37441 (size: 8.4 KiB, free: 433.9 MiB)\n",
      "15:20:59.745 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 69 from broadcast at DAGScheduler.scala:1585\n",
      "15:20:59.746 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 64 (MapPartitionsRDD[145] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "15:20:59.746 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 64.0 with 8 tasks resource profile 0\n",
      "15:20:59.747 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 64.0 (TID 134) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.747 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 64.0 (TID 135) (172.23.57.81, executor driver, partition 1, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.747 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 64.0 (TID 136) (172.23.57.81, executor driver, partition 2, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.748 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 64.0 (TID 137) (172.23.57.81, executor driver, partition 3, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.748 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 64.0 (TID 138) (172.23.57.81, executor driver, partition 4, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.748 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 64.0 (TID 139) (172.23.57.81, executor driver, partition 5, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.748 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 64.0 (TID 140) (172.23.57.81, executor driver, partition 6, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.748 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 64.0 (TID 141) (172.23.57.81, executor driver, partition 7, PROCESS_LOCAL, 9683 bytes) \n",
      "15:20:59.749 [Executor task launch worker for task 1.0 in stage 64.0 (TID 135)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 64.0 (TID 135)\n",
      "15:20:59.749 [Executor task launch worker for task 0.0 in stage 64.0 (TID 134)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 64.0 (TID 134)\n",
      "15:20:59.749 [Executor task launch worker for task 3.0 in stage 64.0 (TID 137)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 64.0 (TID 137)\n",
      "15:20:59.749 [Executor task launch worker for task 5.0 in stage 64.0 (TID 139)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 64.0 (TID 139)\n",
      "15:20:59.749 [Executor task launch worker for task 2.0 in stage 64.0 (TID 136)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 64.0 (TID 136)\n",
      "15:20:59.749 [Executor task launch worker for task 4.0 in stage 64.0 (TID 138)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 64.0 (TID 138)\n",
      "15:20:59.749 [Executor task launch worker for task 6.0 in stage 64.0 (TID 140)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 64.0 (TID 140)\n",
      "15:20:59.750 [Executor task launch worker for task 7.0 in stage 64.0 (TID 141)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 64.0 (TID 141)\n",
      "15:20:59.755 [Executor task launch worker for task 5.0 in stage 64.0 (TID 139)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00005-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17698846, partition values: [empty row]\n",
      "15:20:59.756 [Executor task launch worker for task 6.0 in stage 64.0 (TID 140)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00006-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17101541, partition values: [empty row]\n",
      "15:20:59.756 [Executor task launch worker for task 3.0 in stage 64.0 (TID 137)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00004-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17903023, partition values: [empty row]\n",
      "15:20:59.756 [Executor task launch worker for task 0.0 in stage 64.0 (TID 134)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00000-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18113266, partition values: [empty row]\n",
      "15:20:59.756 [Executor task launch worker for task 4.0 in stage 64.0 (TID 138)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00002-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-17831672, partition values: [empty row]\n",
      "15:20:59.757 [Executor task launch worker for task 2.0 in stage 64.0 (TID 136)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00003-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18034332, partition values: [empty row]\n",
      "15:20:59.757 [Executor task launch worker for task 1.0 in stage 64.0 (TID 135)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00001-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-18047978, partition values: [empty row]\n",
      "15:20:59.770 [Executor task launch worker for task 7.0 in stage 64.0 (TID 141)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_flights/part-00007-b34af671-d5d7-439b-bf6c-1fc0fdc158b1-c000.snappy.parquet, range: 0-16756678, partition values: [empty row]\n",
      "15:20:59.778 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_66_piece0 on 172.23.57.81:37441 in memory (size: 6.3 KiB, free: 433.9 MiB)\n",
      "15:20:59.791 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_64_piece0 on 172.23.57.81:37441 in memory (size: 93.3 KiB, free: 434.0 MiB)\n",
      "15:21:00.142 [Executor task launch worker for task 1.0 in stage 64.0 (TID 135)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 64.0 (TID 135). 2210 bytes result sent to driver\n",
      "15:21:00.146 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 64.0 (TID 135) in 398 ms on 172.23.57.81 (executor driver) (1/8)\n",
      "15:21:00.147 [Executor task launch worker for task 7.0 in stage 64.0 (TID 141)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 64.0 (TID 141). 2210 bytes result sent to driver\n",
      "15:21:00.147 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 64.0 (TID 141) in 399 ms on 172.23.57.81 (executor driver) (2/8)\n",
      "15:21:00.153 [Executor task launch worker for task 4.0 in stage 64.0 (TID 138)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 64.0 (TID 138). 2210 bytes result sent to driver\n",
      "15:21:00.154 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 64.0 (TID 138) in 406 ms on 172.23.57.81 (executor driver) (3/8)\n",
      "15:21:00.155 [Executor task launch worker for task 2.0 in stage 64.0 (TID 136)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 64.0 (TID 136). 2210 bytes result sent to driver\n",
      "15:21:00.156 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 64.0 (TID 136) in 408 ms on 172.23.57.81 (executor driver) (4/8)\n",
      "15:21:00.160 [Executor task launch worker for task 3.0 in stage 64.0 (TID 137)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 64.0 (TID 137). 2210 bytes result sent to driver\n",
      "15:21:00.161 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 64.0 (TID 137) in 414 ms on 172.23.57.81 (executor driver) (5/8)\n",
      "15:21:00.165 [Executor task launch worker for task 0.0 in stage 64.0 (TID 134)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 64.0 (TID 134). 2210 bytes result sent to driver\n",
      "15:21:00.166 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 64.0 (TID 134) in 419 ms on 172.23.57.81 (executor driver) (6/8)\n",
      "15:21:00.199 [Executor task launch worker for task 5.0 in stage 64.0 (TID 139)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 64.0 (TID 139). 2339 bytes result sent to driver\n",
      "15:21:00.199 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 64.0 (TID 139) in 451 ms on 172.23.57.81 (executor driver) (7/8)\n",
      "15:21:00.243 [Executor task launch worker for task 6.0 in stage 64.0 (TID 140)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 64.0 (TID 140). 2339 bytes result sent to driver\n",
      "15:21:00.244 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 64.0 (TID 140) in 496 ms on 172.23.57.81 (executor driver) (8/8)\n",
      "15:21:00.244 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 64.0, whose tasks have all completed, from pool \n",
      "15:21:00.244 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 64 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.505 s\n",
      "15:21:00.244 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:21:00.244 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "15:21:00.244 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:21:00.244 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "15:21:00.248 [Thread-3] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "15:21:00.255 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetUtils - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:21:00.256 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:21:00.256 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:21:00.256 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:21:00.256 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:21:00.256 [Thread-3] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:21:00.256 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:21:00.277 [Thread-3] INFO  org.apache.spark.sql.execution.aggregate.HashAggregateExec - spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "15:21:00.290 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:21:00.291 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 43 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:00.292 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 66 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:21:00.292 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 65)\n",
      "15:21:00.292 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:00.292 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 66 (MapPartitionsRDD[148] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:00.313 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_70 stored as values in memory (estimated size 251.0 KiB, free 423.8 MiB)\n",
      "15:21:00.315 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_70_piece0 stored as bytes in memory (estimated size 92.4 KiB, free 423.7 MiB)\n",
      "15:21:00.316 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_70_piece0 in memory on 172.23.57.81:37441 (size: 92.4 KiB, free: 433.9 MiB)\n",
      "15:21:00.316 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 70 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:00.317 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[148] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:00.317 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 66.0 with 1 tasks resource profile 0\n",
      "15:21:00.318 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 66.0 (TID 142) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "15:21:00.318 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 66.0 (TID 142)\n",
      "15:21:00.327 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 2 (457.0 KiB) non-empty blocks including 2 (457.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:21:00.327 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:21:00.329 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:21:00.330 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:21:00.330 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:21:00.330 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "15:21:00.330 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "15:21:00.331 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "15:21:00.331 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:21:00.331 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.parquet.hadoop.codec.CodecConfig - Compression: SNAPPY\n",
      "15:21:00.333 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.parquet.hadoop.ParquetOutputFormat - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "15:21:00.334 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"DESTINATION_AIRPORT\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"Count_airports\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary DESTINATION_AIRPORT (STRING);\n",
      "  required int64 Count_airports;\n",
      "}\n",
      "\n",
      "       \n",
      "15:21:00.465 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_202506131521003604898053948731100_0066_m_000000_142' to file:/home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/_temporary/0/task_202506131521003604898053948731100_0066_m_000000\n",
      "15:21:00.465 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202506131521003604898053948731100_0066_m_000000_142: Committed. Elapsed time: 0 ms.\n",
      "15:21:00.466 [Executor task launch worker for task 0.0 in stage 66.0 (TID 142)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 66.0 (TID 142). 7242 bytes result sent to driver\n",
      "15:21:00.467 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 66.0 (TID 142) in 150 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:00.468 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 66.0, whose tasks have all completed, from pool \n",
      "15:21:00.468 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 66 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.174 s\n",
      "15:21:00.468 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:00.468 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 66: Stage finished\n",
      "15:21:00.469 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 43 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.178022 s\n",
      "15:21:00.469 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 16397c66-6a4a-4017-b8df-861bf435df41.\n",
      "15:21:00.482 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 16397c66-6a4a-4017-b8df-861bf435df41 committed. Elapsed time: 13 ms.\n",
      "15:21:00.483 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 16397c66-6a4a-4017-b8df-861bf435df41.\n"
     ]
    }
   ],
   "source": [
    "#Create parquet files\n",
    "\n",
    "try:\n",
    "    avg_flight.write.parquet(config[\"Parquet_file\"][\"avg_flight\"], mode=\"overwrite\")\n",
    "\n",
    "    Rank_airline_in_airport.write.parquet(config[\"Parquet_file\"][\"airline_in_airport\"], mode=\"overwrite\")\n",
    "\n",
    "    flights_per_cancell.write.parquet(config[\"Parquet_file\"][\"flights_per_cancell\"], mode=\"overwrite\")\n",
    "\n",
    "    airport_notin_thelist.write.parquet(config[\"Parquet_file\"][\"airport_notin_thelist\"], mode=\"overwrite\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error write parquet files:\" + str(error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
