{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55254b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58dc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_DB (parquet_DF, table):\n",
    "    try:\n",
    "        parquet_DF.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table,\n",
    "            mode=\"overwrite\",\n",
    "            properties=properties\n",
    "        )\n",
    "    except Exception as error:\n",
    "        logger.error(\"Error ruta .yaml:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf37df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:21:38.722 [Thread-4] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .config(\"spark.jars\", \"/home/illidan/proyecto_desde0/postgre/jars/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "772d68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD conection\n",
    "\n",
    "try:\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5433/sparkdb_dev\"\n",
    "    properties = {\n",
    "        \"user\": config[\"sparkdb_dev\"][\"user\"],\n",
    "        \"password\": config[\"sparkdb_dev\"][\"password\"],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "except Exception as error:\n",
    "    logger.error(\"Error conection BD:\" + str(error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb483186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:21:38.767 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 1 ms to list leaf files for 1 paths.\n",
      "15:21:38.811 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:21:38.813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 20 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:38.813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:21:38.813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:38.813 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:38.814 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[73] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:38.824 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 102.9 KiB, free 433.7 MiB)\n",
      "15:21:38.852 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 433.7 MiB)\n",
      "15:21:38.853 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "15:21:38.853 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:38.854 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[73] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:38.854 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks resource profile 0\n",
      "15:21:38.856 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 20) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9367 bytes) \n",
      "15:21:38.857 [Executor task launch worker for task 0.0 in stage 20.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 20)\n",
      "15:21:38.878 [Executor task launch worker for task 0.0 in stage 20.0 (TID 20)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 20). 1763 bytes result sent to driver\n",
      "15:21:38.879 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 20) in 24 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:38.879 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "15:21:38.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.065 s\n",
      "15:21:38.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:38.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 20: Stage finished\n",
      "15:21:38.880 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 20 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.068828 s\n",
      "15:21:38.901 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "15:21:38.936 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:21:38.937 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 21 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:38.937 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:21:38.937 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:38.937 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:38.939 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[75] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:38.947 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 102.9 KiB, free 433.6 MiB)\n",
      "15:21:38.983 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 433.6 MiB)\n",
      "15:21:38.983 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.2 MiB)\n",
      "15:21:38.984 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:38.985 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[75] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:38.985 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks resource profile 0\n",
      "15:21:38.986 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_28_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "15:21:38.991 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 172.23.57.81:34293 in memory (size: 14.3 KiB, free: 434.3 MiB)\n",
      "15:21:38.994 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 21) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9375 bytes) \n",
      "15:21:38.995 [Executor task launch worker for task 0.0 in stage 21.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 21)\n",
      "15:21:39.005 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on 172.23.57.81:34293 in memory (size: 14.6 KiB, free: 434.3 MiB)\n",
      "15:21:39.011 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 172.23.57.81:34293 in memory (size: 34.8 KiB, free: 434.3 MiB)\n",
      "15:21:39.026 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 172.23.57.81:34293 in memory (size: 34.9 KiB, free: 434.4 MiB)\n",
      "15:21:39.026 [Executor task launch worker for task 0.0 in stage 21.0 (TID 21)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 21). 1908 bytes result sent to driver\n",
      "15:21:39.027 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 21) in 33 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:39.028 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "15:21:39.031 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.090 s\n",
      "15:21:39.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:39.033 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 21: Stage finished\n",
      "15:21:39.034 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 21 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.097924 s\n",
      "15:21:39.078 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "15:21:39.110 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.111 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 22 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:39.111 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:21:39.111 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:39.112 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:39.112 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[77] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:39.119 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 102.9 KiB, free 434.2 MiB)\n",
      "15:21:39.140 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.1 MiB)\n",
      "15:21:39.142 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "15:21:39.143 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:39.144 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[77] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:39.144 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 1 tasks resource profile 0\n",
      "15:21:39.145 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 22) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9376 bytes) \n",
      "15:21:39.146 [Executor task launch worker for task 0.0 in stage 22.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 22)\n",
      "15:21:39.166 [Executor task launch worker for task 0.0 in stage 22.0 (TID 22)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 22). 1815 bytes result sent to driver\n",
      "15:21:39.167 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 22) in 22 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:39.168 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "15:21:39.168 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 22 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.055 s\n",
      "15:21:39.168 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:39.169 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 22: Stage finished\n",
      "15:21:39.169 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 22 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.058491 s\n",
      "15:21:39.184 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "15:21:39.210 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.211 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 23 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:39.211 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "15:21:39.211 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:39.211 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:39.212 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[79] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:39.219 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 102.9 KiB, free 434.0 MiB)\n",
      "15:21:39.222 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.0 MiB)\n",
      "15:21:39.222 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "15:21:39.223 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:39.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[79] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:39.223 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 1 tasks resource profile 0\n",
      "15:21:39.225 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 23) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9378 bytes) \n",
      "15:21:39.226 [Executor task launch worker for task 0.0 in stage 23.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 23)\n",
      "15:21:39.238 [Executor task launch worker for task 0.0 in stage 23.0 (TID 23)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 23). 1727 bytes result sent to driver\n",
      "15:21:39.239 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 23) in 15 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:39.240 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "15:21:39.240 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
      "15:21:39.240 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:39.240 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 23: Stage finished\n",
      "15:21:39.241 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 23 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.030626 s\n"
     ]
    }
   ],
   "source": [
    "#Read parquet file\n",
    "try:\n",
    "    read_avg_flight = config['Parquet_file']['avg_flight']\n",
    "    read_airline_in_airport = config['Parquet_file']['airline_in_airport']\n",
    "    read_flights_per_cancell = config['Parquet_file']['flights_per_cancell']\n",
    "    read_airport_notin_thelist = config['Parquet_file']['airport_notin_thelist']\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read parquet:\" + str(error))\n",
    "\n",
    "\n",
    "try:\n",
    "    avg_flight = spark.read.parquet(read_avg_flight)\n",
    "    airline_in_airport =  spark.read.parquet(read_airline_in_airport)\n",
    "    flights_per_cancell = spark.read.parquet(read_flights_per_cancell)\n",
    "    airport_notin_thelist = spark.read.parquet(read_airport_notin_thelist)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31bdbb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:21:39.328 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "15:21:39.329 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "15:21:39.341 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 200.9 KiB, free 433.8 MiB)\n",
      "15:21:39.358 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_30_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "15:21:39.365 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_31_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "15:21:39.366 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.0 MiB)\n",
      "15:21:39.366 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 172.23.57.81:34293 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "15:21:39.368 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 32 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.370 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:21:39.373 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_29_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "15:21:39.397 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.399 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 24 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:39.399 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 24 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "15:21:39.399 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:39.399 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:39.403 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 24 (MapPartitionsRDD[85] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:39.434 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33 stored as values in memory (estimated size 31.6 KiB, free 434.1 MiB)\n",
      "15:21:39.437 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_33_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 434.1 MiB)\n",
      "15:21:39.438 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_33_piece0 in memory on 172.23.57.81:34293 (size: 14.3 KiB, free: 434.4 MiB)\n",
      "15:21:39.438 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 33 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:39.439 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[85] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:39.439 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 24.0 with 1 tasks resource profile 0\n",
      "15:21:39.441 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 24.0 (TID 24) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9874 bytes) \n",
      "15:21:39.442 [Executor task launch worker for task 0.0 in stage 24.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 24.0 (TID 24)\n",
      "15:21:39.466 [Executor task launch worker for task 0.0 in stage 24.0 (TID 24)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/avg_flight/part-00000-d48be383-ba1e-423e-ae08-3fd0f88d6a29-c000.snappy.parquet, range: 0-1142, partition values: [empty row]\n",
      "15:21:39.500 [Executor task launch worker for task 0.0 in stage 24.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 24.0 (TID 24). 1757 bytes result sent to driver\n",
      "15:21:39.501 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 24.0 (TID 24) in 61 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:39.501 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "15:21:39.502 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 24 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.097 s\n",
      "15:21:39.503 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:39.503 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 24: Stage finished\n",
      "15:21:39.503 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.105348 s\n",
      "15:21:39.574 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "15:21:39.575 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "15:21:39.584 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34 stored as values in memory (estimated size 201.2 KiB, free 433.9 MiB)\n",
      "15:21:39.597 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_34_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.9 MiB)\n",
      "15:21:39.597 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_34_piece0 in memory on 172.23.57.81:34293 (size: 35.0 KiB, free: 434.3 MiB)\n",
      "15:21:39.598 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 34 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.599 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:21:39.613 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 25 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:39.615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "15:21:39.615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:39.616 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:39.617 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[91] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:39.628 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35 stored as values in memory (estimated size 33.3 KiB, free 433.9 MiB)\n",
      "15:21:39.640 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_35_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 433.8 MiB)\n",
      "15:21:39.640 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_35_piece0 in memory on 172.23.57.81:34293 (size: 15.0 KiB, free: 434.3 MiB)\n",
      "15:21:39.641 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:39.642 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[91] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:39.642 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks resource profile 0\n",
      "15:21:39.643 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 25) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9882 bytes) \n",
      "15:21:39.644 [Executor task launch worker for task 0.0 in stage 25.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 25)\n",
      "15:21:39.656 [Executor task launch worker for task 0.0 in stage 25.0 (TID 25)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/part-00000-7d23c0df-bb29-4208-8599-23d11472e04e-c000.snappy.parquet, range: 0-10542, partition values: [empty row]\n",
      "15:21:39.723 [Executor task launch worker for task 0.0 in stage 25.0 (TID 25)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 25). 1843 bytes result sent to driver\n",
      "15:21:39.725 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 25) in 82 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:39.725 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_33_piece0 on 172.23.57.81:34293 in memory (size: 14.3 KiB, free: 434.3 MiB)\n",
      "15:21:39.725 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "15:21:39.726 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.108 s\n",
      "15:21:39.727 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:39.727 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 25: Stage finished\n",
      "15:21:39.727 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 25 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.113123 s\n",
      "15:21:39.738 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_32_piece0 on 172.23.57.81:34293 in memory (size: 34.9 KiB, free: 434.4 MiB)\n",
      "15:21:39.830 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "15:21:39.831 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "15:21:39.842 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36 stored as values in memory (estimated size 201.1 KiB, free 433.9 MiB)\n",
      "15:21:39.849 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.9 MiB)\n",
      "15:21:39.850 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_36_piece0 in memory on 172.23.57.81:34293 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "15:21:39.850 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 36 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.851 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:21:39.865 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:39.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 26 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:39.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 26 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "15:21:39.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:39.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:39.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 26 (MapPartitionsRDD[97] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:39.877 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37 stored as values in memory (estimated size 32.4 KiB, free 433.9 MiB)\n",
      "15:21:39.878 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_37_piece0 stored as bytes in memory (estimated size 14.6 KiB, free 433.8 MiB)\n",
      "15:21:39.878 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_37_piece0 in memory on 172.23.57.81:34293 (size: 14.6 KiB, free: 434.3 MiB)\n",
      "15:21:39.879 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 37 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:39.879 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[97] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:39.880 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 26.0 with 1 tasks resource profile 0\n",
      "15:21:39.880 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 26.0 (TID 26) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9883 bytes) \n",
      "15:21:39.881 [Executor task launch worker for task 0.0 in stage 26.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 26.0 (TID 26)\n",
      "15:21:39.888 [Executor task launch worker for task 0.0 in stage 26.0 (TID 26)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/part-00000-cee7e7e0-bd1d-4b7c-b26e-923db2a962c2-c000.snappy.parquet, range: 0-1786, partition values: [empty row]\n",
      "15:21:39.915 [Executor task launch worker for task 0.0 in stage 26.0 (TID 26)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 26.0 (TID 26). 1757 bytes result sent to driver\n",
      "15:21:39.916 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 26.0 (TID 26) in 36 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:39.916 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "15:21:39.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 26 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.049 s\n",
      "15:21:39.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:39.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 26: Stage finished\n",
      "15:21:39.918 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 26 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.052472 s\n",
      "15:21:39.983 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "15:21:39.983 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "15:21:39.993 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38 stored as values in memory (estimated size 200.9 KiB, free 433.6 MiB)\n",
      "15:21:40.001 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.6 MiB)\n",
      "15:21:40.001 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_38_piece0 in memory on 172.23.57.81:34293 (size: 34.8 KiB, free: 434.3 MiB)\n",
      "15:21:40.002 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 38 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:40.003 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:21:40.018 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:21:40.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 27 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:21:40.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "15:21:40.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:21:40.019 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:21:40.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[103] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:21:40.026 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39 stored as values in memory (estimated size 31.6 KiB, free 433.6 MiB)\n",
      "15:21:40.034 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 433.6 MiB)\n",
      "15:21:40.034 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_39_piece0 in memory on 172.23.57.81:34293 (size: 14.3 KiB, free: 434.3 MiB)\n",
      "15:21:40.036 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
      "15:21:40.036 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_35_piece0 on 172.23.57.81:34293 in memory (size: 15.0 KiB, free: 434.3 MiB)\n",
      "15:21:40.037 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[103] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:21:40.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 1 tasks resource profile 0\n",
      "15:21:40.039 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 27) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9885 bytes) \n",
      "15:21:40.040 [Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 27)\n",
      "15:21:40.042 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_34_piece0 on 172.23.57.81:34293 in memory (size: 35.0 KiB, free: 434.3 MiB)\n",
      "15:21:40.046 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_37_piece0 on 172.23.57.81:34293 in memory (size: 14.6 KiB, free: 434.3 MiB)\n",
      "15:21:40.052 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_36_piece0 on 172.23.57.81:34293 in memory (size: 34.9 KiB, free: 434.4 MiB)\n",
      "15:21:40.053 [Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/part-00000-6a3ef101-8abe-4055-b505-be7275825ef4-c000.snappy.parquet, range: 0-3699, partition values: [empty row]\n",
      "15:21:40.098 [Executor task launch worker for task 0.0 in stage 27.0 (TID 27)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 27). 1757 bytes result sent to driver\n",
      "15:21:40.100 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 27) in 61 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:21:40.100 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "15:21:40.101 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.080 s\n",
      "15:21:40.102 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:21:40.102 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 27: Stage finished\n",
      "15:21:40.102 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 27 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.083819 s\n"
     ]
    }
   ],
   "source": [
    "#write parquet file into DB\n",
    "\n",
    "write_DB(avg_flight, \"avg_flight\")\n",
    "write_DB(airline_in_airport,\"airline_in_airport\")\n",
    "write_DB(flights_per_cancell,\"flights_cancellation\")\n",
    "write_DB(airport_notin_thelist,\"airport_notin_thelist\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
