{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f592a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2, concat_ws, col, concat, when, length, lpad, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9bdf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read later\n",
    "\n",
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")\n",
    "\n",
    "def create_hash(Datafram):\n",
    "    value = Datafram.withColumn(\"hash_id\", sha2(concat_ws(\"||\", *Datafram.columns), 256))\n",
    "    return value\n",
    "\n",
    "def write_DB (parquet_DF, table):\n",
    "    try:\n",
    "        parquet_DF.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table,\n",
    "            mode=\"append\",\n",
    "            properties=properties\n",
    "        )\n",
    "    except Exception as error:\n",
    "        logger.error(\"Error write into DB:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f681809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:11.358 [Thread-4] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .config(\"spark.jars\", \"/home/illidan/proyecto_desde0/postgre/jars/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b2ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06da9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD conection\n",
    "\n",
    "try:\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5433/sparkdb_dev\"\n",
    "    properties = {\n",
    "        \"user\": \"spark\", #config['sparkdb_dev']['user'],\n",
    "        \"password\": \"spark\", #config['sparkdb_dev']['password'],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "except Exception as error:\n",
    "    logger.error(\"Error conection BD:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e30a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:11.418 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "03:03:11.422 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/illidan/proyecto_desde0/ETL/spark-warehouse'.\n",
      "03:03:11.446 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42d78688{/SQL,null,AVAILABLE,@Spark}\n",
      "03:03:11.448 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1afe699d{/SQL/json,null,AVAILABLE,@Spark}\n",
      "03:03:11.449 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f0be16a{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "03:03:11.451 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2886a741{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "03:03:11.453 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25abe822{/static/sql,null,AVAILABLE,@Spark}\n",
      "03:03:12.506 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 70 ms to list leaf files for 1 paths.\n",
      "03:03:13.086 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "03:03:13.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "03:03:13.109 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "03:03:13.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "03:03:13.112 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "03:03:13.117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "03:03:13.217 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 102.9 KiB, free 434.3 MiB)\n",
      "03:03:13.255 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
      "03:03:13.259 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.23.57.81:33405 (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:13.263 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "03:03:13.283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "03:03:13.285 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "03:03:13.350 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9367 bytes) \n",
      "03:03:13.370 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:13.819 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3053 bytes result sent to driver\n",
      "03:03:13.831 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 507 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "03:03:13.833 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "03:03:13.838 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.701 s\n",
      "03:03:13.841 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "03:03:13.842 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "03:03:13.844 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.757311 s\n",
      "03:03:14.099 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.23.57.81:33405 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:14.709 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "03:03:14.839 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "03:03:14.841 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "03:03:14.841 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "03:03:14.842 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "03:03:14.842 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "03:03:14.846 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "03:03:14.862 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 102.9 KiB, free 434.3 MiB)\n",
      "03:03:14.872 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
      "03:03:14.875 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.23.57.81:33405 (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:14.876 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "03:03:14.878 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "03:03:14.878 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "03:03:14.880 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9367 bytes) \n",
      "03:03:14.882 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n",
      "03:03:14.913 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1662 bytes result sent to driver\n",
      "03:03:14.917 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 37 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "03:03:14.918 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "03:03:14.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.071 s\n",
      "03:03:14.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "03:03:14.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "03:03:14.921 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.081834 s\n",
      "03:03:14.959 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "03:03:15.018 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "03:03:15.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "03:03:15.021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "03:03:15.021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "03:03:15.021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "03:03:15.022 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "03:03:15.041 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 102.9 KiB, free 434.2 MiB)\n",
      "03:03:15.048 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.1 MiB)\n",
      "03:03:15.050 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.23.57.81:33405 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "03:03:15.052 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "03:03:15.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "03:03:15.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "03:03:15.055 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9368 bytes) \n",
      "03:03:15.056 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "03:03:15.078 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1945 bytes result sent to driver\n",
      "03:03:15.083 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 29 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "03:03:15.084 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "03:03:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.061 s\n",
      "03:03:15.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "03:03:15.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "03:03:15.086 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.067811 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d3d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the year, month and day columns into a single date column\n",
    "try:\n",
    "        df_flights = df_flights.withColumn(\"DATE\", \n",
    "                                                to_date(\n",
    "                                                        concat(\n",
    "                                                                col(\"YEAR\"),\n",
    "                                                                when(length(col(\"MONTH\")) == 1, lpad(col(\"MONTH\"), 2, \"0\")).otherwise(col(\"MONTH\")),\n",
    "                                                                when(length(col(\"DAY\")) == 1, lpad(col(\"DAY\"), 2, \"0\")).otherwise(col(\"DAY\"))\n",
    "                                                        ),\\\n",
    "                                                        \"yyyyMMdd\"\n",
    "                                                )\n",
    "                                        )\n",
    "except Exception as error:\n",
    "    logger.error(\"Error al crear la columna DATE:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd7fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:15.568 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:33405 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:15.574 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:33405 in memory (size: 37.2 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "#Create hash_id to identify the rows when insert into the database\n",
    "try:\n",
    "    df_airline = create_hash(df_airline)\n",
    "    df_airports = create_hash(df_airports)\n",
    "    df_flights = create_hash(df_flights)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error create_hash:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b537050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:51:25.440 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "03:51:25.440 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "03:51:25.449 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39 stored as values in memory (estimated size 200.9 KiB, free 429.7 MiB)\n",
      "03:51:25.457 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_39_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 429.7 MiB)\n",
      "03:51:25.458 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_39_piece0 in memory on 172.23.57.81:33405 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "03:51:25.458 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 39 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "03:51:25.459 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "03:51:25.475 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "03:51:25.476 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 24 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "03:51:25.476 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 28 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "03:51:25.476 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "03:51:25.477 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "03:51:25.478 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 28 (MapPartitionsRDD[97] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "03:51:25.487 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_40 stored as values in memory (estimated size 35.4 KiB, free 429.6 MiB)\n",
      "03:51:25.488 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_40_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 429.6 MiB)\n",
      "03:51:25.489 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_40_piece0 in memory on 172.23.57.81:33405 (size: 16.4 KiB, free: 434.3 MiB)\n",
      "03:51:25.489 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
      "03:51:25.490 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[97] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "03:51:25.490 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 28.0 with 1 tasks resource profile 0\n",
      "03:51:25.491 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 28.0 (TID 24) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9874 bytes) \n",
      "03:51:25.492 [Executor task launch worker for task 0.0 in stage 28.0 (TID 24)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 28.0 (TID 24)\n",
      "03:51:25.499 [Executor task launch worker for task 0.0 in stage 28.0 (TID 24)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "03:51:25.523 [Executor task launch worker for task 0.0 in stage 28.0 (TID 24)] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 28.0 (TID 24)\n",
      "java.sql.BatchUpdateException: Batch entry 1 INSERT INTO airlines (\"iata_code\",\"airline\",\"hash_id\") VALUES (('AA'),('American Airlines Inc.'),('cce55255cd561b6ada9e616bf4045b122e4bc120662168088299cb9c54353405')) was aborted: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.4.jar:3.5.4]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
      "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420) ~[postgresql-42.7.4.jar:42.7.4]\n",
      "\t... 21 more\n",
      "03:51:25.534 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_38_piece0 on 172.23.57.81:33405 in memory (size: 7.0 KiB, free: 434.3 MiB)\n",
      "03:51:25.534 [task-result-getter-1] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 28.0 (TID 24) (172.23.57.81 executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO airlines (\"iata_code\",\"airline\",\"hash_id\") VALUES (('AA'),('American Airlines Inc.'),('cce55255cd561b6ada9e616bf4045b122e4bc120662168088299cb9c54353405')) was aborted: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "03:51:25.534 [task-result-getter-1] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 28.0 failed 1 times; aborting job\n",
      "03:51:25.535 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "03:51:25.535 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 28\n",
      "03:51:25.535 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 28: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 24) (172.23.57.81 executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO airlines (\"iata_code\",\"airline\",\"hash_id\") VALUES (('AA'),('American Airlines Inc.'),('cce55255cd561b6ada9e616bf4045b122e4bc120662168088299cb9c54353405')) was aborted: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:\n",
      "03:51:25.535 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 28 (jdbc at NativeMethodAccessorImpl.java:0) failed in 0.056 s due to Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 24) (172.23.57.81 executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO airlines (\"iata_code\",\"airline\",\"hash_id\") VALUES (('AA'),('American Airlines Inc.'),('cce55255cd561b6ada9e616bf4045b122e4bc120662168088299cb9c54353405')) was aborted: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:\n",
      "03:51:25.536 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 24 failed: jdbc at NativeMethodAccessorImpl.java:0, took 0.061294 s\n",
      "03:51:25.539 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_36_piece0 on 172.23.57.81:33405 in memory (size: 7.1 KiB, free: 434.3 MiB)\n",
      "03:51:25.723 [Thread-4] ERROR __main__ - Error write into DB:An error occurred while calling o442.jdbc.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 24) (172.23.57.81 executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO airlines (\"iata_code\",\"airline\",\"hash_id\") VALUES (('AA'),('American Airlines Inc.'),('cce55255cd561b6ada9e616bf4045b122e4bc120662168088299cb9c54353405')) was aborted: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\n",
      "\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 1 INSERT INTO airlines (\"iata_code\",\"airline\",\"hash_id\") VALUES (('AA'),('American Airlines Inc.'),('cce55255cd561b6ada9e616bf4045b122e4bc120662168088299cb9c54353405')) was aborted: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:580)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"airlines_pkey\"\n",
      "  Detail: Key (iata_code)=(AA) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n",
      "\t... 21 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#charge the data into the database\n",
    "\n",
    "write_DB(df_airline, \"airlines\")\n",
    "\n",
    "DB_airline = spark.read.jdbc(url=jdbc_url, table=\"airlines\", properties=properties).select(\"IATA_CODE\", \"hash_id\")\n",
    "\n",
    "#find the rows in df_airline that are not in the database\n",
    "Airline_to_insert = df_airline.join(DB_airline, df_airline.IATA_CODE == DB_airline.IATA_CODE, \"left_anti\")\n",
    "\n",
    "#this are the rows thar are in the database by IATA_CODE\n",
    "rows_same_Id = df_airline.join(DB_airline, df_airline.IATA_CODE == DB_airline.IATA_CODE, \"inner\").select(df_airline.IATA_CODE, df_airline.hash_id)\n",
    "\n",
    "rows_to_update = rows_same_Id.join(DB_airline, rows_same_Id.hash_id == DB_airline.hash_id, \"left_anti\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e46b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- hash_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_airline.printSchema()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
