{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f592a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2, concat_ws, col, concat, when, length, lpad, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9bdf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read later\n",
    "\n",
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")\n",
    "\n",
    "def create_hash(Datafram):\n",
    "    value = Datafram.withColumn(\"hash_id\", sha2(concat_ws(\"||\", *Datafram.columns), 256))\n",
    "    return value\n",
    "\n",
    "def write_DB (parquet_DF, table):\n",
    "    try:\n",
    "        parquet_DF.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table,\n",
    "            mode=\"append\",\n",
    "            properties=properties\n",
    "        )\n",
    "    except Exception as error:\n",
    "        logger.error(\"Error write into DB:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f681809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:11.358 [Thread-4] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .config(\"spark.jars\", \"/home/illidan/proyecto_desde0/postgre/jars/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b2ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06da9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD conection\n",
    "\n",
    "try:\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5433/sparkdb_dev\"\n",
    "    properties = {\n",
    "        \"user\": \"spark\", #config['sparkdb_dev']['user'],\n",
    "        \"password\": \"spark\", #config['sparkdb_dev']['password'],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "except Exception as error:\n",
    "    logger.error(\"Error conection BD:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e30a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:11.418 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "03:03:11.422 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/illidan/proyecto_desde0/ETL/spark-warehouse'.\n",
      "03:03:11.446 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42d78688{/SQL,null,AVAILABLE,@Spark}\n",
      "03:03:11.448 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1afe699d{/SQL/json,null,AVAILABLE,@Spark}\n",
      "03:03:11.449 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f0be16a{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "03:03:11.451 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2886a741{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "03:03:11.453 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25abe822{/static/sql,null,AVAILABLE,@Spark}\n",
      "03:03:12.506 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 70 ms to list leaf files for 1 paths.\n",
      "03:03:13.086 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "03:03:13.108 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "03:03:13.109 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "03:03:13.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "03:03:13.112 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "03:03:13.117 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "03:03:13.217 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 102.9 KiB, free 434.3 MiB)\n",
      "03:03:13.255 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
      "03:03:13.259 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.23.57.81:33405 (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:13.263 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "03:03:13.283 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "03:03:13.285 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "03:03:13.350 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9367 bytes) \n",
      "03:03:13.370 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:13.819 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3053 bytes result sent to driver\n",
      "03:03:13.831 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 507 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "03:03:13.833 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "03:03:13.838 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.701 s\n",
      "03:03:13.841 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "03:03:13.842 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "03:03:13.844 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.757311 s\n",
      "03:03:14.099 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.23.57.81:33405 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:14.709 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "03:03:14.839 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "03:03:14.841 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "03:03:14.841 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "03:03:14.842 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "03:03:14.842 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "03:03:14.846 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "03:03:14.862 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 102.9 KiB, free 434.3 MiB)\n",
      "03:03:14.872 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
      "03:03:14.875 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.23.57.81:33405 (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:14.876 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "03:03:14.878 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "03:03:14.878 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "03:03:14.880 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9367 bytes) \n",
      "03:03:14.882 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n",
      "03:03:14.913 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1662 bytes result sent to driver\n",
      "03:03:14.917 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 37 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "03:03:14.918 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "03:03:14.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.071 s\n",
      "03:03:14.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "03:03:14.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "03:03:14.921 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.081834 s\n",
      "03:03:14.959 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "03:03:15.018 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "03:03:15.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "03:03:15.021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "03:03:15.021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "03:03:15.021 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "03:03:15.022 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "03:03:15.041 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 102.9 KiB, free 434.2 MiB)\n",
      "03:03:15.048 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.1 MiB)\n",
      "03:03:15.050 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.23.57.81:33405 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "03:03:15.052 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "03:03:15.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "03:03:15.053 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "03:03:15.055 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9368 bytes) \n",
      "03:03:15.056 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "03:03:15.078 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1945 bytes result sent to driver\n",
      "03:03:15.083 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 29 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "03:03:15.084 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "03:03:15.085 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.061 s\n",
      "03:03:15.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "03:03:15.086 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "03:03:15.086 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.067811 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d3d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the year, month and day columns into a single date column\n",
    "try:\n",
    "        df_flights = df_flights.withColumn(\"DATE\", \n",
    "                                                to_date(\n",
    "                                                        concat(\n",
    "                                                                col(\"YEAR\"),\n",
    "                                                                when(length(col(\"MONTH\")) == 1, lpad(col(\"MONTH\"), 2, \"0\")).otherwise(col(\"MONTH\")),\n",
    "                                                                when(length(col(\"DAY\")) == 1, lpad(col(\"DAY\"), 2, \"0\")).otherwise(col(\"DAY\"))\n",
    "                                                        ),\\\n",
    "                                                        \"yyyyMMdd\"\n",
    "                                                )\n",
    "                                        )\n",
    "except Exception as error:\n",
    "    logger.error(\"Error al crear la columna DATE:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd7fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:03:15.568 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:33405 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "03:03:15.574 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:33405 in memory (size: 37.2 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "#Create hash_id to identify the rows when insert into the database\n",
    "try:\n",
    "    df_airline = create_hash(df_airline)\n",
    "    df_airports = create_hash(df_airports)\n",
    "    df_flights = create_hash(df_flights)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error create_hash:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:13:01.311 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "15:13:01.311 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "15:13:01.459 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_82 stored as values in memory (estimated size 200.9 KiB, free 427.4 MiB)\n",
      "15:13:01.507 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_81_piece0 on 172.23.57.81:33405 in memory (size: 22.8 KiB, free: 434.3 MiB)\n",
      "15:13:01.534 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_76_piece0 on 172.23.57.81:33405 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "15:13:01.537 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_82_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 427.7 MiB)\n",
      "15:13:01.537 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_82_piece0 in memory on 172.23.57.81:33405 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "15:13:01.538 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 82 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:13:01.539 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "15:13:01.555 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_80_piece0 on 172.23.57.81:33405 in memory (size: 393.0 B, free: 434.3 MiB)\n",
      "15:13:01.576 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 196 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
      "15:13:01.577 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 52 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:13:01.577 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 69 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "15:13:01.577 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:13:01.577 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:13:01.578 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 69 (MapPartitionsRDD[196] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:13:01.583 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_83 stored as values in memory (estimated size 19.6 KiB, free 429.7 MiB)\n",
      "15:13:01.631 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_83_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 429.7 MiB)\n",
      "15:13:01.631 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_83_piece0 in memory on 172.23.57.81:33405 (size: 8.6 KiB, free: 434.3 MiB)\n",
      "15:13:01.632 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 83 from broadcast at DAGScheduler.scala:1585\n",
      "15:13:01.647 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[196] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:13:01.647 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 69.0 with 1 tasks resource profile 0\n",
      "15:13:01.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 198 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
      "15:13:01.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 53 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:13:01.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 70 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "15:13:01.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "15:13:01.648 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:13:01.649 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 70 (MapPartitionsRDD[198] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:13:01.656 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 69.0 (TID 52) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9863 bytes) \n",
      "15:13:01.659 [Executor task launch worker for task 0.0 in stage 69.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 69.0 (TID 52)\n",
      "15:13:01.724 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_84 stored as values in memory (estimated size 14.0 KiB, free 429.7 MiB)\n",
      "15:13:01.765 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_84_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 429.7 MiB)\n",
      "15:13:01.766 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_84_piece0 in memory on 172.23.57.81:33405 (size: 7.4 KiB, free: 434.3 MiB)\n",
      "15:13:01.767 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 84 from broadcast at DAGScheduler.scala:1585\n",
      "15:13:01.767 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[198] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:13:01.767 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 70.0 with 1 tasks resource profile 0\n",
      "15:13:01.769 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 70.0 (TID 53) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9014 bytes) \n",
      "15:13:01.771 [Executor task launch worker for task 0.0 in stage 69.0 (TID 52)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/df_airline/part-00000-0ebc70d5-797e-4b31-aceb-ec1761263b3f-c000.snappy.parquet, range: 0-1036, partition values: [empty row]\n",
      "15:13:01.773 [Executor task launch worker for task 0.0 in stage 70.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 70.0 (TID 53)\n",
      "15:13:01.799 [Executor task launch worker for task 0.0 in stage 70.0 (TID 53)] INFO  org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD - closed connection\n",
      "15:13:01.865 [Executor task launch worker for task 0.0 in stage 70.0 (TID 53)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 70.0 (TID 53). 1989 bytes result sent to driver\n",
      "15:13:01.866 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 70.0 (TID 53) in 97 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:13:01.866 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "15:13:01.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 70 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.217 s\n",
      "15:13:01.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:13:01.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 69)\n",
      "15:13:01.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:13:01.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "15:13:01.872 [Thread-4] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(15), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "15:13:01.924 [broadcast-exchange-10] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:13:01.925 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 54 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "15:13:01.925 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 72 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "15:13:01.925 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 71)\n",
      "15:13:01.925 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:13:01.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 72 (MapPartitionsRDD[200] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "15:13:01.928 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_85 stored as values in memory (estimated size 8.2 KiB, free 429.6 MiB)\n",
      "15:13:01.937 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_85_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 429.6 MiB)\n",
      "15:13:01.937 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_85_piece0 in memory on 172.23.57.81:33405 (size: 4.2 KiB, free: 434.3 MiB)\n",
      "15:13:01.938 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 85 from broadcast at DAGScheduler.scala:1585\n",
      "15:13:01.938 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[200] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "15:13:01.938 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 72.0 with 1 tasks resource profile 0\n",
      "15:13:01.938 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_84_piece0 on 172.23.57.81:33405 in memory (size: 7.4 KiB, free: 434.3 MiB)\n",
      "15:13:02.034 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 72.0 (TID 54) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 9198 bytes) \n",
      "15:13:02.035 [Executor task launch worker for task 0.0 in stage 72.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 72.0 (TID 54)\n",
      "15:13:02.040 [Executor task launch worker for task 0.0 in stage 72.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1008.0 B) non-empty blocks including 1 (1008.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:13:02.040 [Executor task launch worker for task 0.0 in stage 72.0 (TID 54)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:13:02.074 [Executor task launch worker for task 0.0 in stage 72.0 (TID 54)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 72.0 (TID 54). 3935 bytes result sent to driver\n",
      "15:13:02.086 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 72.0 (TID 54) in 146 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:13:02.086 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "15:13:02.087 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 72 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.160 s\n",
      "15:13:02.087 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:13:02.087 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 72: Stage finished\n",
      "15:13:02.088 [broadcast-exchange-10] INFO  org.apache.spark.scheduler.DAGScheduler - Job 54 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.163734 s\n",
      "15:13:02.123 [broadcast-exchange-10] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_86 stored as values in memory (estimated size 2.0 MiB, free 427.7 MiB)\n",
      "15:13:02.125 [broadcast-exchange-10] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_86_piece0 stored as bytes in memory (estimated size 393.0 B, free 427.7 MiB)\n",
      "15:13:02.125 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_86_piece0 in memory on 172.23.57.81:33405 (size: 393.0 B, free: 434.3 MiB)\n",
      "15:13:02.125 [broadcast-exchange-10] INFO  org.apache.spark.SparkContext - Created broadcast 86 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "15:13:02.145 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_85_piece0 on 172.23.57.81:33405 in memory (size: 4.2 KiB, free: 434.3 MiB)\n",
      "15:13:02.236 [Executor task launch worker for task 0.0 in stage 69.0 (TID 52)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 69.0 (TID 52). 2240 bytes result sent to driver\n",
      "15:13:02.261 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 69.0 (TID 52) in 613 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:13:02.261 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
      "15:13:02.261 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 69 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.682 s\n",
      "15:13:02.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "15:13:02.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "15:13:02.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "15:13:02.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "15:13:02.264 [Thread-4] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "15:13:03.192 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "15:13:03.193 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 55 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "15:13:03.194 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 74 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "15:13:03.194 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 73)\n",
      "15:13:03.194 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "15:13:03.194 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 74 (MapPartitionsRDD[205] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "15:13:03.238 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_87 stored as values in memory (estimated size 47.7 KiB, free 427.6 MiB)\n",
      "15:13:03.250 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_87_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 427.6 MiB)\n",
      "15:13:03.251 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_87_piece0 in memory on 172.23.57.81:33405 (size: 22.8 KiB, free: 434.3 MiB)\n",
      "15:13:03.251 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 87 from broadcast at DAGScheduler.scala:1585\n",
      "15:13:03.251 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[205] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "15:13:03.252 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 74.0 with 1 tasks resource profile 0\n",
      "15:13:03.253 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 74.0 (TID 55) (172.23.57.81, executor driver, partition 0, NODE_LOCAL, 9198 bytes) \n",
      "15:13:03.253 [Executor task launch worker for task 0.0 in stage 74.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 74.0 (TID 55)\n",
      "15:13:03.262 [Executor task launch worker for task 0.0 in stage 74.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "15:13:03.262 [Executor task launch worker for task 0.0 in stage 74.0 (TID 55)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms\n",
      "15:13:03.267 [Executor task launch worker for task 0.0 in stage 74.0 (TID 55)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 74.0 (TID 55). 5990 bytes result sent to driver\n",
      "15:13:03.268 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 74.0 (TID 55) in 16 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "15:13:03.268 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 74.0, whose tasks have all completed, from pool \n",
      "15:13:03.268 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 74 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.073 s\n",
      "15:13:03.269 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "15:13:03.269 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 74: Stage finished\n",
      "15:13:03.269 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 55 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.076168 s\n",
      "15:13:03.301 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_83_piece0 on 172.23.57.81:33405 in memory (size: 8.6 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "#charge the data into the database\n",
    "\n",
    "#write_DB(df_airline, \"airlines\")\n",
    "\n",
    "DB_airline = spark.read.jdbc(url=jdbc_url, table=\"airlines\", properties=properties).select(\"IATA_CODE\", \"hash_id\")\n",
    "\n",
    "#find the rows in df_airline that are not in the database\n",
    "#iF the id in DF is not in the database, insert it\n",
    "Airline_to_insert = df_airline.join(DB_airline, df_airline.IATA_CODE == DB_airline.IATA_CODE, \"left_anti\")\n",
    "write_DB(Airline_to_insert, \"airlines\")\n",
    "\n",
    "#this are the rows thar are in the database by IATA_CODE\n",
    "#if the Id in DF is in the database, we need to check if the hash_id is the same, if is not, we need to update the row, but if it is the same, we do nothing\n",
    "rows_same_Id = df_airline.join(DB_airline, df_airline.IATA_CODE == DB_airline.IATA_CODE, \"inner\").select(df_airline.IATA_CODE, df_airline.hash_id)\n",
    "\n",
    "rows_to_update = rows_same_Id.join(DB_airline, rows_same_Id.hash_id == DB_airline.hash_id, \"left_anti\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e46b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- hash_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_airline.printSchema()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
