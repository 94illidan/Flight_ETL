{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f592a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2, concat_ws, col, concat, when, length, lpad, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9bdf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read later\n",
    "\n",
    "def control_null (dataframe): \n",
    "    for dfcol in dataframe.columns:\n",
    "        df_null = dataframe.filter(col(dfcol).isNull()).count()\n",
    "        if df_null > 0:\n",
    "            print(f\"{dfcol} : {df_null} null\")\n",
    "        else:\n",
    "            print(f\"{dfcol} no tiene null\")\n",
    "\n",
    "def create_hash(Datafram):\n",
    "    value = Datafram.withColumn(\"hash_id\", sha2(concat_ws(\"||\", *Datafram.columns), 256))\n",
    "    return value\n",
    "\n",
    "def write_DB (parquet_DF, table):\n",
    "    try:\n",
    "        parquet_DF.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table,\n",
    "            mode=\"overwrite\",\n",
    "            properties=properties\n",
    "        )\n",
    "    except Exception as error:\n",
    "        logger.error(\"Error ruta .yaml:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f681809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:58:59.989 [Thread-3] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b2ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "    \n",
    "try:\n",
    "    read_parquet_airline = config[\"Parquet_file\"][\"df_airline\"]\n",
    "    read_parquet_flights = config[\"Parquet_file\"][\"df_flights\"]\n",
    "    read_parquet_airports = config[\"Parquet_file\"][\"df_airports\"]\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:59:00.022 [Thread-3] ERROR __main__ - Error conection BD:name 'sparkdb_dev' is not defined\n"
     ]
    }
   ],
   "source": [
    "#BD conection\n",
    "\n",
    "try:\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5433/sparkdb_dev\"\n",
    "    properties = {\n",
    "        \"user\": config[sparkdb_dev][user],\n",
    "        \"password\": config[sparkdb_dev][password],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "except Exception as error:\n",
    "    logger.error(\"Error conection BD:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e30a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:59:00.205 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "00:59:00.215 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/illidan/proyecto_desde0/ETL/spark-warehouse'.\n",
      "00:59:00.237 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@524041c0{/SQL,null,AVAILABLE,@Spark}\n",
      "00:59:00.239 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12054ca3{/SQL/json,null,AVAILABLE,@Spark}\n",
      "00:59:00.240 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36bdd607{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "00:59:00.242 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7034f0ba{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "00:59:00.258 [Thread-3] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c57414d{/static/sql,null,AVAILABLE,@Spark}\n",
      "00:59:01.406 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 136 ms to list leaf files for 1 paths.\n",
      "00:59:02.088 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "00:59:02.109 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "00:59:02.110 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "00:59:02.111 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "00:59:02.112 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "00:59:02.119 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "00:59:02.223 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "00:59:02.274 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "00:59:02.279 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.23.57.81:36935 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "00:59:02.284 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "00:59:02.320 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "00:59:02.321 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "00:59:02.391 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "00:59:02.414 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:59:02.862 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3053 bytes result sent to driver\n",
      "00:59:02.873 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 509 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "00:59:02.874 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "00:59:02.881 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.743 s\n",
      "00:59:02.885 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "00:59:02.885 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished\n",
      "00:59:02.887 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.798911 s\n",
      "00:59:03.362 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 172.23.57.81:36935 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "00:59:03.700 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.\n",
      "00:59:03.755 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "00:59:03.757 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "00:59:03.758 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "00:59:03.758 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "00:59:03.758 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "00:59:03.761 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "00:59:03.777 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "00:59:03.784 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "00:59:03.785 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.23.57.81:36935 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "00:59:03.787 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "00:59:03.788 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "00:59:03.789 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "00:59:03.793 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "00:59:03.794 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)\n",
      "00:59:03.822 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1662 bytes result sent to driver\n",
      "00:59:03.826 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 34 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "00:59:03.827 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "00:59:03.828 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.065 s\n",
      "00:59:03.829 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "00:59:03.829 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished\n",
      "00:59:03.830 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.074420 s\n",
      "00:59:03.871 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 5 ms to list leaf files for 1 paths.\n",
      "00:59:03.948 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "00:59:03.951 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "00:59:03.951 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "00:59:03.951 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "00:59:03.952 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "00:59:03.953 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "00:59:03.971 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 102.6 KiB, free 434.2 MiB)\n",
      "00:59:03.980 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.1 MiB)\n",
      "00:59:03.983 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.23.57.81:36935 (size: 36.9 KiB, free: 434.3 MiB)\n",
      "00:59:03.988 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "00:59:03.990 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "00:59:03.990 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "00:59:03.993 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9225 bytes) \n",
      "00:59:03.995 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)\n",
      "00:59:04.043 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:36935 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "00:59:04.055 [Executor task launch worker for task 0.0 in stage 2.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1988 bytes result sent to driver\n",
      "00:59:04.059 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 67 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "00:59:04.059 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "00:59:04.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.104 s\n",
      "00:59:04.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "00:59:04.061 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished\n",
      "00:59:04.062 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.113402 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d3d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the year, month and day columns into a single date column\n",
    "try:\n",
    "        df_flights = df_flights.withColumn(\"DATE\", \n",
    "                                                to_date(\n",
    "                                                        concat(\n",
    "                                                                col(\"YEAR\"),\n",
    "                                                                when(length(col(\"MONTH\")) == 1, lpad(col(\"MONTH\"), 2, \"0\")).otherwise(col(\"MONTH\")),\n",
    "                                                                when(length(col(\"DAY\")) == 1, lpad(col(\"DAY\"), 2, \"0\")).otherwise(col(\"DAY\"))\n",
    "                                                        ),\\\n",
    "                                                        \"yyyyMMdd\"\n",
    "                                                )\n",
    "                                        )\n",
    "except Exception as error:\n",
    "    logger.error(\"Error al crear la columna DATE:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd7fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create hash_id to identify the rows when insert into the database\n",
    "try:\n",
    "    df_airline = create_hash(df_airline)\n",
    "    df_airports = create_hash(df_airports)\n",
    "    df_flights = create_hash(df_flights)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error create_hash:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b537050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#charge the data into the database\n",
    "\n",
    "\n",
    "#write_DB(avg_flight, \"avg_flight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e46b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- hash_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_airline.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
