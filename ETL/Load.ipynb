{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55254b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58dc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_DB (parquet_DF, table):\n",
    "    try:\n",
    "        parquet_DF.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table,\n",
    "            mode=\"overwrite\",\n",
    "            properties=properties\n",
    "        )\n",
    "    except Exception as error:\n",
    "        logger.error(\"Error ruta .yaml:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf37df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:05:22.068 [Thread-4] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .config(\"spark.jars\", \"/home/illidan/proyecto_desde0/postgre/jars/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772d68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD conection\n",
    "\n",
    "try:\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5433/sparkdb_dev\"\n",
    "    properties = {\n",
    "        \"user\": config[\"sparkdb_dev\"][\"user\"],\n",
    "        \"password\": config[\"sparkdb_dev\"][\"password\"],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "except Exception as error:\n",
    "    logger.error(\"Error conection BD:\" + str(error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb483186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:05:22.110 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "14:05:22.159 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "14:05:22.163 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:05:22.165 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:22.165 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:05:22.165 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "14:05:22.166 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:22.166 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:22.168 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:22.174 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "14:05:22.180 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 102.9 KiB, free 434.3 MiB)\n",
      "14:05:22.184 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
      "14:05:22.187 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.4 MiB)\n",
      "14:05:22.188 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:22.189 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:22.189 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0\n",
      "14:05:22.191 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9367 bytes) \n",
      "14:05:22.192 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)\n",
      "14:05:22.253 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1720 bytes result sent to driver\n",
      "14:05:22.256 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 65 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:22.256 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "14:05:22.262 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.091 s\n",
      "14:05:22.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:22.263 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished\n",
      "14:05:22.264 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.099959 s\n",
      "14:05:22.300 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 1 paths.\n",
      "14:05:22.368 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:05:22.370 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:22.370 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:05:22.371 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:22.371 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:22.372 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[11] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:22.389 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 102.9 KiB, free 434.2 MiB)\n",
      "14:05:22.393 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.1 MiB)\n",
      "14:05:22.396 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "14:05:22.399 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:22.401 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:22.402 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "14:05:22.404 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9375 bytes) \n",
      "14:05:22.406 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)\n",
      "14:05:22.434 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 1865 bytes result sent to driver\n",
      "14:05:22.436 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 32 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:22.436 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "14:05:22.438 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.064 s\n",
      "14:05:22.439 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:22.439 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished\n",
      "14:05:22.439 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.070327 s\n",
      "14:05:22.470 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 1 paths.\n",
      "14:05:22.530 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:05:22.531 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:22.531 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:05:22.531 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:22.531 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:22.533 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:22.545 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 102.9 KiB, free 434.0 MiB)\n",
      "14:05:22.548 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.0 MiB)\n",
      "14:05:22.551 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "14:05:22.552 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:22.553 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:22.554 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks resource profile 0\n",
      "14:05:22.555 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 6) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9376 bytes) \n",
      "14:05:22.557 [Executor task launch worker for task 0.0 in stage 6.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 6)\n",
      "14:05:22.572 [Executor task launch worker for task 0.0 in stage 6.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 6). 1772 bytes result sent to driver\n",
      "14:05:22.573 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 6) in 18 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:22.573 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "14:05:22.574 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.040 s\n",
      "14:05:22.575 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:22.575 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 6: Stage finished\n",
      "14:05:22.576 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.045524 s\n",
      "14:05:22.604 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "14:05:22.641 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "14:05:22.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 7 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:22.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "14:05:22.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:22.643 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:22.644 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:22.652 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 102.9 KiB, free 433.9 MiB)\n",
      "14:05:22.654 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 433.9 MiB)\n",
      "14:05:22.656 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 172.23.57.81:34293 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "14:05:22.657 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:22.658 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:22.658 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks resource profile 0\n",
      "14:05:22.660 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 7) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9378 bytes) \n",
      "14:05:22.661 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 7)\n",
      "14:05:22.675 [Executor task launch worker for task 0.0 in stage 7.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 7). 1727 bytes result sent to driver\n",
      "14:05:22.677 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 7) in 16 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:22.677 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "14:05:22.677 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.032 s\n",
      "14:05:22.678 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:22.678 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished\n",
      "14:05:22.679 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.037114 s\n"
     ]
    }
   ],
   "source": [
    "#Read parquet file\n",
    "try:\n",
    "    read_avg_flight = config['Parquet_file']['avg_flight']\n",
    "    read_airline_in_airport = config['Parquet_file']['airline_in_airport']\n",
    "    read_flights_per_cancell = config['Parquet_file']['flights_per_cancell']\n",
    "    read_airport_notin_thelist = config['Parquet_file']['airport_notin_thelist']\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read parquet:\" + str(error))\n",
    "\n",
    "\n",
    "try:\n",
    "    avg_flight = spark.read.parquet(read_avg_flight)\n",
    "    airline_in_airport =  spark.read.parquet(read_airline_in_airport)\n",
    "    flights_per_cancell = spark.read.parquet(read_flights_per_cancell)\n",
    "    airport_notin_thelist = spark.read.parquet(read_airport_notin_thelist)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31bdbb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:05:22.944 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "14:05:22.951 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "14:05:22.957 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "14:05:22.962 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 172.23.57.81:34293 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "14:05:23.616 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:05:23.617 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:05:23.955 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 245.257963 ms\n",
      "14:05:23.973 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 200.9 KiB, free 434.2 MiB)\n",
      "14:05:23.984 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)\n",
      "14:05:23.988 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 172.23.57.81:34293 (size: 34.9 KiB, free: 434.4 MiB)\n",
      "14:05:23.990 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 8 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:24.009 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:05:24.136 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:24.137 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:24.137 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "14:05:24.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:24.141 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:24.142 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[21] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:24.207 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 31.5 KiB, free 434.1 MiB)\n",
      "14:05:24.210 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 434.1 MiB)\n",
      "14:05:24.211 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 172.23.57.81:34293 (size: 14.3 KiB, free: 434.4 MiB)\n",
      "14:05:24.212 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:24.213 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[21] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:24.213 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks resource profile 0\n",
      "14:05:24.219 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 8) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9874 bytes) \n",
      "14:05:24.220 [Executor task launch worker for task 0.0 in stage 8.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 8)\n",
      "14:05:24.343 [Executor task launch worker for task 0.0 in stage 8.0 (TID 8)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.986778 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:05:25.440 [Executor task launch worker for task 0.0 in stage 8.0 (TID 8)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.70337 ms\n",
      "14:05:25.444 [Executor task launch worker for task 0.0 in stage 8.0 (TID 8)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/avg_flight/part-00000-993f1908-08c5-4c37-91c4-d7ad5e3d16a3-c000.snappy.parquet, range: 0-1142, partition values: [empty row]\n",
      "14:05:25.569 [Executor task launch worker for task 0.0 in stage 8.0 (TID 8)] INFO  org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor [.snappy]\n",
      "14:05:25.770 [Executor task launch worker for task 0.0 in stage 8.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 8). 1800 bytes result sent to driver\n",
      "14:05:25.771 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 8) in 1556 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:25.772 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "14:05:25.773 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0) finished in 1.626 s\n",
      "14:05:25.773 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:25.773 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 8: Stage finished\n",
      "14:05:25.774 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: jdbc at NativeMethodAccessorImpl.java:0, took 1.637658 s\n",
      "14:05:25.876 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:05:25.876 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:05:25.906 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.987553 ms\n",
      "14:05:25.913 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 201.2 KiB, free 433.9 MiB)\n",
      "14:05:25.920 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.9 MiB)\n",
      "14:05:25.922 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 172.23.57.81:34293 (size: 35.0 KiB, free: 434.3 MiB)\n",
      "14:05:25.923 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 10 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:25.924 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:05:25.945 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:25.946 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:25.947 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "14:05:25.947 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:25.950 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:25.951 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[27] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:25.963 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 33.3 KiB, free 433.9 MiB)\n",
      "14:05:25.968 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 433.8 MiB)\n",
      "14:05:25.970 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 172.23.57.81:34293 (size: 15.0 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:05:25.971 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:25.971 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:25.972 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks resource profile 0\n",
      "14:05:25.974 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 9) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9882 bytes) \n",
      "14:05:25.975 [Executor task launch worker for task 0.0 in stage 9.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 9)\n",
      "14:05:26.015 [Executor task launch worker for task 0.0 in stage 9.0 (TID 9)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 28.677646 ms\n",
      "14:05:26.050 [Executor task launch worker for task 0.0 in stage 9.0 (TID 9)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.513712 ms\n",
      "14:05:26.051 [Executor task launch worker for task 0.0 in stage 9.0 (TID 9)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/part-00000-3c2dd5b1-da99-4363-8a99-8c60695e3bf8-c000.snappy.parquet, range: 0-10542, partition values: [empty row]\n",
      "14:05:26.084 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.23.57.81:34293 in memory (size: 14.3 KiB, free: 434.3 MiB)\n",
      "14:05:26.180 [Executor task launch worker for task 0.0 in stage 9.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 9). 1800 bytes result sent to driver\n",
      "14:05:26.181 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 9) in 207 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:26.182 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "14:05:26.183 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.230 s\n",
      "14:05:26.183 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:26.183 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 9: Stage finished\n",
      "14:05:26.184 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.238213 s\n",
      "14:05:26.273 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:05:26.273 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:05:26.301 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.079144 ms\n",
      "14:05:26.308 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 201.1 KiB, free 433.7 MiB)\n",
      "14:05:26.316 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.7 MiB)\n",
      "14:05:26.318 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 172.23.57.81:34293 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "14:05:26.319 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 12 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:26.321 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:05:26.343 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:26.344 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 10 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:26.344 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "14:05:26.344 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:26.347 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:26.348 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[33] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:26.359 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 32.4 KiB, free 433.6 MiB)\n",
      "14:05:26.361 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.6 KiB, free 433.6 MiB)\n",
      "14:05:26.363 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 172.23.57.81:34293 (size: 14.6 KiB, free: 434.3 MiB)\n",
      "14:05:26.363 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:26.364 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[33] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:26.364 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks resource profile 0\n",
      "14:05:26.365 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 10) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9883 bytes) \n",
      "14:05:26.366 [Executor task launch worker for task 0.0 in stage 10.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 10)\n",
      "14:05:26.394 [Executor task launch worker for task 0.0 in stage 10.0 (TID 10)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.986745 ms\n",
      "14:05:26.418 [Executor task launch worker for task 0.0 in stage 10.0 (TID 10)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.552794 ms\n",
      "14:05:26.420 [Executor task launch worker for task 0.0 in stage 10.0 (TID 10)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/part-00000-a63a7dec-3162-410e-a215-aba5d892c50d-c000.snappy.parquet, range: 0-1786, partition values: [empty row]\n",
      "14:05:26.454 [Executor task launch worker for task 0.0 in stage 10.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 10). 1757 bytes result sent to driver\n",
      "14:05:26.456 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 10) in 90 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:26.456 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "14:05:26.457 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.108 s\n",
      "14:05:26.458 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:26.458 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished\n",
      "14:05:26.458 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 10 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.115157 s\n",
      "14:05:26.543 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "14:05:26.543 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "14:05:26.570 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.261653 ms\n",
      "14:05:26.574 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 200.9 KiB, free 433.4 MiB)\n",
      "14:05:26.582 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.4 MiB)\n",
      "14:05:26.583 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 172.23.57.81:34293 (size: 34.8 KiB, free: 434.2 MiB)\n",
      "14:05:26.584 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 14 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:26.586 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "14:05:26.603 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "14:05:26.604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 11 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "14:05:26.604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "14:05:26.605 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "14:05:26.607 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "14:05:26.609 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[39] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "14:05:26.619 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 31.6 KiB, free 433.4 MiB)\n",
      "14:05:26.635 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 433.3 MiB)\n",
      "14:05:26.636 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 172.23.57.81:34293 (size: 14.3 KiB, free: 434.2 MiB)\n",
      "14:05:26.640 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 172.23.57.81:34293 in memory (size: 14.6 KiB, free: 434.2 MiB)\n",
      "14:05:26.638 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
      "14:05:26.640 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[39] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "14:05:26.641 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks resource profile 0\n",
      "14:05:26.643 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 11) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9885 bytes) \n",
      "14:05:26.644 [Executor task launch worker for task 0.0 in stage 11.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 11)\n",
      "14:05:26.647 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 172.23.57.81:34293 in memory (size: 15.0 KiB, free: 434.2 MiB)\n",
      "14:05:26.662 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 172.23.57.81:34293 in memory (size: 35.0 KiB, free: 434.3 MiB)\n",
      "14:05:26.671 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 172.23.57.81:34293 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "14:05:26.672 [Executor task launch worker for task 0.0 in stage 11.0 (TID 11)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.586536 ms\n",
      "14:05:26.711 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 172.23.57.81:34293 in memory (size: 34.9 KiB, free: 434.4 MiB)\n",
      "14:05:26.716 [Executor task launch worker for task 0.0 in stage 11.0 (TID 11)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 38.030123 ms\n",
      "14:05:26.718 [Executor task launch worker for task 0.0 in stage 11.0 (TID 11)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/part-00000-490a142d-31e1-4034-a9e2-beff47721a75-c000.snappy.parquet, range: 0-3699, partition values: [empty row]\n",
      "14:05:26.759 [Executor task launch worker for task 0.0 in stage 11.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 11). 1757 bytes result sent to driver\n",
      "14:05:26.760 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 11) in 118 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "14:05:26.761 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "14:05:26.762 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.151 s\n",
      "14:05:26.762 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "14:05:26.762 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 11: Stage finished\n",
      "14:05:26.763 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 11 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.159294 s\n"
     ]
    }
   ],
   "source": [
    "#write parquet file into DB\n",
    "\n",
    "write_DB(avg_flight, \"avg_flight\")\n",
    "write_DB(airline_in_airport,\"airline_in_airport\")\n",
    "write_DB(flights_per_cancell,\"flights_cancellation\")\n",
    "write_DB(airport_notin_thelist,\"airport_notin_thelist\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
