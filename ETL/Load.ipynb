{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55254b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b58dc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_DB (parquet_DF, table):\n",
    "    try:\n",
    "        parquet_DF.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table,\n",
    "            mode=\"overwrite\",\n",
    "            properties=properties\n",
    "        )\n",
    "    except Exception as error:\n",
    "        logger.error(\"Error ruta .yaml:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf37df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:20:31.672 [Thread-4] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .config(\"spark.jars\", \"/home/illidan/proyecto_desde0/postgre/jars/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "try:\n",
    "    #reading the config file\n",
    "    #geting file path into a dictionary\n",
    "    with open(\"/home/illidan/proyecto_desde0/Config_file/Config.Yaml\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "772d68d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:20:31.687 [Thread-4] ERROR __main__ - Error conection BD:name 'sparkdb_dev' is not defined\n"
     ]
    }
   ],
   "source": [
    "#BD conection\n",
    "\n",
    "try:\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5433/sparkdb_dev\"\n",
    "    properties = {\n",
    "        \"user\": config[sparkdb_dev][user],\n",
    "        \"password\": config[sparkdb_dev][password],\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "except Exception as error:\n",
    "    logger.error(\"Error conection BD:\" + str(error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb483186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:20:31.714 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "01:20:31.743 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "01:20:31.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 29 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:31.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "01:20:31.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:31.744 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:31.745 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[110] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:31.754 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_43 stored as values in memory (estimated size 102.9 KiB, free 433.1 MiB)\n",
      "01:20:31.765 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_43_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 433.1 MiB)\n",
      "01:20:31.766 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_43_piece0 in memory on 172.23.57.81:44505 (size: 37.2 KiB, free: 434.2 MiB)\n",
      "01:20:31.767 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_42_piece0 on 172.23.57.81:44505 in memory (size: 14.3 KiB, free: 434.2 MiB)\n",
      "01:20:31.767 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 43 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:31.767 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[110] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:31.768 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 1 tasks resource profile 0\n",
      "01:20:31.769 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 29) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9367 bytes) \n",
      "01:20:31.771 [Executor task launch worker for task 0.0 in stage 29.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 29)\n",
      "01:20:31.772 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_41_piece0 on 172.23.57.81:44505 in memory (size: 34.8 KiB, free: 434.2 MiB)\n",
      "01:20:31.788 [Executor task launch worker for task 0.0 in stage 29.0 (TID 29)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 29). 1720 bytes result sent to driver\n",
      "01:20:31.790 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 29) in 21 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:31.790 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "01:20:31.791 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.045 s\n",
      "01:20:31.791 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:31.791 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 29: Stage finished\n",
      "01:20:31.792 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 29 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.048516 s\n",
      "01:20:31.830 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 172.23.57.81:44505 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:31.830 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.\n",
      "01:20:31.834 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 172.23.57.81:44505 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:31.864 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "01:20:31.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 30 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:31.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 30 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "01:20:31.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:31.866 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:31.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 30 (MapPartitionsRDD[112] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:31.873 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_44 stored as values in memory (estimated size 102.9 KiB, free 433.7 MiB)\n",
      "01:20:31.885 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_44_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 433.7 MiB)\n",
      "01:20:31.886 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_44_piece0 in memory on 172.23.57.81:44505 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "01:20:31.887 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_43_piece0 on 172.23.57.81:44505 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "01:20:31.887 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 44 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:31.888 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[112] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:31.888 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 30.0 with 1 tasks resource profile 0\n",
      "01:20:31.890 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 30.0 (TID 30) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9375 bytes) \n",
      "01:20:31.892 [Executor task launch worker for task 0.0 in stage 30.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 30.0 (TID 30)\n",
      "01:20:31.907 [Executor task launch worker for task 0.0 in stage 30.0 (TID 30)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 30.0 (TID 30). 1774 bytes result sent to driver\n",
      "01:20:31.909 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 30.0 (TID 30) in 20 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:31.909 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "01:20:31.910 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 30 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.043 s\n",
      "01:20:31.910 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:31.910 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 30: Stage finished\n",
      "01:20:31.911 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 30 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.045810 s\n",
      "01:20:31.947 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 1 ms to list leaf files for 1 paths.\n",
      "01:20:31.978 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "01:20:31.979 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 31 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:31.979 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 31 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "01:20:31.979 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:31.979 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:31.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 31 (MapPartitionsRDD[114] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:31.986 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_45 stored as values in memory (estimated size 102.9 KiB, free 433.7 MiB)\n",
      "01:20:31.998 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_45_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 433.7 MiB)\n",
      "01:20:31.998 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_45_piece0 in memory on 172.23.57.81:44505 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "01:20:31.999 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_44_piece0 on 172.23.57.81:44505 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "01:20:31.999 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:32.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[114] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:32.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 31.0 with 1 tasks resource profile 0\n",
      "01:20:32.001 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 31.0 (TID 31) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9376 bytes) \n",
      "01:20:32.002 [Executor task launch worker for task 0.0 in stage 31.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 31.0 (TID 31)\n",
      "01:20:32.016 [Executor task launch worker for task 0.0 in stage 31.0 (TID 31)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 31.0 (TID 31). 1772 bytes result sent to driver\n",
      "01:20:32.017 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 31.0 (TID 31) in 16 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:32.018 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "01:20:32.018 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 31 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.038 s\n",
      "01:20:32.018 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:32.018 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 31: Stage finished\n",
      "01:20:32.019 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 31 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.041064 s\n",
      "01:20:32.050 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 1 ms to list leaf files for 1 paths.\n",
      "01:20:32.076 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "01:20:32.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 32 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:32.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 32 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "01:20:32.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:32.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:32.079 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 32 (MapPartitionsRDD[116] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:32.087 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_46 stored as values in memory (estimated size 102.9 KiB, free 433.7 MiB)\n",
      "01:20:32.101 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_46_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 433.7 MiB)\n",
      "01:20:32.103 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_46_piece0 in memory on 172.23.57.81:44505 (size: 37.2 KiB, free: 434.3 MiB)\n",
      "01:20:32.105 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 46 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:32.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[116] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:32.106 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 32.0 with 1 tasks resource profile 0\n",
      "01:20:32.107 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_45_piece0 on 172.23.57.81:44505 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "01:20:32.108 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 32.0 (TID 32) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9378 bytes) \n",
      "01:20:32.109 [Executor task launch worker for task 0.0 in stage 32.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 32.0 (TID 32)\n",
      "01:20:32.128 [Executor task launch worker for task 0.0 in stage 32.0 (TID 32)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 32.0 (TID 32). 1727 bytes result sent to driver\n",
      "01:20:32.129 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 32.0 (TID 32) in 21 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:32.129 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "01:20:32.130 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 32 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.051 s\n",
      "01:20:32.130 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:32.130 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 32: Stage finished\n",
      "01:20:32.131 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 32 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.053892 s\n"
     ]
    }
   ],
   "source": [
    "#Read parquet file\n",
    "try:\n",
    "    read_avg_flight = config['Parquet_file']['avg_flight']\n",
    "    read_airline_in_airport = config['Parquet_file']['airline_in_airport']\n",
    "    read_flights_per_cancell = config['Parquet_file']['flights_per_cancell']\n",
    "    read_airport_notin_thelist = config['Parquet_file']['airport_notin_thelist']\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read parquet:\" + str(error))\n",
    "\n",
    "\n",
    "try:\n",
    "    avg_flight = spark.read.parquet(read_avg_flight)\n",
    "    airline_in_airport =  spark.read.parquet(read_airline_in_airport)\n",
    "    flights_per_cancell = spark.read.parquet(read_flights_per_cancell)\n",
    "    airport_notin_thelist = spark.read.parquet(read_airport_notin_thelist)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bdbb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:20:32.279 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "01:20:32.280 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "01:20:32.291 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_47 stored as values in memory (estimated size 200.9 KiB, free 433.6 MiB)\n",
      "01:20:32.309 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_46_piece0 on 172.23.57.81:44505 in memory (size: 37.2 KiB, free: 434.3 MiB)\n",
      "01:20:32.311 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_47_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.7 MiB)\n",
      "01:20:32.312 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_47_piece0 in memory on 172.23.57.81:44505 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:32.312 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 47 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:32.314 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "01:20:32.334 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:32.335 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 33 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:32.335 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 33 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "01:20:32.335 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:32.339 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:32.340 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 33 (MapPartitionsRDD[122] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:32.350 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_48 stored as values in memory (estimated size 31.5 KiB, free 433.7 MiB)\n",
      "01:20:32.351 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_48_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 433.7 MiB)\n",
      "01:20:32.352 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_48_piece0 in memory on 172.23.57.81:44505 (size: 14.3 KiB, free: 434.3 MiB)\n",
      "01:20:32.353 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:32.353 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[122] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:32.353 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 33.0 with 1 tasks resource profile 0\n",
      "01:20:32.354 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 33.0 (TID 33) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9874 bytes) \n",
      "01:20:32.371 [Executor task launch worker for task 0.0 in stage 33.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 33.0 (TID 33)\n",
      "01:20:32.386 [Executor task launch worker for task 0.0 in stage 33.0 (TID 33)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/avg_flight/part-00000-519423e8-a7b5-47af-a072-189f3b23bd16-c000.snappy.parquet, range: 0-1142, partition values: [empty row]\n",
      "01:20:32.429 [Executor task launch worker for task 0.0 in stage 33.0 (TID 33)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 33.0 (TID 33). 1843 bytes result sent to driver\n",
      "01:20:32.430 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 33.0 (TID 33) in 76 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:32.431 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "01:20:32.431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 33 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.090 s\n",
      "01:20:32.431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:32.431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 33: Stage finished\n",
      "01:20:32.432 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 33 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.097694 s\n",
      "01:20:32.539 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "01:20:32.539 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "01:20:32.557 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_49 stored as values in memory (estimated size 201.1 KiB, free 433.5 MiB)\n",
      "01:20:32.569 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_47_piece0 on 172.23.57.81:44505 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:32.573 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_48_piece0 on 172.23.57.81:44505 in memory (size: 14.3 KiB, free: 434.3 MiB)\n",
      "01:20:32.573 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_49_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.7 MiB)\n",
      "01:20:32.574 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_49_piece0 in memory on 172.23.57.81:44505 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:32.574 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 49 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:32.575 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "01:20:32.590 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:32.591 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 34 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:32.591 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 34 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "01:20:32.591 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:32.595 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:32.596 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 34 (MapPartitionsRDD[128] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:32.608 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_50 stored as values in memory (estimated size 32.4 KiB, free 433.7 MiB)\n",
      "01:20:32.610 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_50_piece0 stored as bytes in memory (estimated size 14.6 KiB, free 433.7 MiB)\n",
      "01:20:32.612 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_50_piece0 in memory on 172.23.57.81:44505 (size: 14.6 KiB, free: 434.3 MiB)\n",
      "01:20:32.612 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 50 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:32.613 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[128] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:32.625 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 34.0 with 1 tasks resource profile 0\n",
      "01:20:32.627 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 34.0 (TID 34) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9882 bytes) \n",
      "01:20:32.627 [Executor task launch worker for task 0.0 in stage 34.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 34.0 (TID 34)\n",
      "01:20:32.635 [Executor task launch worker for task 0.0 in stage 34.0 (TID 34)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/airline_in_airport/part-00000-a79ffc6a-3e89-4441-a031-73fa3811feb0-c000.snappy.parquet, range: 0-2089, partition values: [empty row]\n",
      "01:20:32.673 [Executor task launch worker for task 0.0 in stage 34.0 (TID 34)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 34.0 (TID 34). 1843 bytes result sent to driver\n",
      "01:20:32.674 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 34.0 (TID 34) in 48 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:32.674 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "01:20:32.675 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 34 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.077 s\n",
      "01:20:32.675 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:32.675 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 34: Stage finished\n",
      "01:20:32.675 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 34 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.085081 s\n",
      "01:20:32.781 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "01:20:32.782 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "01:20:32.790 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_51 stored as values in memory (estimated size 201.1 KiB, free 433.5 MiB)\n",
      "01:20:32.801 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_50_piece0 on 172.23.57.81:44505 in memory (size: 14.6 KiB, free: 434.3 MiB)\n",
      "01:20:32.805 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_51_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.5 MiB)\n",
      "01:20:32.806 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_51_piece0 in memory on 172.23.57.81:44505 (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:32.807 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 51 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:32.807 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_49_piece0 on 172.23.57.81:44505 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:32.808 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "01:20:32.825 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:32.826 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 35 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:32.826 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 35 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "01:20:32.826 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:32.829 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:32.830 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 35 (MapPartitionsRDD[134] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:32.841 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_52 stored as values in memory (estimated size 32.4 KiB, free 433.7 MiB)\n",
      "01:20:32.855 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_52_piece0 stored as bytes in memory (estimated size 14.6 KiB, free 433.7 MiB)\n",
      "01:20:32.856 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_52_piece0 in memory on 172.23.57.81:44505 (size: 14.6 KiB, free: 434.3 MiB)\n",
      "01:20:32.857 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 52 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:32.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[134] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:32.857 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 35.0 with 1 tasks resource profile 0\n",
      "01:20:32.858 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 35.0 (TID 35) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9883 bytes) \n",
      "01:20:32.859 [Executor task launch worker for task 0.0 in stage 35.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 35.0 (TID 35)\n",
      "01:20:32.867 [Executor task launch worker for task 0.0 in stage 35.0 (TID 35)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/flights_per_cancell/part-00000-145d0d81-1938-4efa-b75e-ea05c85c9c31-c000.snappy.parquet, range: 0-1786, partition values: [empty row]\n",
      "01:20:32.906 [Executor task launch worker for task 0.0 in stage 35.0 (TID 35)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 35.0 (TID 35). 1843 bytes result sent to driver\n",
      "01:20:32.907 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 35.0 (TID 35) in 49 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:32.908 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "01:20:32.909 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 35 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.077 s\n",
      "01:20:32.909 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:32.909 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 35: Stage finished\n",
      "01:20:32.909 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 35 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.084448 s\n",
      "01:20:32.992 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "01:20:32.992 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "01:20:33.002 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_53 stored as values in memory (estimated size 200.9 KiB, free 433.5 MiB)\n",
      "01:20:33.014 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_52_piece0 on 172.23.57.81:44505 in memory (size: 14.6 KiB, free: 434.3 MiB)\n",
      "01:20:33.022 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_51_piece0 on 172.23.57.81:44505 in memory (size: 34.9 KiB, free: 434.3 MiB)\n",
      "01:20:33.023 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_53_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.5 MiB)\n",
      "01:20:33.023 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_53_piece0 in memory on 172.23.57.81:44505 (size: 34.8 KiB, free: 434.3 MiB)\n",
      "01:20:33.024 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 53 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:33.025 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "01:20:33.044 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "01:20:33.046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 36 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "01:20:33.046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 36 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "01:20:33.046 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "01:20:33.050 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "01:20:33.051 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 36 (MapPartitionsRDD[140] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "01:20:33.059 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_54 stored as values in memory (estimated size 31.6 KiB, free 433.7 MiB)\n",
      "01:20:33.076 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_54_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 433.7 MiB)\n",
      "01:20:33.076 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_54_piece0 in memory on 172.23.57.81:44505 (size: 14.3 KiB, free: 434.3 MiB)\n",
      "01:20:33.077 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 54 from broadcast at DAGScheduler.scala:1585\n",
      "01:20:33.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[140] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "01:20:33.078 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 36.0 with 1 tasks resource profile 0\n",
      "01:20:33.079 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 36.0 (TID 36) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9885 bytes) \n",
      "01:20:33.080 [Executor task launch worker for task 0.0 in stage 36.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 36.0 (TID 36)\n",
      "01:20:33.089 [Executor task launch worker for task 0.0 in stage 36.0 (TID 36)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/illidan/proyecto_desde0/archivos_parquet/airport_notin_thelist/part-00000-713c4842-b8a2-4621-87e7-3f99e4e07467-c000.snappy.parquet, range: 0-3699, partition values: [empty row]\n",
      "01:20:33.135 [Executor task launch worker for task 0.0 in stage 36.0 (TID 36)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 36.0 (TID 36). 1843 bytes result sent to driver\n",
      "01:20:33.136 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 36.0 (TID 36) in 57 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "01:20:33.136 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "01:20:33.137 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 36 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.085 s\n",
      "01:20:33.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "01:20:33.138 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 36: Stage finished\n",
      "01:20:33.138 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 36 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.093560 s\n"
     ]
    }
   ],
   "source": [
    "#write parquet file into DB\n",
    "\n",
    "write_DB(avg_flight, \"avg_flight\")\n",
    "write_DB(airline_in_airport,\"airline_in_airport\")\n",
    "write_DB(flights_per_cancell,\"flights_cancellation\")\n",
    "write_DB(airport_notin_thelist,\"airport_notin_thelist\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
