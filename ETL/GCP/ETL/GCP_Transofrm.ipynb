{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a3abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, max, min, count, broadcast, desc, asc, when, rank, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31cfebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:09:38.804 [Thread-3] INFO  __main__ - Log de ejemplo guardado en archivo y consola.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", r'-Dlog4j.configurationFile=file:/home/illidan/proyecto_desde0/ETL/log4j.properties')\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Log de ejemplo guardado en archivo y consola.\")\n",
    "\n",
    "errores_detectados = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ebb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  read_parquet_airline = \"/home/illidan/proyecto_desde0/archivos_parquet/df_airline\"\n",
    "  read_parquet_flights =  \"/home/illidan/proyecto_desde0/archivos_parquet/df_flights\"\n",
    "  read_parquet_airports =  \"/home/illidan/proyecto_desde0/archivos_parquet/df_airports\"\n",
    "except Exception as error:\n",
    "    logger.error(\"Error ruta .yaml:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4b3eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:09:38.859 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 7 ms to list leaf files for 1 paths.\n",
      "16:09:38.877 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 172.23.57.81:38145 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "16:09:38.916 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "16:09:38.917 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "16:09:38.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "16:09:38.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "16:09:38.918 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "16:09:38.920 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[7] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "16:09:38.938 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 102.6 KiB, free 434.3 MiB)\n",
      "16:09:38.941 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.3 MiB)\n",
      "16:09:38.942 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 172.23.57.81:38145 (size: 36.9 KiB, free: 434.4 MiB)\n",
      "16:09:38.943 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "16:09:38.944 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "16:09:38.944 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0\n",
      "16:09:38.947 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "16:09:38.949 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)\n",
      "16:09:38.984 [Executor task launch worker for task 0.0 in stage 3.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2967 bytes result sent to driver\n",
      "16:09:38.993 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 47 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "16:09:38.993 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "16:09:38.995 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.073 s\n",
      "16:09:38.995 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "16:09:38.995 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished\n",
      "16:09:38.997 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.080483 s\n",
      "16:09:39.045 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 5 ms to list leaf files for 1 paths.\n",
      "16:09:39.110 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "16:09:39.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "16:09:39.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "16:09:39.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "16:09:39.113 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "16:09:39.115 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "16:09:39.131 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 102.6 KiB, free 434.2 MiB)\n",
      "16:09:39.141 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.1 MiB)\n",
      "16:09:39.143 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 172.23.57.81:38145 (size: 36.9 KiB, free: 434.3 MiB)\n",
      "16:09:39.144 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "16:09:39.145 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "16:09:39.145 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0\n",
      "16:09:39.147 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9224 bytes) \n",
      "16:09:39.148 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)\n",
      "16:09:39.190 [Executor task launch worker for task 0.0 in stage 4.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1748 bytes result sent to driver\n",
      "16:09:39.194 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 48 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "16:09:39.194 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "16:09:39.195 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.079 s\n",
      "16:09:39.196 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "16:09:39.196 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished\n",
      "16:09:39.197 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.085965 s\n",
      "16:09:39.208 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 172.23.57.81:38145 in memory (size: 36.9 KiB, free: 434.4 MiB)\n",
      "16:09:39.275 [Thread-3] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 6 ms to list leaf files for 1 paths.\n",
      "16:09:39.347 [Thread-3] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "16:09:39.349 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "16:09:39.349 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "16:09:39.350 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "16:09:39.353 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "16:09:39.356 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[11] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "16:09:39.369 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 102.6 KiB, free 434.2 MiB)\n",
      "16:09:39.373 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 434.1 MiB)\n",
      "16:09:39.375 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 172.23.57.81:38145 (size: 36.9 KiB, free: 434.3 MiB)\n",
      "16:09:39.376 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "16:09:39.377 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "16:09:39.378 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0\n",
      "16:09:39.379 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5) (172.23.57.81, executor driver, partition 0, PROCESS_LOCAL, 9225 bytes) \n",
      "16:09:39.380 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)\n",
      "16:09:39.397 [Executor task launch worker for task 0.0 in stage 5.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 1902 bytes result sent to driver\n",
      "16:09:39.399 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 20 ms on 172.23.57.81 (executor driver) (1/1)\n",
      "16:09:39.400 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "16:09:39.401 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.044 s\n",
      "16:09:39.402 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "16:09:39.402 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished\n",
      "16:09:39.402 [Thread-3] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.055017 s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_flights = spark.read.parquet(read_parquet_flights)\n",
    "    df_airline = spark.read.parquet(read_parquet_airline)\n",
    "    df_airports = spark.read.parquet(read_parquet_airports)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error read.parquet:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1952a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #avg per fly\n",
    "    avg_flight = df_flights.join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                        .groupBy(df_flights.AIRLINE, df_airline.AIRLINE) \\\n",
    "                        .agg(round(avg(df_flights.DISTANCE), 2).alias(\"avg_DISTANCE\")) \\\n",
    "                        .orderBy(desc(\"avg_DISTANCE\")) \\\n",
    "                        .select(df_airline.AIRLINE.alias(\"AIRLINE_Name\"), \"avg_DISTANCE\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable avg_flight:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "237699b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    #how many times a flight visit an airport\n",
    "    airline_in_airport = df_flights.join(broadcast(df_airports), df_flights.ORIGIN_AIRPORT == df_airports.IATA_CODE) \\\n",
    "                                .join(broadcast(df_airline), df_flights.AIRLINE == df_airline.IATA_CODE) \\\n",
    "                                .repartition(df_flights.DESTINATION_AIRPORT) \\\n",
    "                                .groupBy(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE) \\\n",
    "                                    .agg(count(df_flights.AIRLINE).alias(\"Count_visit_per_airline\")) \\\n",
    "                                .orderBy(asc(df_flights.DESTINATION_AIRPORT)) \\\n",
    "                                .select(df_flights.DESTINATION_AIRPORT, df_airline.AIRLINE,\"Count_visit_per_airline\")\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airline_in_airport:\" + str(error))\n",
    "\n",
    "#a Rank of wich airline is the most visited in each airport\n",
    "try:\n",
    "    window_spec = Window.partitionBy(\"DESTINATION_AIRPORT\").orderBy(desc(\"Count_visit_per_airline\"))\n",
    "    \n",
    "    Rank_airline_in_airport = airline_in_airport.withColumn(\"ranking\", rank().over(window_spec))\n",
    "\n",
    "\n",
    "except Exception as error:\n",
    "    logger.error(\"Error Rank_airline_in_airport:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad20a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #mention how many times an airline visit a destination and canceled this arrival\n",
    "    flights_per_cancell = df_flights.select(col(\"AIRLINE\"), col(\"CANCELLED\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .filter(col(\"CANCELLED\") == \"1\") \\\n",
    "                                .repartition(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .groupBy(col(\"AIRLINE\"), col(\"DESTINATION_AIRPORT\")) \\\n",
    "                                    .agg(count(col(\"AIRLINE\")).alias(\"count_airlines_cancel\")) \\\n",
    "                                .orderBy(desc(\"DESTINATION_AIRPORT\")) \\\n",
    "                                .limit(100)\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable flights_per_cancell:\" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7929cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #i create this variable because i need to know how many airports have flights but are not in the list of airports\n",
    "    airport_notin_thelist = df_flights.join(broadcast(df_airports), df_flights.DESTINATION_AIRPORT == df_airports.IATA_CODE, \"left_anti\") \\\n",
    "                                        .select(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .repartition(\"DESTINATION_AIRPORT\") \\\n",
    "                                        .groupBy(\"DESTINATION_AIRPORT\").agg(count(\"*\").alias(\"Count_airports\"))\n",
    "except Exception as error:\n",
    "    logger.error(\"Error variable airport_notin_thelist:\" + str(error))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
